const pages = {
    "page1": "When you begin a journey, it\u2019s a good idea to have a mental map of the terrain you\u2019ll be passing through. The same is true for an intellectual journey, such as learning to write computer programs. In this case, you\u2019ll need to know the basics of what computers are and how they work. You\u2019ll want to have some idea of what a computer program is and how one is created. Since you will be writing programs \u00ae in the Java programming language, you\u2019ll want to know something about that language in particular and about the modern, networked computing environment fo\u00aer which Java is designed. As you read this chapter, don\u2019t worry i\u00aef you can\u2019t understand everything in detail. (In fact, it would be impossible for you to learn all the details from the brief expositions in this chapter.) Concentrate on learning \u00ae enough about the big ideas to orient yourself, in preparation for the rest of the book. Most of what is covered in this chapter will be covered in much greater detail later in the book. A computer is a complex system consisting of many different components. But at the heart or the brain, if you want of the computer is a single component that does the actual computing. This is the Central Processing Unit, or CPU. \u00ae In a modern desktop computer, the CPU is a single \u201cchip\u201d on the order of one square inch in size. The job of the CPU is to execute programs. A program is simply a list of unambiguous instructions meant to be followed mechanically by a computer. A computer is built to carry out instructions that are written in a very simple type of language called machine language. Each type of computer has its own machine language, and the computer can directly execute a program only if the program is expressed in that language. (It can execute programs written in other languages if they are first translated into machine language.) When the CPU executes a program, that program is stored in the computer\u2019s main memory (also called the RAM or random access memory). In addition to the program, memory can also hold data that is being used or processed by the program. Main memory consists of a sequence of locations. These locations are numbered, and the sequence number\u00ae of a location is called its address. An address provides a way of picking out one particular piece of information from among the millions stored in memory. When the CPU needs to access the program instruction o\u00aer data in a particular location, it sends the address of that information as a signal to the memory; the memory responds by sending back\u00ae the data contained in the specifie location. The CPU can also store information in memory by specifying the information to be stored and the \u00aeaddress of the location where it is to be st\u00aeored. On the level of machine language, the operation of the CPU is fairly straightforward (although it is very complicated in detail). The CPU executes a program that is stored as a sequence of machine language instructions in main memory. It does this by repeatedly reading, or fetching, an instruction from memory and then carrying out, or executing, that instruction. This process fetch an instruction, execute it, fetch \u00aeanother instruction, execute it, and so on forever is called the fetch-and-execute cycle. With one exception, which will be covered in the next section, this is all that the CPU ever does. The details of the fetch-and-execute cycle are not terribly important, but there are a few basic things you should know. The CPU contains a few internal registers, which are small memory units capable of holding a single number or machine language instruction. The CPU uses one of these registers the program counter, or PC to keep track of where it is in the program it is executing. The PC stores the address of the next instruction that the CPU should execute. At the beginning of each fetch-and\u00ae-execute cycle, the CPU checks the PC to see which instruction it should fetch. During the course of the f\u00aeetch-and-execute cycle, the number in the PC is updated to indicate the instruction that is to be executed in the next cycle. A computer executes machine language programs mechanically that is without understanding them or thinking about them simply because of the way it is physically put together. This is not an easy concept. A computer is a machine built of millions of tiny switches called transistors, which have the property that they can be wired together in such a way that an output from one switch can turn another switch on or off. As a computer computes, these switches turn each other on or off in a pattern determined both by the way they are wired together and by the program that the computer is executing.",
    "page2": "Machine language instruction\u00aes are expressed as binary numbers. A binary number is made up of just two possible digits, zero and one. So, a machine language instruction is just a sequence of zeros and ones. Each particular sequence encodes some particular instruction. The data that the computer manipulates is also encoded as binary numbers. A computer can work directly with binary numbers because switches can readily represent such numbers: Turn the switch on to represent a one; turn it off to represent a zero. Machine language instructions are stored in memory as patterns of switches turned on or off, When a machine language instruction is loaded into CPU, all that happens is that certain switches are turned on or off in the pattern that encodes that particular instruction. The CPU is built to respond to this pattern by executing the instruction it encodes; it does this simply because of the way all the other switches in the CPU are wired together. So, you should understand this much about how computers work: Main memory holds machine language programs and data. These are encoded as binary numbers. The CPU fetches machine language instructions from memory one after another and executes them. It does this mechanically, without thinking about or understanding what it does-and therefore the program it executes must be perfect, complete in all details, and unambiguous because the CPU can do nothing but execute it exactly as written. Here is a schematic view of this first-stage understanding of t\u00aehe computer Asynchronous Events: Polling Loops and Interrupts The CPU spends almost all of its time fetching instructions from memory and executing them. However, the CPU and main memory are only two out of many components in areal computer system, A complete system contains other devices such as: A hard disk for storing programs and data files.(Note that main memory holds only a comparatively small amount of information, and holds it only as long as the power is turned on. A hard disk is necessary for permanent storage of larger amounts of information, but programs have to be loaded from disk into main memory before they can actually be executed.) A keyboard and mouse for user input. A monitor and printer which can be used to display the computer's output. A modem that allows the computer to communicate with other computers over telephone lines. A network interface that allows the computer to communicate with other computers that are connected to it on a network. A scanner that converts images into coded binary numbers that can be stored and manipulated on the computer. The list of devices is entirely open ended, and computer systems are built so that they can\u00ae easily be expanded by adding new devices. Somehow the CPU has to communicate with and control all these devices. The CPU can only do this by executing machine language instructions (which is all it can do, period). The way this works is that for each device in a system, there is a device driver, which consists of software that the CPU executes when it has to deal with the device. Installing a new device on a system generally has two steps: plugging the device physically into the computer, and installing the device driver software. Without the device driver, the actual physical device would be useless, since the CPU would not be able to communicate with it. A computer system consisting of many devices is typically organized by connecting those devices to one or more busses. A bus is a set of wires that carry various sorts of information between the devices connected to those wires. The wires carry data, addresses, and controls signals. An address directs the data to a particular device and perhaps to a particular register or location within that device. Control signals can be used, for example, by one device to alert another that data is available for it on the data bus. A fairly simple computer system might be organized like this Now, devices such as keyboard, mouse, and network interface can produce input that needs to be processed by the CPU\u00ae. How does the CPU know that the data is there? One simple idea, which turns out to be nit very satisfactor\u00aey, is for the CPU to keep checking for incoming data over and over. Whenever it finds data, it processes it. This method is called polling, si\u00aence the CPU polls the input devices continually to see whether they have any i\u00aenput data to report. Unfortunately, although polling is very simple, it is also very inefficient. The CPU can waste an awful lot of time just\u00ae waiting for input. To avoid this inefficiency, interrupts are often used instead of polling. An interrupt is a signal sent by another device to the CPU. The CPU responds to an interrupt signal by putting aside whatever it is doing in order to respond to the interrupt. Once it has handled the interrupt, it returns to what it was doing before the interrupts occurred. For example, when you press a key on your computer keyboard, a keyboard interrupt is sent to the CPU. The CPU responds to this signal by interru\u00aepting what it is doing, reading the key that you pressed, processing it, and returning to the task it was performing before you pressed the key.",
    "page3": "Again, you should understand that this is a purely \u00aemechanical process: A device signals an interrupt simply by turning on a wire. The CPU is built so that when that wire is turned on, the CPU saves enough information about what it is currently doing so that\u00ae it can return to the same state later. This information consists of the contents of important internal registers such as the program counter. Then the CPU jumps to some predetermined memory location and begins executing the instructions stored there. Those instructions make up an interrupt handler that does the processing necessary to respond to the interrupt. (This interrupt handler is part of the device driver software for the device that signalled the interrupt.) At the end of the interrupt handler is an instruction that tells the CPU to jump back to what it was doing; it does that by restoring its previously saved state. Interrupts allow the CPU to deal with asynchronous events. In the regular fetch-and execute cycle, things happen in a predetermined order; everything that happens is \u201csynchronized\u201d with everything else. Interrupts make it possible for the CPU to deal efficiently with events that happen \u201casynchronously,\u201d that is, at unpredictable times. As another example of how interrupts are used, consider what happens when the CPU need to access data that is stored on the hard disk. The CPU can access data directly only if it is in main memory. Data on the disk has to be copied into memory before it can be accessed. Unfortunately, on the scale of speed at which the CPU operates, the disk drive is extremely slow. When the CPU needs data from the disk, it sends a signal to the disk drive telling it to locate the data and get it ready. (This signal is sent synchronously, under the control of a regular program.) Then, instead of just waiting the long and unpredictalble amount of time that the disk drive will take to do this, the CPU goes on with some other task. When the disk drive has the data ready, it sends an interrupt signal to the CPU. The interrupt handler can then read the requested data. Now, you might have noticed that all this only makes sense if the CPU actually has several tasks to perform. If it has nothing better to do, it might as well spend its time polling for input or waiting for disk drive operations to complete. All modern computers use multitasking to perform several tasks at once. Some computers can be used by several people at once. Since the CPU is so fast, it can quickly switch its attention from one user to another, devoting a fraction of a second to each user in turn. This application of multitasking is called timesharing. But a modern personal computer with just a single user also uses multitasking. For example, the user might be typing a paper while a clock is continuously displaying the time and a file is being downloaded over the network. Each of the individual tasks that the CPU is working on is called a thread. (Or a pr\u00aeocess; there are technical differences between threads and processes, but they are not important here.) At any given time, only one thread can actually be executed by a CPU. The CPU will continue running the same thread until one of several things happens The thread might voluntarily yield control, to give other threads a chance to run. The thread might have to wait for some asynchronous event to occur. For example, the thread might request some data from the disk drive, or it might wait for the user to press a key. While it is waiting, the thread is said to be blocked, and other thread\u00aes have a chance to run. When the event occurs, an interrupt wi\u00aell \u201cwake up\u201d the thread so that it can continue running. The thread might use up its allotted slice of time and be suspended to allow other threads to run. Not all computers can \u201cforcibly\u201d suspend a thread in this way; those that can are said to use preemptive multitas\u00aeking. To do preemptive multitasking, a computer needs a special timer device that generates an interrupt at regular intervals, such as 100 times per second. When a timer interrupt occurs, the CPU has a chance to switch from one thread to another, whether the thread that is currently running likes it or not. Ordinary users, and indeed ordinary programmers, have no need to deal with interrupts and interrupt handlers. They can concentrate on the different tasks or threads that they want the computer to perform; the details of how the computer manages to get all those tasks done are not important to them. In fact, most users, and many programmers, can ignore threads and multitasking altogether. However, threads have become increasingly important as computers have become more powerful and as they have begun to make more use of multitasking. Indeed, threads are built into the Java programming language as a fundamental programming concept. Just as important in Java and in modern programming in general is the basic concept of asynchronous events. While programmers don\u2019t actually deal with interrupts directly, they do often find themselves writing event handlers, which, like interrupt handlers, are called asynchronously when specified events occur. Such \u201cev\u00ae\u00aeent-driven programming\u201d has a very different feel from the more traditional straight-through, synchronous programming. We will begin with the more traditional type of programming, which is still used for programming individual tasks, but we will return to threads and events later in the text. By the way, the software that does all the interrupt handling and the communication with the user and with hardware devices is called the operating system. The operating system is the basic, essential software without which a computer would not be able to function. Other programs, such as word processors and World Wide Web browsers, are dependent upon the operating system. Common operating systems include Linux, DOS, W\u00aeindows 2000, Windows XP, and the Macintosh OS",
    "page4": "The Java Virtual Machine Machine language consists of very simple instructions that can be executed directly by the CPU of a computer. Almost all programs, though, are written in high-level programming languages such as Java, Pascal, or C++. \u00ae A program written in a high-level language cannot be run directly on any computer. First, it has to be translated into machine language. This translation can be done by a program called a compiler. A compiler takes a high-level-language program and translates it into an executable machine-language program. Once the translation is done, the machine-language program can be run any number of times, but of course it can only be run on one type of computer (since each type of computer has its own individual machine language).\u00ae If the program is to run on another type of computer it \u00aehas to be re-translated, using a different compiler, into the appropriate machine language. There is an alternative to compiling a high-level language program. Instead of using a compiler, which translates the program all at once, you can use an interpreter, which translates it instruction-by-instruction, as necessary. An interpreter is a program that acts much like a CPU, with a kind of fetch-and-execute cycle. \u00ae In order to execute a program, the interpreter runs in a loop in which it repeatedly reads one instruction from the \u00aeprogram, decides what is necessary to carry out that instruction, and then performs the appropriate machine-language commands to do so.\u00ae one use of interpreters is to execute high-level language programs. For example, the programming language Lisp is usually executed by an interpreter rather than a compiler. However, interpreters have another purpose: they can let you use a machine-language program meant for one type of computer on a completely different type of computer. For example, there is a program called \u201cVirtual PC\u201d that runs on Macintosh computers. \u00ae Virtual PC is an interpreter that executes machine-language programs written for IBM-PC-clone computers. If you run Virtual PC on your Macintosh, you can run any PC program, including programs written for Windows. (Unfortunately, a PC program will run much more slowly than it would on an actual IBM clone. The problem is that Virtual PC executes several Macintosh machine-language instructions for each PC machine-language instruction in the program it is interpreting. \u00ae Compiled programs are inherently faster than interpreted programs. The designers of Java chose to use a combination of compilation and interpretation. Programs written in Java are compiled into machine language, but it is a machine language for a computer that doesn\u2019t really exist. This so-called \u201cvirtual\u201d computer is known as the Java virtual machine. The machine language for the Java virtual machine is called Java bytecode. There is no reason why Java bytecode could not be used as the machine language of a real computer, rather than a virtual computer.\u00ae However, one of the main selling points of Java is that it can actually be used on any computer. All that the computer needs is an interpreter for Java bytecode. Such an interpreter simulates the Java virtual machine in the same way that Virtual PC simulates a PC computer. Of course, a different Java bytecode interpreter is needed for each type of computer, but once a computer has a Java bytecode interpreter, it can run any Java bytecode program. And the same Java bytecode program can be run on any computer that has such an interpreter. This is one of the essential features of Java: the same compiled program can be run on many different types of computers.\u00ae Why, you might wonder, use the intermediate Java bytecode at all? Why not just distribute the original Java program and let each person compile it into the machine language of whatever computer they want to run it on? There are many reasons. First of all, a compiler has to understand Java, a complex high-level language. The compiler is itself a complex program.\u00ae A Java bytecode interpreter, on the other hand, is a fairly small, simple program. This makes it easy to write a bytecode interpreter for a new type of computer; once that is done, that computer can run any compiled Java program. \u00aeIt would be much harder to write a Java compiler f\u00aeor the same computer. Fu\u00aerthermore, many Java programs are meant to be downloaded over a network. This leads to obvious security concerns: you don\u2019t want to download and run a program that will damage your computer or your files. The bytecode interpreter acts as a buffer between you and the program you download. You are really running the interpreter, which runs the downloaded program indirectly.\u00ae The interpreter can protect you from potentially dangerous actions on the part of that program. i should note that there is no necessary connection between Java and Java bytecode. A program written in Java could certainly be compiled into the machine language of a real computer. And programs written in other languages could be compiled into Java bytecode. However, it is the combination of Java and Java bytecode that is \u00aeplatform-independent, secure, and network compatible while allowing you to program in a modern high-level object-oriented language.\u00ae I should also note that the really hard part of platform-independence is providing a \u201cGraphical User Interface\u201d with windows, buttons, etc. that will work on all the platforms that support Java.",
    "page5": "Programs must be designed. No one can just sit down at the computer and compose a program of any complexity. The discipline called software engineering is concerned with the construction of correct, working, well-written programs. The software engineer tends to use accepted and proven methods for analyzing the problem to be solved and for designing a program to solve that problem. During the 1970s and into the 80s, the primary software engineering methodology was structured programming. The structured programming approach to program design was based on the following advice: To solve a large problem, break the problem into several pieces and work on each piece separately; to solve each piece, treat it as a new problem which can itself be broken down into smaller problems; eventually, you will work your way down to problems that can be solved directly, without further decomposition. This approach is called top-down programming. There is nothing wrong with top-down programming. It is a valuable and often-used approach to problem-solving. However, it is incomplete. For one thing, it deals almost entirely with producing the instructions necessary to solve a problem. But as time went on, people realized that the design of the data structures for a program was as least as important as the design of subroutines and control structures. Top-down programming doesn\u2019t give adequate considera\u00aetion to the data that the program manipulates. Another problem with strict top-down programming is that it makes it difficult to reuse work done for other projects. By starting with a particular problem and subdividing it into convenient pieces, top-down programming tends to produce a design that is unique to that problem. It is unlikely that you will be able to take a large chunk of programming from another program and fit it into your project, at least not without extensive modification. Producing high-quality programs is difficult and expensive, so programmers and the people who employ them are always eager to reuse past work. So, in practice, top-down design is often combined with bottom-up design. In bottom-up design, the approach is to start \u201cat the bottom,\u201d with problems that you already know how to solve (and for which you might already have a reusable software component at hand). From there, you can work upwards towards a solution to the overall problem. The reusable components should be as \u201cmodular\u201d as possible. A module is a component of a larger system that interacts with the rest of the system in a simple, well-defined, straightforward manner. The idea is that a module can be \u201cplugged into\u201d a system. The details of what goes on inside the module are not important to the system as a whole, as long as the module fulfills its assigned role correctly. This is called information hiding, and it is one of the most important principles of software engineering. One common format for software modules is to contain some data, along with some subroutines for manipulating that data. For example, a mailing-list module might contain a list of names and addresses along with a subroutine for adding a new name, a subroutine for printing mailing labels, and so forth. In such modules, the data itself is often hidden inside the module; a program that uses the module can then manipulate the data only indirectly, by calling the subroutines provided by the module. This protects the data, since it can only be manipulated in known, well-defined ways. And it makes it easier for programs to use the module, since they don\u2019t have to worry about the details of how the data is represented. Information about the representation of the data is hidden. Modules that could support this kind of information-hiding became common in programming languages in the early 1980s. Since then, a more advanced form of the same idea has more or less taken over software engineering. This latest approach is called object-oriented programming, often abbreviated as OOP. The central concept of object-oriented programming is the object, which is a kind of module containing data and subroutines. The point-of-view in OOP is that an object is a kind of self sufficient entity that has an internal state (the data it contains) and that can respond to messages (calls to its subroutines). A mailing list object, for example, has a state consisting of a list of names and addresses. If you send it a message telling it to add a name, it will respond by modifying its state to reflect the change. If you send it a message telling it to print itself, it will respond by printing out its list of names and addresses. The OOP approach to software engineering is to start by identifying the objects involved in a problem and the messages that those objects should respond to. The program that results is a collection of objects, each with its own data and its own set of responsibilities. The objects interact by sending messages to each other. There is not much \u201ctop-down\u201d in such a program, and people used to more traditional programs can have a hard time getting used to OOP. However, people who use OOP would claim that object-oriented programs tend to be better models of the way the world itself works, and that they are therefore easier to write, easier to understand, and more likely to be correct. You should think of objects as \u201cknowing\u201d how to respond to certain messages. Different objects might respond to the same message in different ways. For example, a \u201cprint\u201d message would produce very different results, depending on the object it is sent to. This property of objects that different objects can respond to the same message in different ways is called po\u00aelymorphism. It is common for objects to bear a kind of \u201cfamily resemblance\u201d to one another. Objects that contain the same type of data and that respond to the same messages in the same way belong to the same class. (In actual programming, the class is primary; that is, a class is created and then one or more objects are created\u00ae using that class as a template.) But objects can be similar without being in exactly the same class.",
    "page6": "For example, consider a drawing program that lets the user draw lines, rectangles, ovals, polygons, and curves on the screen. In the program, each visible object on the screen could be represented by a software object in the program. There would be five classes of objects in the program, one for each type of visible object that can be drawn. All the lines would belong to one class, all the rectangles to another class, and so on. These classes are obviously related; all of them represent \u201cdrawable objects.\u201d They would, for example, all presumably be able to respond to a \u201cdraw yourself\u201d message. Another level of grouping, based on the data  to represent each type of object, is less obvious, but would be very useful in a program: We can group polygons and curves together as \u201cmultipoint objects,\u201d while lines, rectangles, and ovals are \u201ctwo-point objects.\u201d (A line is determined by its endpoints, a rectangle by two of its corners, and an oval by two corners of the rectangle that contains it.) We could diagram these relationships as follows DrawableObject, MultipointObject, and TwoPointObject would be classes in the program. MultipointObject and TwoPointObject would be subclasses of DrawableObject. The class Line would be a subclass of TwoPointObject and (indirectly) of DrawableObject. A subclass of a class is said to inherit the properties of that class. The subclass can add to its inheritance and it can even \u201coverride\u201d part of that inheritance (by defining a different response to some method). Nevertheless, lines, rectangles, and so on are drawable objects, and the class DrawableObject expresses this relationship. Inheritance is a powerful means for organizing a program. It is also related to the problem of reusing software components. A class is the ultimate reusable component. Not only can it be reused directly if it fits exactly into a program you are trying to write, but if it just almost fits, you can still reuse it by defining a subclass and making only the small changes necessary to adapt it exactly to you\u00aer needs. So, OOP is meant to be both a superior program-development tool and a partial solution to the software reuse problem. Objects, classes, and object-oriented programming will be important themes throughout the rest of this text. The Modern User Interface - When computers were first introduced, ordinary people including most programmers couldn\u2019t get near them. They were locked up in rooms with white-coated attendants who would take your programs and data, feed them to the computer, and return the computer\u2019s response some time later. When timesharing where the computer switches its attention rapidly from one person to another was invented in the 1960s, it became possible for several people to interact directly with the computer at the same time. On a timesharing system, users sit at \u201cterminals\u201d where they type commands to the computer, and the computer types back its response. Early personal comp\u00aeuters also used typed commands and responses, except that there was only one person involved at a time. This type of interaction between a user and a computer is called a command-line interface. Today, of course, most people interact with computers in a completely different way. They use a Graphical User Interface, or GUI. The computer draws interface components on the screen. The components include things like windows, scroll bars, menus, buttons, and icons. Usually, a mouse is used to manipulate such components. Assuming that you have not just been teleported in from the 1970s, you are no doubt already familiar with the basics of graphical user interfaces! A lot of GUI interface components have become fairly standard. That is, they have similar appearance and behavior on many different computer platforms including Macintosh, Windows, and Linux. Java programs, which are supposed to run on many different platforms without modification to the program, can use all the standard GUI components. They might vary a little in appearance from platform to platform, but their functionality should be identical on any computer on which the program runs. Shown below is an image of a very simple Java program actually an \u201capplet\u201d, since it is meant to app\u00aeear on a Web page that shows a few standard GUI interface components. There are four components that the user can interact with: a button, a checkbox, a text field, and a pop-up menu. These components are labeled. There are a few other components in the applet. The labels themselves are components (even though you can\u2019t interact with them). The right half of the applet is a text area component, which can display multiple lines of text, and a scrollbar component appears alongside the text area when the number of lines of text becomes larger than will fit in the text area. And in fact, in Java terminology, the whole applet is itself considered to be a \u201ccomponent. \u201dNow, Java actually has two complete sets of GUI components. One of these, the AWT or Abstract Windowing Toolkit, was available in the original version of Java. The other, which is known as Swing, is included in Java version 1.2 or later, and is used in preference to the AWT in most modern Java programs. The applet that is shown above uses components that are part of Swing. If your Web browser uses an old version of Java, you might get an error when the browser tries to load the applet. Remember that most of the applets in this textbook require Java 5.0 (or higher). When a user interacts with the GUI components in this applet, an \u201cevent\u201d is generated. For example, clicking a push button generates an event, and pressing return while typing in a text field generates an event. Each time an event is generated, a message is sent to the applet telling it that the event has occurred, and the applet responds according to its program. In fact, the program consists mainly of \u201cevent handlers\u201d that tell the applet how to respond to various types of events. In this example, the applet has been programmed to respond to each event by displaying a message in the text area.",
    "page7": "The use of the term \u201cmessage\u201d here is deliberate. Messages, as you saw in the previous section, are sent to\u00ae objects. In fact, Java GUI components are implemented as objects. Java includes many predefined classes that represent various types of GUI components. Some of these classes are subclasses of others. Here is a diagram showing some of Swing\u2019s GUI classes and their relationships. Don\u2019t worry about the details for now, but try to get some feel about how object-oriented programming and inheritance are used here. Note that all the GUI classes are subclasses, directly or indirectly, of a class called JComponent, which represents general properties that are shared by all Swing components. Two of the direct subclasses of JComponent themselves have subclasses. The classes JTextArea and JTextField, which have certain behaviors in common, are grouped together as subclasses of JTextComponent. Similarly JButton and JToggleButton are subclasses of JAbstractButton, which represents properties common to both buttons and checkboxes. (JComboBox, by the way, is the Swing class that represents pop-up menus.) Just from this brief discussion, perhaps you can see how GUI programming can make effective use of object-oriented design. In fact, GUI\u2019s, with their \u201cvisible objects,\u201d are probably a major factor contributing to the popularity of OOP. The Internet and the World-Wide Web - Computers can be connected together on networks. A computer on a network can communicate with other computers on the same network by\u00ae exchanging data and files or by sending and receiving messages. Computers on a network can even work together on a large computation. Today, millions of computers throughout the world are connected to a single huge network called the Internet. New computers are being connected to the Internet every day.\u00ae Computers can join the Internet by using a modem to establish a connection through telephone lines. Broadband connections to the Internet, such as DSL and cable modems, are increasingly common. They allow faster data transmission than is possible through telephone modems. There are elaborate protocols for communication over the Internet. A protocol is simply a detailed specification of how communication is to proceed. For two computers to communicate at all, they must both be using the same protocols. The most basic protocols on the Internet are the Internet Protocol (IP), which specifies how data is to be physically transmitted from one computer to another, and the Transmission Control Protocol (TCP), which ensures that data sent using IP is received in its entirety and without error. These two protocols, which are referred to collectively as TCP/IP, provide a foundation for communication. Other protocols use TCP/IP to send specific types of information such as web pages, electronic mail, and data files. All communication over the Internet is in the form of packets. A packet consists of some data being sent from one computer to another, along with addressing information that indicates where on the Internet that data is supposed to go. Think of a packet as an envelope with an address on the outside and a message on the inside. (The message is the data.) The packet also includes a \u201creturn address,\u201d that is, the address of the sender. A packet can hold only a limited amount of data; longer messages must be divided among several packets, which are then sent individually over the net and reassembled at their destination. Every computer on the Internet has an IP address, a number that identifies it uniquely among all the computers on the net. The IP address is used for addressing packets. A computer can only send data to another computer on the Internet if it knows that computer\u2019s IP address. Since people prefer to use names rather than numbers, most computers are also identified by names, called domain names. For example, the main computer of the Mathematics Department at Hobart and William Smith Colleges has the domain name math.hws.edu. (Domain names are just for convenience; your computer still needs to know IP addresses before it can communicate. There are computers on the Internet whose job it is to translate domain names to IP addresses. When you use a domain name, your computer sends a message to a domain name server to find out the corresponding IP address. Then, your computer uses the IP address, rather than the domain name, to communicate with the other computer.) The Internet provides a number of services to the computers connected to it (and, of course, to the users of those computers). These services use TCP/IP to send various types of data over the net. Among the most popular services are instant messaging, file sharing, electronic mail, and the World-Wide Web. Each\u00ae service has its own protocols, which are used to control transmission of data over the network. Each service also has some sort of user interface, which allows the user to view, send, and receive data through the service. For example, the email service uses a protocol known as SMTP (Simple Mail Transfer Protocol) to transfer email messages from one computer to another. Other protocols, such as POP and IMAP, are used to fetch messages from an email account so that the recipient can read them. A person who uses email, however, doesn\u2019t need to understand or even know about these protocols. Instead, they are used behind the scenes by the programs that the person uses to send and receive email messages. These programs provide an easy-to-use user interface to the underlying network protocols. The World-Wide Web is perhaps the most exciting of network services. The World-Wide Web allows you to request pages of information that are stored on computers all over the Internet. A Web page can contain links to other pages on the same computer from which it was obtained or to other computers anywhere in the world. A computer that stores such pages of information is called a web server. The user interface to the Web is the type of program known as a web browser. Common web browsers include Internet Explorer and Firefox. You use a Web browser to request a page of information. The browser will send a request for that page to the computer on which the page is stored, and when a response is received from that computer, the web browser displays it to you in a neatly formatted form. A web browser is just a user interface to the Web. Behind the scenes, the web browser uses a protocol called HTTP (HyperText Transfer Protocol) to send each page request and to receive the response from the web server.",
    "page8": "On a basic level (the level of machine language), a computer can perform only very simple operations. A computer performs complex tasks by stringing together large numbers of such operations. Such tasks must be \u201cscripted\u201d in complete and perfect detail by programs. Creating complex programs will never be really easy, but the difficulty can be handled to some extent by giving the program a clear overall structure. The design of the overall structure of a program is what I call \u201cprogramming in the large.\u00ae\u201d Programming in the small, which is sometimes called coding, would then refer to filling in the details of that design. The details are the explicit, step-by-step instructions for performing fairly small-scale tasks. When you do coding, you are working fairly \u201cclose to the machine,\u201d with some of the same concepts that you might use in machine language: memory locations, arithmetic operations, loops and branches. In a high-level language such as Java, you get to work with these concepts on a level several steps above machine language.\u00ae However, you still have to worry about getting all the details exactly right. This chapter and the next examine the facilities for programming in the small in the Java programming language. Don\u2019t be misled by the term \u201cprogramming in the small\u201d into thinking that this material is easy or unimportant. This material is an essential foundation fo\u00aer all types of programming. If you don\u2019t understand it, you can\u2019t write programs, no matter how good you get at designing their large-scale structure. The Basic Java Application - A program is a sequence of instructions that a computer can execute to perform some task. A simple enough idea, but for the computer to make any use of the instructions, they must be written in a form that the computer can use. This means that programs have to be written in programming languages. Programming languages differ from ordinary human languages in being completely unambiguous and very strict about what is and is not allowed in a program. The rules that determine what is allowed are called the syntax of the language. Syntax rules specify the basic vocabulary of the language and how programs can be constructed using things like loops, branches, and subroutines. A syntactically correct program is one that can be successfully compiled or interpreted; programs that have syntax errors will be rejected (hopefully with a useful error message that will help you fix the problem). So, to be a successful programmer, you have to develop a detailed knowledge of the syntax of the programming language that you are using. However, syntax is only part of the story. It\u2019s not enough to write a program that will run you want a program that will run and produce the correct result! That is, the meaning of the program has to be right. The meaning of a program is referred to as its semantics. A semantically correct program is one that does what you want it to. Furthermore, a program can be syntactically and semantically correct but still be a pretty bad program. Using the language correctly is not the same as using it well. For example, a good program has \u201cstyle.\u201d It is written in a way that will make it easy for people to read and to understand. It follows conventions that will be familiar to other programmers. And it has an overall design that will make sense to human readers. The computer is completely oblivious to such things, but to a human reader, they are paramount. These aspects of programming are sometimes referred to as pragmatics. When I introduce a new language feature, I will explain the syntax, the semantics, and some of the pragmatics of that feature. You should memorize the syntax; that\u2019s the easy part. Then you should get a feeling for the semantics by following the examples given, making sure that you understand how they work, and maybe writing short programs of your own to test your understanding. And you should try to appreciate and absorb the pragmatics this means learning how to use the language feature well, with sty\u00aele that will earn you the admiration of other programmers. Of course, even when you\u2019ve become familiar with all the individual features of the language, that doesn\u2019t make you a programmer. You still have to learn how to construct complex programs to solve particular problems. For that, you\u2019ll need both experience and taste. You\u2019ll find hints about software development throughout this textbook. We begin o\u00aeur exploration of Java with the problem that has become traditional for such beginnings: to write a program that displays the message \u201cHello World!\u201d. This might seem like a trivial problem, but getting a computer to do this is really a big first step in learning a new programming language (especially if it\u2019s your first programming language). It means that you understand the basic process of: 1. getting the program text into the computer, 2. compiling the program, and 3. running the compiled program. The first time through, each of these steps will probably take you a few tries to get right. I won\u2019t go into the details here of how you do each of these steps; it depends on the particular computer and Java programming environment that you are using. See Section 2.6 for information about creating and running Java programs in specific programming environments. But in general, you will type the program using some sort of text editor and save the program in a file. Then, you will use some command to try to compile the file. You\u2019ll either get a message that the program contains syntax errors, or you\u2019ll get a compiled version of the program. In the case of Java, the program is compiled into Java bytecode, not into machine language. Finally, you can run the compiled program by giving some appropriate command. For Java, you will actually use an interpreter to execute the Java bytecode. Your programming environment might automate some of the steps for you, but you can be sure that the same three steps are being done in the background.",
    "page9": "Here is a Java program to display the message \u201cHello World!\u201d. Don\u2019t expect to understand what\u2019s going on here just yet some of it you won\u2019t really understand until a few chapters from now: // A program to display the message // \\\"Hello World!\\\" on standard output public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello World!\\\"); } } // end of class HelloWorld The command that actually displays the message is: System.out.println(\\\"Hello World!\\\"); This command is an example of a subroutine call statement. It uses a \u201cbui\u00aelt-in subroutine\u201d named System.out.println to do the actual work. Recall that a subroutine consists of the instructions for performing some task, chunked together and given a name. That name can be used to \u201ccall\u201d the subroutine whenever that task needs to be performed. A built-in subroutine is one that is already defined as part of the language and therefore automatically available for use in any program. When you run this program, the message \u201cHello World!\u201d (without the quotes) will be displayed on standard output. Unfortunately, I can\u2019t say exactly what that means! Java is meant to run on many different platforms, and standard output will mean different things on different platforms. However, you can expect the message to show up in some convenient place. (If you use a command-line in\u00aeterface, like that in Sun Microsystem\u2019s Java Development Kit, you type in a command to tell the computer to run the program. The computer will type the output from the program, Hello World!, on the next line.) You must be curious about all the other stuff in the above program. Part of it consists of comments. Comments in a pr\u00aeogram are entirely igno\u00aered by the computer; they are there for human readers only. This doesn\u2019t mean that they are unimportant. Programs are meant to be read by people as well as by computers, and without comments, a program can be very difficult to\u00ae understand. Java has two types of comments. The first type, used in th\u00aee above program, begins with // and extends to the end of a line. The computer ignores the // and everything that follows it on the same line. Java has another style of comment that can extend over many lines. That type of comment begins with /* and ends with */. Everything else i\u00aen the program is required by the rules of Java syntax. All programming in Java is done inside \u201cclasses.\u201d The first line in the above program (not counting the comments) says that this is a class named HelloWorld. \u201cHelloWorld,\u201d the name of \u00aethe class, also serves as the name of the program. Not every class is a program. In order to define a program, a class must include a subroutine named main, with a definition that takes the form public static void main(String[] args) { (statements) } When you tell the Java interpreter to run the program, the interpreter calls the main() subroutine, and the statements that it contains are executed. These statements make up the script that tells the computer exactly what to do when the program is executed. The main() routine can call subroutines that are defined in the same class or even in other classes, but it is the main() routine that determines how and in what order the other subroutines are used. The word \u201cpublic\u201d in the first line of main() means that this routine can be called from outside the program. This is essential because the main() routine is called by the Java interpreter, which is something external to the program itself. The remainder of the first line of the routine is harder to explain at the moment; for now, just think of it as part of the required syntax. The definition of the subroutine that is, the instructions that say what it does consists of the sequence of \u201cstatements\u201d enclosed between braces, { and }. Here, I\u2019ve used statements as a placeholder for the actual statements that make up the program. Throughout this textbook, I will always use a similar format: anything that you see in this style of text (italic in angle brackets) is a placeholder that describes something you need to type when you write an actual program. As noted above, a subroutine can\u2019t exist by itself. It has to be part of a \u201cclass\u201d. A program is defined by a public class that takes the form. public class hprogram-namei { hoptional-variable-declarations-and-subroutinesi public static void main(String[] args) { statements } optional-variable-declarations-and-subroutines } The name on the first line is the name of the program, as well as the name of the class. If the name of the class is HelloWorld, then the class must be saved in a file called HelloWorld.java. When this file is compiled, another file named HelloWorld.class will be produced. This class file, HelloWorld.class, contains the Java bytecode that is exec\u00aeuted by a Java interpreter. HelloWorld.java is called the source code for the program. To execute the program, y\u00aeou only need the compiled class file, not the source code. The layout of the program on the page, such as the use of blank lines and indentation, is not part of the syntax or semantics of the language. The computer doesn\u2019t care about layout you could run the entire program together on one line as far as it is concerned. However, layout is important to human readers, and there are certain style guidelines for layout that are followed by most programmers. These style guidelines are part of the pragmatics of the Java programming language.",
    "page10": "Variables and the Primitive Types - Names are fundamental to programming. In programs, names are used to refer to many different sorts of things. In order to use those things, a programmer must understand the rules for giving names to things and the rules for using the names to work with those things. That is, the programmer must understand the syntax and the semantics of names. According to the syntax rules of Java, a name is a sequence of one or more characters. It must begin with a letter or underscore and must consist entirely of letters, digits, and underscores. (\u201cUnderscore\u201d refers to the character \u2019 \u2019.) For example, here are some legal names: No spaces are allowed in identifiers; HelloWorld is a legal identifier, but \u201cHello World\u201d is not. Upper case and lower case letters are considered to be different, so that HelloWorld, helloworld, HELLOWORLD, and hElloWorLD are all distinct names. Certain names are reserved for special uses in Java, and cannot be used by the programmer for other purposes. These reserved words include: class, public, static, if, else, while, and several dozen other words. Java is actually pretty liberal about what counts as a letter or a digit. Java uses the Unicode character set, which includes thousands of characters from many different languages and different alphabets, and many of these characters count as letters or digits. However, I will be sticking to what can be typed on a regular English keyboard. The pragmatics of naming includes style guidelines about how to choose names for things. For example, it is customary for names of classes to begin with upper case letters, while names of variables and of subroutines begin with lower case letters; you can avoid a lot of confusion by following the same convention in your own programs. Most Java programmers do not use underscores in names, although some do use them at the beginning of the names of certain kinds of variables. When a name is made up of several words, such as HelloWorld or interestRate, it is customary to capitalize each word, except possibly the first; this is sometimes referred to as camel case, since the upper case letters in the middle of a name are supposed to look something like the humps on a camel\u2019s back. Finally, I\u2019ll note that things are often referred to by compound names which consist of several ordinary names separated by periods. (Compound names are also called qualified names.) You\u2019ve already seen an example: System.out.println. The idea here is that things in Java can contain other things. A compound name is a kind of path to an item through one or more levels of containment. The name System.out.println indicates that something called \u201cSystem\u201d contains something called \u201cout\u201d which in turn contains something called \u201cprintln\u201d. Non-com\u00aepound names are called simple identifiers. I\u2019ll use the term identifier to refer to any name simple or compound that can be used to refer to something in Java. (Note that the reserved words are not identifiers, since they can\u2019t be used as names for things.) Variables - Programs manipulate data that are stored in memory. In machine language, data can only be referred to by giving the numerical address of the location in memory where it is stored. In a high-level language such as Java, names are used instead of numbers to refer to data. It is the job of the computer to keep track of where in memory the data is actually stored; the programmer only\u00ae has to remember the name. A name used in this way to refer to data stored in memory is called a variable. Variables are actually rather subtle. Properly speaking, a variable is not a name for the data itself but for a location in memory that can hold data. You should think o\u00aef a variable as a container or box where you can store data that you will need to use later. The variable refers directly to the box and only indirectly to the data in the box. Since the data in the box can change, a variable can refer to different data values at different times during the execution of the program, but it always refers to the same box. Confusion ca\u00aen arise, especially for beginning programmers, because when a variable is used in a program in certain ways, it refers to the container, but when it is used in other ways, it refers to the data in the container. You\u2019ll see examples of both cases below. (In this way, a variable is something like the title, \u201cThe President of the United States.\u201d This title can refer to different people at different times, but it always refers to the same office. If I say \u201cthe President went fishing,\u201d I mean that George W. Bush went fishing. But if I say \u201cHillary Clinton wants to be President\u201d I mean that she wants to fill the office, not that she wants to be George Bush.) In Java, the only way to get data into a variable that is, into the box that the variable names is with an assignment statement. An assignment statement takes the form: (variable) = (expression); where expression represents anything that refers to or computes a data value. When the computer comes to an assignment statement in the course of executing a program, it evaluates the expression and puts the resulting data value into the variable. For example, consider the simple assignment statement rate = 0.07; The variable in this assignment statement is rate, and the expression is the number 0.07. The computer executes this assignment statement by putting the number 0.07 in the variable rate, replacing whatever was there before. Now, consider the following more complicated assignment statement, which might come later in the same program.",
    "page11": "Types and Literals- A variable in Java is designed to hold only one particular type of data; it can legally hold that type of data and no other. The compiler will consider it to be a syntax error if you try to violate thi\u00aes rule. We say that Java is a strongly typed language because it enforces this rule. There are eight so-called primitive types built into Java. The primitive types are named byte, short, int, long, float, double, char, and boolean. The first four types hold integers (whole numbers such as 17, -38477, and 0). The four integer types are distinguished by the ranges of integers they can hold. The float and double types hold real numbers (such as 3.6 and -145.99). Again, the two real types are distinguished by their range and accuracy. A variable of type char holds a single character from the Unicode character set. And a variable of type boolean holds one of the two logical values true or false. Any data value stored in the computer\u2019s memory must be represented as a binary number, that is as a string of zeros and ones.\u00ae A single zero or one is called a bit. A string of eight bits is called a byte. Memory is usually measured in terms of bytes. Not surprisingly, the byte data type refers to a single byte of memory. A variable of type byte holds a string of eight bits, which can represent any of the integers between -128 and 127, inclusive. (There are 256 integers in that range; eight bits can represent 256 two raised to the power eight different values.) As for the other integer types, short corresponds to two bytes (16 bits). Variables of type short have values in the range -32768 to 32767. int corresponds to four bytes (32 bits). Variables of type int have values in the range -2147483648 to 2147483647. long corresponds to eight bytes (64 bits). Variables of type long have values in the range -9223372036854775808 to 9223372036854775807. You don\u2019t have to remember these numbers, but they do give you some idea of the size of integers that you can work with. Usually, you should just stick to the int data type, which is good enough for most purposes. The float data type is represented in four bytes of memory, using a standard method for encoding real numbers. The maximum value for a float is about 10 raised to the power 38. A float can have about 7 significant digits. (So that 32.3989231134 and 32.3989234399 would both have to be rounded off to about 32.398923 in order to be stored in a variable of type float.) A double takes up 8 bytes, can range up to about 10 to the power 308, and has about 15 significant digits. Ordinarily, you should stick to the double type for real values. A variable of type char occupies two bytes in memory. The value of a char variable is a single character such as A, *, x, or a space character. The value can also be a special character such a tab or a carriage return or one of the many Unicode characters that come from different languages. When a character is typed into a program, it must be surrounded by single quotes; for example: \u2019A\u2019, \u2019*\u2019, or \u2019x\u2019. Without the quotes, A would be an identifier and * would be a multiplication operator. The quotes are not part of the value and are not stored in the variable; they are just a convention for naming a particular character constant in a program. A name for a constant value is called a literal. A literal is what you have to type in a program to represent a value. \u2019A\u2019 and \u2019*\u2019 are literals of type char, representing the character values A and *. Certain special characters have special literals that use a backslash, \\, as an \u201cescape character\u201d. In particular, a ta\u00aeb is represented as \u2019t\u2019, a carriage return as \u2019r\u2019, a linefeed as \u2019 \u2019, the single quote character as , and the backslash itself as \u2019\\\u2019. Note that even though you type two characters between the quotes in \u2019t\u2019, the value represented by this literal is a single tab character. Numeric literals are a little more complicated than you might expect. Of course, there are the obvious literals such as 317 and 17.42. But there are other possibilities for expressing numbers in a Java program. First of all, real numbers can be represented in an exponential form such as 1.3e12 or 12.3737e-108. The \u201ce12\u201d and \u201ce-108\u201d represent powers of 10, so that 1.3e12 means 1.3 times 1012 and 12.3737e-108 means 12.3737 times 10\u2212108. This format can be used to express very large and very small numbers. Any numerical literal that contains a decimal point\u00ae or exponential is a literal of type double. To make a literal of type float, you have to append an \u201cF\u00ae\u201d or \u201cf\u201d to the end of the number. For example, \u201c1.2F\u201d stands for 1.2 considered as a value of type float. (Occasionally, you need to know this because the rules of Java say that you can\u2019t assign a value of type double to a vari\u00aeable of type float, so you might be confronted with a ridiculous-seeming error message if you try to do something like \u201cx = 1.2;\u201d when x is a variable of type float. You have to say \u201cx = 1.2F;. This is one reason why I advise sticking to type double for real numbers.) Even for integer literals, there are some complications. Ordinary integers such as 177777 and -32 are literals of type byte, short, or int, depending on their size. You can make a literal of type long by adding \u201cL\u201d as a suffix. For example: 17L or 728476874368L. As another complication, Java allows octal (base-8) and hexadecimal (base-16) literals. I don\u2019t want to cover base-8 and base-16 in deta\u00aeil, but in case you run into them in other people\u2019s programs, it\u2019s worth knowing a few things: Octal numbers use only the digits 0 through 7. In Java, a numeric literal that begins with a 0 is interpreted as an octal number; for example, the literal 045 represents the number 37, not the number 45. Hexadecimal numbers use 16 digits, the usual digits 0 through 9 and the letters A, B, C, D, E, and F. Upper case and lower case letters can be used interchangeably in this context. The letters represent the numbers 10 through 15. In Java, a hexadecimal literal begins with 0x or 0X, as in 0x45 or 0xFF7A. Hexadecimal numbers are also used in character literals to represent arbitrary Unicode characters. A Unicode literal consists of u followed by four hexadecimal digits. For example, the character literal \u2019u00E9\u2019 represents the Unicode character that is an \u201ce\u201d with an acute accent.",
    "page12": "Variables in Programs- A varia\u00aeble can be \u00aeused in a program only if it has first been declared. A variable declaration statement is used to declare one or more variables and to give them names. When the computer executes a variable declaration, it sets aside memory for the variable and associates the variable\u2019s name with that memory. A simple variable declaration takes the form. (type-name) (variable-name-or-names); The hvariable-name-or-namesi can be a single variable name or a list of variable names separated by commas. (We\u2019ll see later that variable declaration statements can actually be somewhat more complicated than this.) Good programming style is to declare only one variable in a declaration statement, unless the variables are closely related in some way. For example: int numberOfStudents; String name; double x, y; boolean isFinished; char firstInitial, middleInitial, lastInitial; It is also good style to include a comment with each variable declaration to explain its purpose in the program, or to give other information that might be useful to a human reader. in this chapter, we will only use variables declared inside the main() subroutine of a program. Variables declared inside a subroutine are called local variables for that subroutine. They exist only inside the subroutine, while it is running, and are completely inaccessible from outside. Variable declarations can occur anywhere inside the subroutine, as long as each variable is declared before it is used in any expression. Some people like to declare all the variables at the beginning of the subroutine. Others like to wait to declare a variable until it is needed. My preference: Declare important variables at the beginning of the subroutine, and use a comment to explain the purpose of each variable. Declare \u201cutility variables\u201d which are not important to the overall logic of the subroutine at the point in the subroutine where they are first used. Strings, Objects, Enums, and Subroutines- The previous section introduced the eight primitive data types and the type String. There is a fundamental difference between the primitive types and the String type: Values of type St\u00aering are objects. While we will not study objects in detail until Chapter 5, it will be useful for you to know a little about them and about a closely related topic: classes. This is not \u00aejust because strings are useful but because objects and classes are essential to understanding another important programming concept, subroutines. Another reason for considering classes and objects at this point is so that we can introduce enums. An enum is a data type that can be created by a Java programmer to represent a small collection of possible values. Technically, an enum is a class and its possible values are objects. Enums will be our first example of adding\u00ae a new type to the Java language. We will look at them later in this section. Built-in Subroutines and Functions- Recall that a subroutine is a set of program instructions that have been chunked together and given a name. In Chapter 4, you\u2019ll learn how to write your own subroutines, but you c\u00aean get a lot done in a program just by calling subroutines that have already been written for you. In Java, every \u00aesubroutine is contained in a class or in an object. Some classes that are standard parts of the Java language contain predefined subroutines that you can use. A value of type String, which is an object, contains subroutines that can be used to manipulate that string. These subroutines are \u201cbuilt into\u201d the Java language. You can call all these subroutines without understanding how they were written or how they work. Indeed, that\u2019s the whole point of subroutines: A subroutine is a \u201cblack box\u201d which can be used without knowing what goes on inside. Classes in Java have two very different functions. First of all, a class can group together variables and subroutines that are contained in that class. These variables and subroutines are called static members of the class. You\u2019ve seen one example: In a class that defines a program, the m\u00aeain() routine is a static member of the class. The parts of a class definition that define static members are marked with the reserved word \u201cstatic\u201d, just like the main() routine of a program. However, classes have a second function. They are used to describe objects. In this role, the class of an object specifies what subroutines and variables are contained in that object. The class is a type in the technical sense of a specification of a certain type of data value and the object is a value of that type. For example, String is actually the name of a class that is included as a standard part of the Java language. String is also a type, and literal strings such as \\\"Hello World\\\" represent values of type String. So, every subroutine is contained either in a class or in an object. Classes contain subroutines called static member subroutines. Classes also describe objects and the subroutines that are contained in those objects. This dual use can be confusing, and in practice most classes are designed to perform primarily or exclusively in only one of the two possible roles. For example, although the String class does contain a few rarely-used static member subroutines, it exists mainly to specify a large number of subroutines that are contained in objects of type String. Another standard class, named Math, exists entirely to group together a number of static member subroutines that compute various common mat\u00aehematical functions.",
    "page13": "To begin to get a handle on all of this complexity\u00ae, let\u2019s look at the subroutine System.out.print as an example. As you have seen earlier in this chapter, this subroutine is used to display information to the user. For example, System.out.print(\\\"Hello World\\\") displays the message, Hello World. System is one of Java\u2019s standard classes. One of the static member variables in this class is named out. Since this variable is contained in the class System, its full name which you have to use to refer to it in your programs is System.out. The variable System.out refers to an object, and that object in turn contains a subroutine named print. The compound identifier System.out.print refers to the subroutine print in the object out in the class System. (As an aside, I will note that the object referred to by System.out is an object of the class PrintStream. PrintStream is another class that is a standard part of Java. Any object of type PrintStream is a destination to which information can be printed; any object of type PrintStream has a print subroutine that can be used to send information to that destination. The object System.out is just one possible destination, and System.out.print is the subroutine that sends information to that particular destination. Other objects of type PrintStream might send information to other destinations such as files or across a network to other computers. Thi\u00aes is object-oriented programming: Many different things which have something in common they can all be used as destinations for information can all be used in the same way through a print\u00ae subroutine. The PrintStream class expresses the commonalities among all these objects.) Since class names and variable names are used in similar ways, it might be hard to tell which is which. Remember that all the built-in, predefined names in Java follow the rule that class names begin with an upper case letter while variable names begin with a lower case letter. While this is not a formal syntax rule, I recommend that you follow it in your own programming. Subroutine names should also begin with lower case letters. There i\u00aes no possibility of confusing a variable with a subroutine, since a subroutine name in a program is always followed by a left parenthesis. (As one final general note, you should be aware that subroutines in Java are often referred to as methods. Generally, the term \u201cmethod\u201d means a subroutine that is contained in a class or in an object. Since this is true of every subroutine in Java, every subroutine in Java is a method. The same is not true for other programming languages. Nevertheless, the term \u201cmethod\u201d is mostly used in the context of object-oriented programming, and until we start doing real object-oriented programming in Chapter 5, I will prefer to use the more general term, \u201csubroutine.\u201d) Classes can contain static member subroutines, as well as static member variables. For example, the System clas\u00aes contains a subroutine named exit. In a program, of course, this subroutine must be referred to as System.exit. Calling this subroutine will terminate the program. You could use\u00ae it if you had some reason to terminate the program before the end of the main routine. For historical reasons, this subroutine takes an integer as \u00aea parameter, so the subroutine call statement might look like \u201cSystem.exit(0);\u201d or \u201cSystem.exit(1);\u201d. (The parameter tells the computer why the program wa\u00aes terminated. A parameter value of 0 indicates that the program ended normally. Any other value indicates that the program was terminated because an error was detected. But in practice, the value of the parameter is usually ignored.) Every subroutine performs some specific task. For some subroutines, that task is to compute or retrieve some data value. Subroutines of this type are called functions. We say that a function returns a value. The returned value must then be used somehow in the program. You are familiar with the mathematical function that computes the square root of a number. Java has a corresponding function called Math.sqrt. This function is a static member subroutine of the \u00aeclass named Math. If x is any numerical value, then Math.sqrt(x) computes and returns the square root of that value. Since Math.sqrt(x) represents a value, it doesn\u2019t make sense to put it on a line by itself in a subroutine call statement such as Math.sqrt(x); // This doesn\u2019t make sense! What, after all, would the computer do with the value computed by the function in this case? You have to tell the computer to do something with the value. You might tell the computer to display it: System.out.print( Math.sqrt(x) ); // Display the square root of x. or you might use an assignment statement to tell the computer to store that value in a variable:",
    "page14": "The function call Math.sqrt(x) represents a value of type double, and it can be used anyplace where a numeric literal of type double could be used. The Math class contains many static member functions. Here is a list of some of the more important of them: Math.abs(x), which computes the absolute value of x. The usual trigonometric functions, Math.sin(x), Math.cos(x), and Math.tan(x). (For all the trigonometric functions, angles are measured in radians, not degrees.) The inverse trigonometric functions arcsin, arccos, and arctan, which are written as: Math.asin(x), Math.acos(x), and Math.atan(x). The return value is expressed in radians, not degrees. The exponential function Math.exp(x) for computing the number e raised to the power x, and the natural logarithm function Math.log(x) for computing the logarithm of x in the base e. Math.pow(x,y) for computing x raised to the power y. Math.floor(x), which rounds x down to the nearest integer value that is less than or equal to x. Even though the return value is mathematically an integer, it is returned as a value of type double, rather than of type int as you might expect. For example, Math.floor(3.76) is 3.0. The function Math.round(x) returns the integer that is closest to x. Math.random(), which returns a randomly chosen double in the range 0.0 <= Math.random() < 1.0. (The computer actually calculates so-called \u201cpseudorandom\u201d numbers, which are not truly random but are random enough for most purposes.) For these functions, the type of the parameter the x or y inside the parentheses can be any value of any numeric type. For most of the functions, the value returned by the function is of type double no matter what the type of the parameter. However, for Math.abs(x), the value returned will be the same type as x; if x is of type int, then so is Math.abs(x). So, for example, while Math.sqrt(9) is the double value 3.0, Math.abs(9) is the int value 9. Note that Math.random() does not have any parameter. You still need the parentheses, even though there\u2019s nothing between them. Th\u00aee parentheses let the computer know that this is a subroutine rather than a variable. Another \u00aeexample of a subroutine that has no parameters is the function System.currentTimeMillis(), from the System class. When this function is executed, it retrieves the current time, expressed as the number of milliseconds that have passed since a \u00aestandardized base time (the start of the year 1970 in Greenwich Mean Time, if you care). One millisecond is one-thousandth of a second. The return value of System.currentTimeMillis() is of type long. This function can be used to measure the time that it takes the computer to perform a task. Just record the time at which the task is begun and the time at which it is finished and take the difference. Here is a sample program that performs a few mathematical tasks and reports the time that it takes for the program to run. On some computers, the time reported might be zero, because it is too small to measure in milliseconds. Even if it\u2019s not zero, you can be sure that most of the time reported by the computer was spent doing output or working on tasks other than the program, since the calculations performed in this pr\u00aeogram occupy only a tiny fraction of a second of a computer\u2019s time. Operations on Strings- A value of type String is an object. That object contains data, namely the sequence of characters that make up the string. It also contains subroutines. All of these subroutines are in fact functions. For example, every string object contains a function named length that computes the number of characters in that string. Suppose that advice is a variable that refers to a String. For example, advice might have been declared and assigned a value as follows: String advice; advice = \\\"Seize the day!\\\"; Then advice.length() is a function call that returns the number of characters in the string \u201cSeize the day!\u201d. In this case, the return value would be 14. In general, for any string variable str, the value of str.length() is an int equal to the number of characters in the string that is the value of str. Note that this function has no parameter; the particular string whose length is being computed is the value of str. The length subroutine is defined by the class String, and it can be used with any value of type String. It can even be used with String literals, which are, after all, just constant values of type String. For example, you could have a program count the characters in \u201cHello World\u201d for you by saying System.out.print(\\\"The number of characters in \\\"); System.out.println(\\\"the string \"Hello World\" is \\\"); System.out.println( \\\"Hello World\\\".length() ); The String class defines a lot of functions. Here are some that you might find useful. Assume that s1 and s2 refer to values of type String:s1.equals(s2) is a function that returns a boolean value. It returns true if s1 consists of exactly the same sequence of characters as s2, and returns false otherwise. s1.equalsIgnoreCase(s2) is another boolean-valued function that checks whether s1 is the same string as s2, but this function considers upper and lower case letters to be equivalent. Thus, if s1 is \u201ccat\u201d, then s1.equals(\\\"Cat\\\") is false, while s1.equalsIgnoreCase(\\\"Cat\\\") is true. s1.len\u00aegth(), as mentioned above, is an integer-valued function that gives the number of characters in s1. s1.length(), as mentioned above, is an integer-valued function that gives the number of characters in s1. s1.charAt(N), where N is an integer, returns a value of type char. It returns the Nth character in the string. Positions are numbered starting with 0, so s1.charAt(0) is actually the first character, s1.charAt(1) is the second, and so on. The final position is s1.length() - 1. For example, the value of \\\"cat\\\".charAt(1) is \u2019a\u2019. An error occurs if the value of the parameter is less than zero or greater than s1.length() - 1.",
    "page15": "s1.substring(N,M), where N and M are integers, returns a value of type String. The returned value consists of the characters in s1 in positions N, N+1,. . . , M-1. Note that the character in position M is not included. The returned value is called a substring of s1. \u2022 s1.indexOf(s2) returns an integer. If s2 occurs as a substring of s1, then the returned value is the starting position of that substring. Otherwise, the returned value is -1. You can also use s1.indexOf(ch) to search for a particular character, ch, in s1. To find the first occurrence of x at or after position N, you can use s1.indexOf(x,N). \u2022 s1.compareTo(s2) is an integer-valued function that compares the two strings. If the strings are equal, the value returned is zero. If s1 is less than s2, the value returned is a number less than zero, and if s1 is greater than s2, the value returned is some number greater than zero. (If both of the strings consist entirely of lower case letters, then \u201cless than\u201d and \u201cgreater than\u201d refer to alphabetical order. Otherwise, the ordering is more complicated.) \u2022 s1.toUpperCase() is a String-valued function that returns a new string that is equal to s1, except that any lower ca\u00aese letters in s1 have been converted to upper case. For example, \\\"Cat\\\".toUpperCase() is the string \\\"CAT\\\". There is also a function s1.toLowerCase(). \u2022 s1.trim() is a String-valued function that returns a new string that is equal to s1 except that any non-printing characters such as spaces and tabs have been trimmed from the beginning and from the end of the string. Thus, if s1 has the value \\\"fred \\\", then s1.trim() is the string \\\"fred\\\". For the functions s1.toUpperCase(), s1.toLowerCase(), and s1.trim(), note that the value of s1 is not modified. Instead a new string is created and returned as the value of the function. The returned value could be used, for example, in an assignment statement such as \u201csmallLetters = s1.toLowerCase();\u201d. To change the value of s1, you could use an assignment \u201cs1 = s1.toLowerCase();\u201d. Here is another extremely useful fact about strings: You can use the plus operator, +, to concatenate two strings. The concatenation of two strings is a new string consisting of all the characters of the first string followed by all the characters of the second string. For example, \\\"Hello\\\" + \\\"World\\\" evaluates to \\\"HelloWorld\\\". (Gotta watch those spaces, of course if you want a space in the \u00aeconcatenated string, it has to be somewhere in the input data, as in \\\"Hello \\\" + \\\"World\\\".) Let\u2019s suppose that name is a variable of type String and that it already refers to the name of the person using the program. Then, the program could greet the user by executing the statement: System.out.println(\\\"Hello, \\\" + name + \\\". Pleased to meet you!\\\"); Even more surprising is that you can actual\u00aely concatenate values of any type onto a String using the + operator. The value is converted to a string, just as it would be if you printed it to the standard output, and then it is concatenated onto the string. For example, the expression \\\"Number\\\" + 42 evaluates to the string \\\"Number42\\\". And the statements System.out.print(\\\"After \\\"); System.out.print(years); System.out.print(\\\" years, the value is \\\"); System.out.print(principal); can be replaced by the single statement: System.out.print(\\\"After \\\" + years + \\\" years, the value is \\\" + principal); Obviously, this is very convenient. It would have shortened some of the examples presented earlier in this chapter. Introduction to Enums- Java comes with eight built-in primitive types and a large set of types that are defined by classes, such as String. But even this large collection of types is not sufficient to cover all the possible situations that a programmer might have to deal with. So, an essential part of Java, just like \u00aealmost any other programming language, is the ability to create new types. For the most part, this is done by defining new classes; you will learn how to do that in Chapter 5. But we will look here at one particular case: the ability to define enums (short for enumerated types). Enums are a recent addition to Java. They were only added in Version 5.0. Many programming languages have something similar, and many people believe that enums should have been part of Java from the beginning. Technically, an enum is considered to be a special kind of class, but that is not important for now. In this section, we will look at enums in a simplified form. In practice, most uses of enums will only need the simplified form that is presented here An enum is a type that has a fixed list of possible values, which is specified when the enum is created. In some ways, an enum is similar to the boolean data type, which has true and false as its only possible values. However, boolean is a primitive type, while an enum is not. The definition of an enum types has the (simplified) form: enum (enum-type-name) { (list-of-enum-values) } This definition cannot be inside a subroutine. You can place it outside the main() routine of the program. The henum-type-namei can be any simple identifier. This identifier becomes the name of the enum type, in the same way that \u201cboolean\u201d is the name of the boolean type and \u201cString\u201d is the name of the String type. Each value in the hlist-of-enum-valuesi must be a simple identifier, and the identifiers in the list are separated by commas. For example, here is the definition of an enum type named Season whose values are the names of the four seasons of the year:",
    "page16": "enum Season { SPRING, SUMMER, FALL, WINTER } By convention, enum values are given names that are made up of upper case letters, but that is a style guideline and not a syntax rule. Enum values are not variables. Each value is a constant that always has the same value. In fact, the possible values of an enum type are usually referred to as enum constants. Note that the enum constants of type Season are considered to be \u201ccontained in\u201d Season, which means following the convention that compound identifiers are used for things that are contained in other things the names that you actually use in your program to refer to them are Season.SPRING, Season.SUMMER, Season.FALL, and Season.WINTER. Once an enum type has been created, it can be used to declare variables in exactly the same ways that other types are used. For example, you can declare a variable named vacation of type Season with the statement: Season vacation; After declaring the variable, you can assign a value to it using an assignment statement. The value on the right-hand side of the assignment can be one of the enum constants of type Season. Remember to use the full name of t\u00aehe constant, including \u201cSeason\u201d! For example: vacation = Season.SUMMER; You can print out an enum value with an output statement such as System.out.print(vacation). The output value will be the name of the enum constan\u00aet (without the \u201cSeason.\u201d). In this case, For some unfathomable reason, Java has never made it easy to read data typed in by the user of a program. You\u2019ve already seen that output can be displayed to the user using the subroutine System.out.print. This subroutine is part of a pre-defined object called Sys\u00aetem.out. The purpose of this object is precisely to display output to the user. There is a corresponding object called System.in that exists to read data input by the user, but it provides only very primitive input facilities, and \u00aeit requires some advanced Java programming skills to use it effectively. Java 5.0 finally makes input a little easier with a new Scanner class. However, it requires some knowledge of object-oriented programming to use this class, so it\u2019s not appropriate for use here at the beginning of this course. (Furthermore, in my opinion, Scanner still does not get things quite right.) There is some excuse for this lack of concern with input, since Java is meant mainly to write programs for Graphical User Interfaces, and those programs have their own style of input/output, which is implemented in Java. However, basic support is needed for input/output in old-fashioned non-GUI programs. Fortunately, it is possible to extend Java by creating new classes that provide subroutines that are not available in the standard part of the language. As soon as a new class is available, the subroutines that it contains can be used in exactly the same way as built-in routines. Along these lines, I\u2019ve written a class called TextIO that defines subroutines for reading values typed by the user of a non-GUI program. The subroutines in this class make it possible to get input from the standard input object, System.in, without knowing about the advanced aspects of Java that are needed to use Scanner or to use System.in directly. TextIO also contains a set of output subroutines. The output subroutines are similar to those provided in System.out, but they provide a few additional features. You can use whichever set of output subroutines you prefer, and you can even mix them in the same program. To use the TextIO class, you must make sure that the class is available to your program. What this means depends on the Java programming environment that you are using. In general, you just have to add the source code file, TextIO.java, to the same directory that contains your main program. See Section 2.6 for more information about how to use TextIO. A First Text Input Example The input routines in the TextIO class are static member functions. (Static member functions were introduced in the previous section.) Let\u2019s suppose that you want your program to read an integer typed in by the user. The TextIO class contains a static member function named getlnInt that you can use for this purpose. Since this function is contained in the TextIO class, you have to refer to it in your program as TextIO.getlnInt. The function has no parameters, so a complete call to the functi\u00aeon takes the form \u201cTextIO.getlnInt()\u201d. This function call represen\u00aets the int value typed by the user, and you have to do something with the returned value, such as assign it to a variable. For example, if userInput is a variable of type int (created with a declaration statement \u201cint userInput;\u201d), then you could use the assignment statement",
    "page17": "enum Season { SPRING, SUMMER, FALL, WINTER } By convention, enum values are given names that are made up of uppe\u00aer case letters, but that is a style guideline and not a syntax rule. Enum values are not variables. Each value is a constant that always has the same value. In fact, the possible values of an enum type are usually referred to as enum constants. Note that the enum constants of type \u00aeSeason are considered to be \u201ccontained in\u201d Season, which means following the convention that compound identifiers are used for things that are contained in other things the names that you actually use in your program to refer to them are Season.SPRING, Season.SUMMER, Season.FALL, and Season.WINTER. Once an enum type has been created, it can be used to declare variables in exactly the same ways that other types are used. For example, you can declare a variable named vacation of type Season with the statement: Season vacation; After declaring the variable, you can assign a value to it using an assignment statement. The value on the right-hand side of the assignment can be one of the enum constants of type Season. Remember to use the full name of the constant, including \u201cSeason\u201d! For example: vacation = Season.SUMMER; You can print out an enum value with an output statement such \u00aeas System.out.print(vacation). The output value will be the name of the enum constant (without the \u201cSeason.\u201d). In this case, the output would be \u201cSUMMER\u201d. For some unfathomable reason, Java has never made it easy to read data typed in by the user of a \u00aeprogram. You\u2019ve already seen that output can be displayed to the user using the subroutine System.out.print. This subroutine is part of a pre-defined object called System.out. The purpose of this object is precisely to display output to the user. There is a co\u00aerresponding object called System.in that exists to read data input by the user, but it provides only very primitive input facilities, and it requires some advanced Java programming skills to use it effectively. Java 5.0 finally makes input a little easier with a new Scanner class. However, it requires some knowledge of object-oriented programming to use this class, so it\u2019s not appropriate for use here at the beginning of this course. (Furthermore, in my opinion, Scanner still does not get things quite right.) There is some excuse for this lack of concern with input, since Java is meant mainly to write programs for Graphical User Interfaces, and those programs have their own style of input/output, which is implemented in Java. However, basic support is needed for input/output in old-fashioned non-GUI programs. Fortunately, it is possible to extend Java by creating new classes that provide subroutines that are not available in the standard part of the language. As soon as a new class is available, the subroutines that it contains can be used in exactly the same way as built-in routines. Along these lines, I\u2019ve written a class called TextIO that defines subroutines for reading values typed by the user of a non-GUI program. The subroutines in this class make it possible to get input from the standard input object, System.in, without knowing about the advanced aspects of Java that are needed to use Scanner or to use System.in directly. TextIO also contains a set of output subroutines. The output subrout\u00aeines are similar to those provided in System.out, but they provide a few additional features. You can use whichever set of output subroutines you prefer, and you can even mix them in the same program. To use the TextIO class, you must make sure that the class is available to your program. What this means depends on the Java programming environment that you are using. In general, you just have to add the source code file, TextIO.java, to the same directory that contains y\u00aeour main program. See Section 2.6 for more information about how to use TextIO. A First Text Input Example The input routines in t\u00aehe TextIO class are static member functions. (Static member functions were introduced in the previous section.) Let\u2019s suppose that\u00ae you want your program to read an integer typed in by the user. The TextIO class contains a static member function named getlnInt that you can use for this purpose. Since this function is contained in the TextIO class, you have to refer to it in your program as TextIO.getlnInt. The function has no parameters, so a complete call to the function takes the form \u201cTextIO.getlnInt()\u201d. This function call represents the int value typed by the user, and you have to do something with the returned value, such as assign it to a variable. For example, if userInput is a variable of type int (created with a declaration statement \u201cint userInput;\u201d), then you could use the assignment statement Formatted Output- If you ran the preceding Interest2 example, you might have noticed that the answer is not always written in the format that is usually used for dollar amounts. In general, dollar amounts are written with two digits after the decimal point. But the program\u2019s output can be a number \u00aelike 1050.0 or 43.575. It would be better if these numbers were printed as 1050.00 and 43.58. Java 5.0 introduced a formatted output capability that makes it much easier than it used to be to control the format of output numbers. A lot of formatting options are available. I will cover just a few of the simplest and most commonly used possibilities here. You can use the function System.out.printf to produce formatted output. (The name \u201cprintf,\u201d which stands for \u201cprint formatted,\u201d is copied from the C and C++ programming languages, which have always have a similar formatting capability). System.out.printf takes two or more parameters. The first parameter is a String that specifies the format of the output. This parameter is called the format string. The remaining parameters specify the values that",
    "page18": "are to be output. Here is a statement that will print a number in the proper format for a dollar amount, where amount is a variable of type double: System.out.printf( \\\"%1.2f\\\", amount ); TextIO can also do formatted output. The function TextIO.putf has the same functionality as System.out.printf. Using TextIO, the above example would be: TextIO.printf(\\\"%1.2\\\",amount); and you could say TextIO.putln(\\\"%1.2f\\\",principal); instead of TextIO.putln(principal); in the Interest2 program to get the output in the right format. The output format of a value is specified by a format specifier. The format string (in the simple cases that I cover here) contains one format specifier for each of the values that is to be output. Some typical format specifiers are %d, %12d, %10s, %1.2f, %15.8e and %1.8g. Every format specifier begins with a percent sign (%) and ends with a letter, possibly with some extra formatting information in between. The letter specifies the type of output that is to be produced. For example, in %d and %12d, the \u201cd\u201d specifies that an integer is to be written. The \u201c12\u201d in %12d specifies the minimum number of spaces that should be used for the output. If the integer that is being output takes up fewer than 12 spaces, extra blank spaces are added in front of the integer to bring the total up to 12. We say that the output is \u201cright-justified in a field of length 12.\u201d The value is not forced into 12 spaces; if the value has more than 12 digits, all the digits will be printed, with no extra spaces. The specifier %d means the same as %1d; that is an integer will be printed using just as many spaces as necessary. (The \u201cd,\u201d by the way, stands for \u201cdecimal\u201d (base-10) numbers. You can use an \u201cx\u201d to output an integer value in hexadecimal form.) The letter \u201cs\u201d at the end of a format specifier can be used with any type of value. It means that the value should be output in its default format, just as it would be in unformatted output. A number, such as the \u201c10\u201d in %10s can be added to specify the (minimum) number of characters. The \u201cs\u201d stands for \u201cstring,\u201d meaning that the value is converted into a String value in the usual way. The format specifiers for values of type double are even more complicated. An \u201cf\u201d, as in %1.2f, is used to output a number in \u201cfloating-point\u201d form, that is with digits after the decimal point. In %1.2f, the \u201c2\u201d specifies the number of digits to use after the decimal point. The \u201c1\u201d specifies the (minimum) number of characters to output, which effectively means that just as many characters as are necessary should be used. Similarly, %12.3f would specify a floating-point format with 3 digits after the decimal point, right-justified in a field of length 12. Very large and very small numbers should be written in exponential format, such as 6.00221415e23, representing \u201c6.002\u00ae21415 times 10 raised to the power 23.\u201d A format specifier such as %15.8e specifies an output in exponential form, with the \u201c8\u201d telling how many digits to use after the decimal point. If you use \u201cg\u201d instead of \u201ce\u201d, the output will be in floating-point form for small values and in exponential form for large values. In %1.8g, the 8 gives the total number of digits in the answer, including both the digits before \u00aethe decimal point and the digits after the decimal point. In addition to format specifiers, the format string in a printf statement can include other characters. These extra characters are just copied to the output. This can be a convenient way to insert values into the middle of an output string. For example\u00ae, if x and y are variables of type int, you could say System.out.printf(\\\"The product of %d and %d is %d\\\", x, y, x*y); When this statement is executed, the value of x is substituted for the first %d in the string, the value of y for the second %d, and the value of the expression x*y for the third, so the output would be something like \u201cThe product of 17 and 42 is 714\u201d (quotation marks not included in output!).",
    "page19": "Introduction to File I/O System.out sends its output to the output destination known as \u201cstandard output.\u201d But standard output is just one possible output destination. For example, data can be written to a file that is stored on the user\u2019s hard drive. The advantage to this, of course, is that the data is saved in the file even after the program ends, and the user can print the file, email it to someone else, edit it with another program, and so on. TextIO has the ability to write data to files and to read data from files. When you write output using the put, putln, or putf method in TextIO, the output is sent to the current output destination. By default, the current output destination is standard output. However, TextIO has some subroutines that can be used to change the current output destination. To write to a file named \u201cresult.txt\u201d, for example, you would use the \u00aestatement: TextIO.writeFile(\\\"result.txt\\\"); After this statement is executed, any output from TextIO output statements will be sent to the file named \u201cresult.txt\u201d instead of to standard output. The file should be created in the same directory that contains the program. Note that if a file with the same name already exists, its previous contents will be erased! In many cases, you want to let the user select the file that will be used for output. The st\u00aeatement TextIO.writeUserSelectedFile(); will open a typical graphical-user-interface fil\u00aee selection dialog where the user can specify the output file. If you want to go back to sending output to standard output, you can say TextIO.writeStandardOutput(); You can also specify the input source for TextIO\u2019s various \u201cget\u201d functions. The default input source is standard input. You can use the statement TextIO.readFile(\\\"data.txt\\\") to read from a file named \u201cdata.txt\u201d instead, or you can let the user select the input file by saying TextIO.readUserSelectedFile(), and you can go back to reading from standard input with TextIO.readStandardInp\u00aeut(). When your program is reading from standard input, the user gets a chance to correct any errors in the input. This is not possible when the program is reading from a file. If illegal data is found when a program tries to read from a file, an error occurs that will crash the program. (Later, we will see that is is possible to \u201ccatch\u201d such errors and recover from them.) Errors can also occur, though more rarely, when writing to fi\u00aeles. A complete understanding of file input/output in Java requires a knowledge of object oriented programming. We will return to the topic later, in Chapter 11. The file I/O capabilities in TextIO are rather primitive by comparison. Nevertheless, they are sufficient for many applications, and they will allow you to get some experience with files sooner rather than later. As a simple example, here is a program that asks the user some questions and outputs the user\u2019s responses to a file named \u201cprofile.txt\u201d: Details of Expressions- This section takes a closer look at expressions. Recall that an expression is a piece of program code that represents or computes a value. An expression can be a literal, a variable, a function call, or several of these things combined with operators such as + and >. The value of an expression can be assigned to a variable, used as a parameter in a subroutine call, or combined with other values into a more complicated expression. (The value can even, in some cases, be ignored, if that\u2019s what you want to do; this is more common than\u00ae you might think.) Expressions are an essential part of programming. So far, these notes have dealt only informally with expressions. This section tells you the more-or-less complete story (leaving out some of the less commonly used operators). The basic building blocks of expressions are literals (such as 674, 3.14, true, and \u2019X\u2019), variables, and function calls. Recall that a function is a subroutine that returns a value. You\u2019ve already seen some examples of functions, such as the input routines from the TextIO class and the math\u00aeematical functions from the Math class. The Math class also contains a couple of mathematical constants that are useful in mathematical expressions: Math.PI represents \u03c0 (the ratio of the circumference of a circle to its diameter), and Math.E represents e (the base of the natural logarithms). These \u201cconstants\u201d are actually member variables in Math of type double. They are only approximations for the mathematical constants, which would require an infinite number of digits to specify exactly. Literals, variables, and function calls are simple expressions. More complex expressions can be built up by using operators to combine simpler expressions. Operators include + for adding two numbers, > for comparing two values, and so on. When several operators appear in an expression, there is a question of precedence, which determines how the operators are grouped for evaluation. For example, in the expression \u201cA + B * C\u201d, B*C is computed first and then the result is added to A. We say that multiplication (*) has higher precedence than addition (+). If the default precedence is not what you want, you can use parentheses to explicitly specify the grouping you want. For example, you could use \u201c(A + B) * C\u201d if you want to add A to B first and then multiply the result by C. The rest of this section gives details of operators in Java. The number of operators in Java is quite large, and I will not cover them all here. Most of the important ones are \u00aehere; a few will be covered in later chapters as they become relevant.",
    "page20": "Arithmetic Operators Arithmetic operators include addition, subtraction, multiplication, and division. They are indicated by +, -, *, and /. These operations can be used on values of any numeric type: byte, short, int, long, float, or double. When the computer actually calculates one of these operations, the two values that it combines must be of the same type. If your program tells the computer to combine two values of different types, the computer will convert one of the values from one type to another. For example, to compute 37.4 + 10, the computer will con\u00aevert the integer 10 to a real number 10.0 and will then compute 37.4 + 10.0. This is called a type conversion. Ordinarily, you don\u2019t have to worry about type conversion in expressions, because the computer does it automatically. When two numerical values are combined (after doing type conversion on one of them, if necessary), the answer will be of the same type. If you multiply two ints, you get an int; if you multiply two doubles, you get a double. This is what you would expect, but you have to be very careful when you use the division operator /. When you divide two integers, the answer will always be an integer; if the quotient has a fractional part, it is discarded. For example, the value of 7/2 is 3, not 3.5. If N is an integer variable, then N/100 is an integer, and 1/N is equal to zero for any N greater than one! This fact is a common source of programming errors. You can force the computer to compute a real number as the answer by making one of the operands real: For example, when the computer evaluates 1.0/N, it first converts N to a real number in order to match the type of 1.0, so you get a real number as the answer. Java also has an operator for computing the remainder when one integer is divided by another. This operator is indicated by %. If A and B are integers, then A % B represents the remainder when A is divided by B. (However, for negative operands, % is not quite the same as the usual mathematical \u201cmodulus\u201d operator, since if one of A or B is negative, then the value of A % B will be negative.) For example, 7 % 2 is 1, while 34577 % 100 is 77, and 50 % 8 is 2. A common use of % is to test whether a given integer is even or odd. N is even if N % 2 is zero, and it is odd if N % 2 is 1. More generally, you can check whether an integer N is evenly divisible by an integer M by checking whether N % M is zero. Finally, you might need the unary minus operator, which takes the negative of a number. For example, -X has the same value as (-1)*X. For completeness, Java also has a unary plus operator, as in +X, even though it doesn\u2019t really do anything. By the way, recall that the + operator can \u00aealso be used to concatenate a value of any type onto a String. This is another example of type conversion. In Java, any type can be automatically converted into type String. Increment and Decrement- You\u2019ll find that adding 1 to a variable is an extremely common operation in programming. Subtracting 1 from a variable is also pretty common. You might perform the operation of adding 1 to a variable with assignment statements such as: counter = counter + 1; goalsScored = goalsScored + 1; The effect of the assignment statement x = x + 1 is to take the old value of the variable x, compute the result of adding 1 to that value, and store the answer as the new value of x. The same operation can be accomplished by writing x++ (or, if you prefer, ++x). This actually changes the value of x, so that it has the same effect as writing \u201cx = x + 1\u201d. The two statements above could be written counter++; goalsScored++; Similarly, you could write x-- (or --x) to subtract 1 from x. That is, x-- performs the same computation as x = x - 1. Adding 1 to a variable is called incrementing that variable, and subtracting 1 is called decrementing. The operators ++ and -- are called the increment operator and the decrement operator, respectively. These operators can be used on variables belonging to any of the numerical type\u00aes and also on variables of type char. Usually, the operators ++ or -- are used in statements like \u201cx++;\u201d or \u201cx--;\u201d. These statements are commands to change the value of x. However, it is also legal to use x++, ++x, x--, or --x as expressions, or as parts of larger expressions. That is, you can write things like: y = x++; y = ++x; TextIO.putln(--x); z = (++x) * (y--); The statement \u201cy = x++;\u201d has the effects of adding 1 to the value of x and, in addition, assigning some value to y. The value assigned to y is the value of the expression x++, which is defined to be the old value of x, before the 1 is added. Thus, if the value of x is 6, the statement \u201cy = x++;\u201d will change the value of x to 7, but it will change the value of y to 6 since the value assigned to y is the old value of x. On the other hand, the value of ++x is defined to be the new value of x, after the 1 is added. So if x is 6, then the statement \u201cy = ++x;\u201d changes the values of both x and y to 7. The decrement operator, --, works in a similar way. This can be confusing. My advice is: Don\u2019t be confused. Use ++ and -- only in stand-alone statements, not in expressions. I will follow this advice in all the examples in these notes.",
    "page21": "Relational Operators Java has boolean variables and boolean-valued expressions that can be used to express conditions that can be either true or false. One way to form a boolean-valued expression to compare two values using a relational operator. Relational operators are used to test whether two values are equal, whether one value is greater than another, and so forth. The relational operators in Java are: ==, !=, <, >, <=, and >=. The meanings of these operators are: A == B Is A \\\"equal to\\\" B? A != B Is A \\\"not equal to\\\" B? A < B Is A \\\"less than\\\" B? A > B Is A \\\"greater than\\\" B? A <= B Is A \\\"less than or equal to\\\" B? A >= B Is A \\\"greater than or equal to\\\" B? These operators can be used to compare values of any of the numeric types. They can also be used to compare values of type char. For characters, < and > are defined according the numeric Unicode values of the characters. (This might not always be what you want. It is not the same as alphabetical order because all the upper case letters come before all the lower case letters.) When using boolean expressions, you should remember that as far as the computer is concerned, there is nothing special about boolean values. In the next chapter, you will see how to use them in loop and branch statements. But you can also assign boolean-valued expressions to boolean variables, just as you can as\u00aesign numeric values to numeric variables. By the way, the operators == and != can be used to compare boolean values. This is occasionally useful. For example, can you figure out what this does: boolean sameSign; sameSign = ((x > 0) == (y > 0)); One thing that you cannot do with the relational operators <, >, <=, and <= is to use them to compare values of type String. You can legally use == and != to compare Strings, but because of peculiarities in the way objects behave, they might not give the results you want. (The == operator checks whether two objects are stored in the same memory location, rather than whether they contain the same value. Occasionally, for some objects, you do want to make such a\u00ae check but rarely for strings. I\u2019ll get back to this in a later chapter.) Instead, you should use the subroutines equals(), equalsIgnoreCase(), and compareTo(), which were described in Section 2.3, to compare two Strings. Boolean Operators- In English, complicated conditions can be formed using the words \u201cand\u201d, \u201cor\u201d, and \u201cnot.\u201d For example, \u201cIf there is a test and you did not study for it. . . \u201d. \u201cAnd\u201d, \u201cor\u201d, and \u201cnot\u201d are boolean operators, and they exist in Java as well as in English. In Java, the boolean operator \u201cand\u201d is represented by &&. The && operator is used to combine two boolean values. The result is also a boolean value. The result is true if both of the combined values are true, and the result is false if either of the combined values is false. For example, \u201c(x == 0) && (y == 0)\u201d is true if and only if both x is equal to 0 and y is equal to 0. The boolean operator \u201cor\u201d is represented by ||. (That\u2019s supposed to be two of the vertical line characters, |.) The expression \u201cA || B\u201d is true if either A is true or B is true, or if both are true. \u201cA || B\u201d is false only if both A and B are false. The operators && and || are said to be short-circuited versions of the boolean operators. This means that the second operand of && or || is not necessarily evaluated. Consider the test (x != 0) && (y/x > 1) Suppose that the value of x is in fact zero. In that case, the division y/x is undefined mathmatically. However, the computer will never perform the division, since when the computer evaluates (x != 0), it finds that the result is false, and so it knows that ((x != 0) && anything) has to be false. Therefore, it doesn\u2019t bother to evaluate the second operand, (y/x > 1). The evaluation has been short-circuited and the division by zero is avoided. Without the shortcircuiting, there would have been a division by zero. (This may seem like a technicality, and it is. But at times, it will make your programming life a little easier.) The bo\u00aeolean operator \u201cnot\u201d is a unary operator. In Java, it is indicated by ! and is written in front of its single operand. For example, if test is a boolean variable, then test = ! test; will reverse the value of test, changing it from true to false, or from false to true.",
    "page22": "Conditional Operator Any good programming language has some nifty little features that aren\u2019t really necessary but that let you feel cool when you use them. Java has the conditional operator. It\u2019s a ternary operator that is, it has three operands and it comes in two pieces, ? and :, that have to be used together. It takes the form boolean-expression ? expression1 : expression2 The computer tests the value of hboolean-expressioni. If the value is true, it evaluates hexpression1i; otherwise, it evaluates hexpression2i. For example: next = (N % 2 == 0) ? (N/2) : (3*N+1); will assign the value N/2 to next if N is even (that is, if N % 2 == 0 is true), and it will assign the value (3*N+1) to next if N is odd. (The parentheses in this example are not required, but they do make the expression easier to read.) Assignment Operators and T\u00aeype-Casts You are already familiar with the assignment statement, which uses the symbol \u201c=\u201d to assign the value of an expression to a variable. In fact, = is really an operator in the sense that an assignment can itself be used as an expression or as part of a more complex expression. The value of an assignment such as A=B is the same as the value that is assigned to A. So, if you want to assign the value of B to A and test at the same time whether that value is zero, you could say: if ( (A=B) == 0 )... Usually, I would say, don\u2019t do things like that! In general, the type of the expression on the right-hand side of an assignment statement must be the same as the type of the variable on the left-hand side. However, in some cases, the computer will automatically convert the value computed by the expression to match the type of the variable. Consider the list of numeric types: byte, short, int, long, float, double. A value of a type that occurs earlier in this list can be converted automatically to a value that occurs later. For example: int A; double X; short B; A = 17; X = A; // OK; A is converted to a double B = A; // illegal; no automatic c\u00aeonversion // from int to short The idea is that conversion should only be done automatically when it can be done without changing the semantics of the value. Any int can be converted to a double with the same numeric value. However, there are int values that lie outside the legal range of shorts. There is simply no way to represent the int 100000 as a short, for example, since the largest value of type short is 32767. In some cases, you might want to force a conversion that wouldn\u2019t be done automatically. For this, you can use what is called a type cast. A type cast is indicated by putting a type name, in paren\u00aetheses, in front of the value you want to convert. For example, int A; short B; A = 17; B = (s\u00aehort)A; // OK; A is explicitly type cast // to a value of type short You can do type casts from any numeric type to any other numeric type. However, you should note that you might change the numeric value of a number by type-casting it. For example, (short)100000 is -31072. (The -31072 is obtained by taking the 4-byte int 100000 and throwing away two of t\u00aehose bytes to obtain a short you\u2019ve lost the real information that was in those two bytes.) As another example of type casts, consider the problem of getting a random integer between 1 and 6. The function Math.random() gives a real number between 0.0 and 0.9999. . . , and so 6*Math.random() is between 0.0 and 5.999. . . . The type-cast operator, (int), can be used to convert this to an integer: (in\u00aet)(6*Math.random()). A real number is cast to an integer by discarding the fractional part. Thus, (int)(6*Math.random()) is one of the integers 0, 1, 2, 3, 4, and 5. To get a number between 1 and 6, we can add 1: \u201c(int)(6*Math.random()) + 1\u201d. You can also type-cast between the type char and the numeric types. The numeric value of a char is its Unicode code number. For example, (char)97 is \u2019a\u2019, and (int)\u2019+\u2019 is 43. (However, a type conversion from char to int is automatic and does not have to be indicated with an explicit type cast.)",
    "page23": "Type Conversion of Strings In addition to automatic type conversions and explicit type casts, there are some other cases where you might want to convert a value of one type into a value of a different type. One common example is the conversion of a String value into some other type, such as converting the string \\\"10\\\" \u00aeinto the int value 10 or the string \\\"17.42e-2\\\" into the double value 0.1742. In Java, these conversions are handled by b\u00aeuilt-in functions. There is a standard class named Integer that contains several subroutines and variables related to the int data type. (Recall that since int is not a class, int itself can\u2019t cont\u00aeain any subroutines or variables.) In particular, if str is any expression of type String, then Integer.parseInt(str) is a function call that attempts to convert the value of str into a value of type int. For example, the value of Integer.parseInt(\\\"10\\\") is the int value 10. If the parameter to Integer.parseInt does not represent a legal int value, then an error occurs. Similarly, the standard class named Double includes a function Double.parseDouble that tries to convert a parameter of type String into a value of type double. For example, the value of the function call Double.parseDouble(\\\"3.14\\\") is the double value 3.14. (Of cou\u00aerse, in practice, the parameter used in Double.parseDouble or Integer.parseInt would be a variable or expression rather than a constant string.) Type conversion functions also exist for converting strings into enumerated type values. (Enumerated types, or enums, were introduced in Subsection 2.3.3.) For any enum type, a predefined function named valueOf is automatically defined for that type. This is a function that takes a string as parameter and tries to convert it to a value belonging to the enum. The valueOf function is part of the enum type, so the name of the enum is part of the full name of the function. For example, if an enum Suit is defined as enum Suit { SPADE, DIAMOND, CLUB, HEART } then the name of the type conversion function would be Suit.valueOf. The value of the function call Suit.valueOf(\\\"CLUB\\\") would be the enumerated type value Suit.CLUB. For the conversion to succeed, the string must exactly match the simple name of one of the enumerated type constants (without the \u201cSuit.\u201d in front). Precedence Rules If you use several operators in one expression, and if you don\u2019t use parentheses to explicitly indicate the order of evaluation, then you have to worry about the precedence rules that determine the order of evaluation. (Advice: don\u2019t confuse yourself or the reader of your program; use parentheses liberally.) Here is a listing of the operators discussed in this section, listed in order from highest precedence (evaluated first) to lowest precedence (evaluated last): Unary operators: ++, --, !, unary - and +, type-cast Multiplication and division: *, /, % Addition and subtraction: +, - Relational operators: <, >, <=, >= Equality and inequality: ==, != Boolean and: && Boolean or: || Conditional operator: ?: Assignment operators: =, +=, -=, *=, /=, %= Operators on the same line have the same precedence. When operators of the same precedence are strung together in the absence of parentheses, unary operators and ass\u00aeignment operators are evaluated right-to-left, while the remaining operators are evaluated left-to-right. For example, A*B/C means (A*B)/C, while A=B=C means A=(B=C). (Can you se\u00aee how the expression A=B=C might be useful, given that the value of B=C as an expression is the same as the value that is assigned to B?) Programming Environments- Although the Java language is highly standardized, the procedures for creating, compiling, and editing Java programs vary widely from one programming environment to another. There are two basic approaches: a command line environment, where the user types commands and the computer responds, and an integrated development environment (IDE), where the user uses the keyboard and mouse to interact with a graphical user interface. While there is just one common command line environment for Java programming, there is a wide variety of IDEs. I cannot give complete or definitive information on Java programming environments in this section, but I will try to give enough information to let you compile and run the examples from this textbook, at least in a command line environment. There are many IDEs, and I can\u2019t co\u00aever them all here. I will concentrate on Eclipse, one of the most popular IDEs for Java programming, but some of the information that is presented will apply to other IDEs as well. One thing to keep in mind is that you do not have to pay any money to do Java programming (aside from buying a computer, of course). Everything that you need can be downloaded for free on the Internet",
    "page24": "Java Development Kit- The basic development system for Java programming is usually referred to as the JDK (Java Development Kit). It is a part of J2SE, the Java 2 Platform Standard Edition. This book requires J2SE version 5.0 (or higher). Confusingly, the JDK that is part of J2SE version 5.0 is sometimes referred to as JDK 1.5 instead of 5.0. Note that J2SE comes in two versions, a Development Kit version and a Runtime version. The Runtime can be used to run Java programs and to view Java applets in Web pages, but it does not allow you to compile your own Java programs. The Development Kit includes the Runtime and adds to it the JDK which lets you compile programs. You need a JDK for use with this textbook. Java was developed by Sun Microsystems, Inc., which makes it\u00aes JDK for Windows and Linux available for free download at its Java Web site, java.sun.com. If you have a Windows computer, it might have come with a Java Runtime, but you might still need to download the JDK. Some versions of Linux come with the JDK either installed by default or on the installation media. If you need to download and install the JDK, be sure to get JDK 5.0 (or higher). As of June, 2006, the download page for JDK 5.0 can be found at http://java.sun.com/j2se/1.5.0/download.jsp. Mac OS comes with Java. The version included with Mac OS 10.4 is 1.4.2, the version previous to Java 5.0. However, JDK Version 5.0 is available for Mac OS 10.4 on Apple\u2019s Web site and can also be installed through the standard Mac OS Software Update application. If a JDK is installed on your computer, you can use the \u00aecommand line environment to compile and run Java programs. Some IDEs depend on the JDK, so even if you plan to use an IDE for programming, you might still need a JDK. Command Line Environment- Many modern computer users find the command line environment to be pretty alien and unintuitive. It is certainly very different from the graphical user interfaces that most people are used to. However, it takes only a little practice to learn the basics of the command line environment and to become productive using it. To use a command line programming environ\u00aement, you will have to open a window where you can type in commands. In Windows, you can open such a command window by running the program named cmd. One way to run cmd is to use the \u201cRun Program\u00ae\u201d feature in the Start menu, and enter \u201ccmd\u201d as the name of the program. In Mac OS, you want to run the Terminal program, which can be be found in t\u00aehe Utilities folder inside the Applications folder. In Linux, there are several possibilities, including Konsole, gterm, and xterm. No matter what type of computer you are using, when you open a command window, it will display a prompt of some sort. Type in a c\u00aeommand at the prompt and press return. The computer will carry out the command, displaying any output in the command window, and will then redisplay the prompt so that you can type another command. One of the central concepts in the command line environment is the current directory which contains the files to which commands that you type apply. (The words \u201cdirectory\u201d and \u201cfolder\u201d mean the same thing.) Often, the name of the current directory is part of the command prompt. You can get a list of the files in the current directory by typing\u00ae in the command dir (on Windows) or ls (on Linux and Mac OS). When the window first opens, the current directory is your home directory, where \u00aeall your files are stored. You can change the current directory using the cd command with the name of the directory that you want to use. For example, to change into your Desktop directory, type in the command cd Desktop and press return. You should create a directory (that is, a folder) to hold your Java work. For example, create a directory named javawork in your home directory. You can do this using your computer\u2019s GUI; another way to do it is to open a command window and enter the command mkdir javawork. When you want t\u00aeo work on programming, open a command window and enter the command cd javawork to change into your work directory. Of course, you can have more than one working directory for your Java work; you can organize your files any way you like.",
    "page25": "The most basic commands for using Java on the command line are javac and java; javac is used to compile Java source code, and java is used to run Java stand-alone applications. If a JDK is correctly installed on your computer, it should recognize these commands when you type them in on the command line. Try typing the commands java -version and javac -version which should tell you which version of Java is installed. If you get a message such as \u201cCommand not found,\u201d then Java is not correctly installed. If the \u201cjava\u201d command works, but \u201cjavac\u201d does not, it means that a Java Runtime is installed rather than a Development\u00ae Kit. To create your own programs, you will need a text editor. A text editor is a computer program that allows you to create and save documents that contain plain text. It is important that the documents be saved as plain text, that is without any special encoding or formatting information. Word processor documents are not appropriate, unless you can get your word processor to save as plain text. A good text editor can make programming a lot more pleasant. Linux comes with several text editors. On Windows, you can use notepad in a pinch, but you will probably want something better. For Mac OS, you might download the free\u00ae TextWrangler application. One possibility that will work on any platform is to use jedit, a good programmer\u2019s text editor that is itself written in Java and that can be downloaded for free from www.jedit.org. To create your own programs, you should open a command line window and cd into the working directory where you will store your source code files. Start up your text editor program, such as by double-clicking its icon or selecting it from a Start menu. Type your code into the editor window, or open an existing source code file that you want to modify. Save the file. Remember that the name of a Java source code file must end in \u201c.java\u201d, and the rest of the file name must match the name of the class th\u00aeat is defined in the file. Once the file is saved in your working directory, go to the command window and use the javac command to compile it, as discussed above. If there are syntax errors in the code, they will be listed in the command window. Each error message contains the line number in the file where the computer found the error. Go back to the editor and try to fix the errors, save your changes, and they try the javac command again. (It\u2019s usually a good idea to just work on the first few errors; sometimes fixing those will make other errors go away.) Remember that when the javac command finally succeeds, you will get no message at all. Then you can use the java command to run your program, as described above. Once you\u2019ve compiled the program, you can run it as many times as you like without recompiling it. That\u2019s really all there is to it: Keep both editor and command-line window open. Edit, save, and compile until you have eliminated all the syntax errors. (Always remember to save the file before compiling it the compiler only sees the saved file, not the version in the editor window.) When you run the program, you might find that it has semantic errors that cause it to run incorrectly. It that case, you have to go back to the edit/save/compile loop to try to find and fix the problem. IDEs and Eclipse In an Integrated Development Environment, everything you need to create, compile, and run programs is integrated into a single package, with a graphical user interface that will be familiar to most computer users. There are many different IDEs for Java program development, ranging from fairly simple wrappers around the JDK to highly complex applications with a multitude of features. For a beginning programmer, there is a danger in using an IDE, since the difficulty of learning to use the IDE, on top of the difficulty of learning to program, can be overwhelming. Recently, however, I have begun\u00ae using one IDE, Eclipse, in my introductory programming courses. Eclipse has a variety of features that are very useful for a beginning programmer. And even though it has many advanced features, its design makes it possible to use Eclipse without understanding its full complexity. It is likely that other modern IDEs have similar properties, but my only in-depth experience is with Eclipse. Eclipse is used by many professional\u00ae programmers and is probably t\u00aehe most commonly used Java IDE. (In fact, Eclipse is\u00ae actually a general development platform that can be used for other purposes besides Java development, but its most common use is Java.) Eclipse is itself written in Java.\u00ae It requires Java 1.4 (or higher) to run, so it works on any computer platform that supports Java 1.4, including Linux, Windows, and recent versions of Mac OS. If you want to use Eclipse to compile and run Java 5.0 programs, you need Eclipse version 3.1 (or higher). Furthermore, Eclipse requires a JDK. You should make sure that JDK 5.0 (or higher) is installed on your computer, as described above, before you install Eclipse. Eclipse can be downloaded for free from www.eclipse.org. The first time you start Eclipse, you will be asked to specify a workspace, which is the directory where all your work will be stored. You can accept the default name, or provide one of your own. When startup is complete, the Eclipse window will be filled by a large \u201cWelcome\u201d screen that includes links to extensive documentation and tutorials. You can close this screen, by clicking the \u201cX\u201d next to the word \u201cWelcome\u201d; you can get back to it later by choosing \u201cWelcome\u201d from the \u201cHelp\u201d menu.",
    "page26": "The Problem of Packages- Every class in Java is contained in something called a package. Classes that are not explicitly put into a different package are in the \u201cdefault\u201d package. Almost all the examples in this textbook are in the default package, and I will not even discuss packages in any depth until Section 4.5. However, some IDEs might force you to pay attention to packages. When you create a class in Eclipse, you might notice a message that says that \u201cThe use of the default package is discouraged.\u201d Although this is true, I have chosen to use it anyway, since it seems easier for beginning programmers to avoid the whole issue of packages, at least at first. Some IDEs might be even less willing than Eclipse to use the default package. If you create a class in a package, the source code starts with a line that specifies which package the class is in. For example, if the class is in a package \u00aenamed testpkg, then the first line of the source code will be package testpkg; In an IDE, this will not cause any problem unless the program you are writing depends on TextIO. You will not be able to use TextIO in a program unless TextIO is placed into the same package as the program. This means that you have to modify the source code file TextIO.java to specify the package; just add a package statement using the same package name as the program. Then add the modified TextIO.java to the same folder that contains the program source code. Once you\u2019ve done this, the example should run in the same way as if it were in the default package. By the way, if you use packages in a command-line environment, other complications arise. For example, if a class is in a package named testpkg, then the source code file must be in a subdirectory named testpkg that is inside your main Java working directory. Nevertheless, when you compile or execute the program, you should be in the main directory, not in the subdirectory. Blocks, Loops, and Branches- The ability of a computer to perform complex tasks is built on just a few ways of combining simple commands into control structures. In Java, there are just six such structures that are used to determine the normal flow of control in a program and, in fact, just three of them would be enough to write programs to perform any task. The six control structures are: the block, the while loop, the do..while loop, the for loop, the if statement, and the switch statement. Each of these structures is considered to be a single \u201cstatement,\u201d but each is in fact a structured statement that can contain one or more other statements inside itself. Blocks The block is the simplest type of structured statement. Its purpose is simply to group a sequence of statements into a single statement. The format of a block is: { (statements) } That is, it consists of a sequence of statements enclosed between a pair of braces, \u201c{\u201d and \u201c}\u201d. (In fact, it is possible for a block to contain no statements at all; such a block is called an empty block, and can actually be useful at tim\u00aees. An empty block consists of nothing but an empty pair of braces.) Block statements usually occur inside other statements, where their purpose is to group together several statements into a unit. However, a block can be legally used wherever a statement can occur. There is one place where a block is required: As you might have already noticed in the case of the main subroutine of a program, the definition of a subroutine is a block, since it is a sequence of statements enclosed inside a pair of braces. I should probably note again at this point that Java is what is called a free-format language. There are no syntax rules about how the language has to be arranged on a page. So, for example, you could write an entire block on one line if you want. But as a matter of good programming style, you should lay out your program on the page in a way that will make its structure as clear as possible. In general, this means putting one statement per line and using indentation to indicate statements that are contained inside control structures. This is the format that I will generally use in my examples. Here are two examples of blocks: { System.out.print(\\\"The answer is \\\"); System.out.println(ans); } { // This block exchanges the values of x and y i\u00aent temp; // A temporary variable for use in this block. temp = x; // Save a copy of the value of x in temp. x = y; // Copy the value of y into x. y = temp; // Copy the value of temp into y. } In the second example, a variable, temp, is declared inside the block. This is perfectly legal, and it is good style to declare a variable inside a block if that variable is used nowhere else but inside the block. A variable declared inside a block is completely inaccessible and invisible from outside that block. When the computer executes the variable declaration statement, it allocates memory to hold the value of the variable. When the block ends, that memory is discarded (that is, made available for reuse). The variable is said to be local to the block. There is a general concept called the \u201cscope\u201d of an identifier. The scope of an identifier is the part of the program in which that identifier is valid. The scope of a variable defined inside a block is limited to that block, and more specifically to the part of the block that comes after the declaration of the variable.",
    "page27": "The Basic While Loop The block statement by itself really doesn\u2019t affect the flow of control in a program. The five remaining control structures do. They can be divided into two classes: loop statements and branching statements. You really just need one control structure from each category in order to have a completely general-purpose programming language. More than that is just convenience. In this se\u00aection, I\u2019ll introduce the while loop and the if statement. I\u2019ll give the full details of these statements and of the other three control structures in later sections. A while loop is used to repeat a given statement over and over. Of course, it\u2019s not likely that you would want to keep repeating it forever. That would be an infinite loop, which is generally a bad thing. (There is an old story about computer pioneer Grace Murray Hopper, who read instructions on a bottle of shampoo telling her to \u201clather, rinse, repeat.\u201d As the story goes, she claims that she tried to follow the directions, but she ran out of shampoo. (In case you don\u2019t get it, this is a joke about the way that computers mindlessly follow instructions.)) To be more specific, a while loop will repeat a statement over and over, but only so long as a specified condition remains true. A while loop has the form: while (boolean-expression) statement Since the statement can be, and usually is, a block, many while loops have the form: while (boolean-expression) { (statements) } The semantics of this statement go like this: When the computer comes to a while statement, it evaluates the boolean-expression, which yields either true or false as the value. If the value is false, the computer skips over the rest of the while loop and proceeds to the next command in the program. If the value of the expression is true, the computer executes the stat\u00aeement or block of statements inside the loop. Then it returns to the beginning of the while loop and repeats the process. That is, it re-evaluates the boolean-expression, ends the loop if the value is false, and continues it if the value is true. This will continue over and over until the value of the expression is false; if that never happens, then there will be an infinite loop. Here is an example of a while loop that simply prints out the numbers 1, 2, 3, 4, 5: int number; // The number to be printed. number = 1; // Start with 1. while ( number < 6 ) { // Keep going as long as number is < 6. System.out.println(number); number = number + 1; // Go on to the next number. } System.out.println(\\\"Done!\\\"); The variable number is initialized with the value 1. So the first time through the while loop, when the computer evaluates the expression \u201cnumber < 6\u201d, it is asking whether 1 is less than 6, which is true. The computer therefor proceeds to execute the two statements inside the loop. The first statement prints out \u201c1\u201d. The second statement adds 1 to number and stores the result back into the variable number; the value of number has been changed to 2. The computer has reached the end of the loop, so it returns to the beginning and asks again whether number is less than 6. Once again this is true, so the computer executes the loop again, this time printing out 2 as the value of number and then changing the value of number to 3. It continues in this way until eventually number becomes equal to 6. At that point, the expression \u201cnumber < 6\u201d evaluates to false. So, the computer jumps past the end of the loop to the next statement and prints out the message \u201cDone!\u201d. Note that when the loop ends, the value of number is 6, but the last\u00ae value that was printed was 5. By the way, you should remember that you\u2019ll never see a while loop standing by itself in a real program. It will always be inside a subroutine which is itself defined inside some class. As an example of a while loop used inside a complete program, here is a little program that computes the interest on an investment over several years. This is an improvement over examples from the previous chapter that just reported the results for one year.",
    "page28": "The Basic If Statement An if statement tells the computer to take one of two alternative courses of action, depending on whether the value of a given boolean-valued expression is true or false. It is an example of a \u201cbranching\u201d or \u201cdecision\u201d statement. An if statement has the form: if ( boolean-expression ) statement1 else statement2 When the computer executes an if statement, it evaluates the boolean expression. If the value is true, the computer executes the first statement and skips the statement that follows the \u201celse\u201d. If the value of the expression is false, then the computer skips the first statement and executes the second one. Note that in any case, one and only one of the two statements inside the if statement is executed. The two statements represent alternative courses of action; the computer decides between these courses of action based on \u00aethe value of the boolean expression. In many cases, you want the computer to choose between doing something and not doing it. You can do this with an if statement that omits the else part.  Algorithm Development- Programming is difficult (like many activities that are useful and worth while and like most of those activities, it can also be rewarding and a lot of fun). When you write a program, you have to tell the computer every small detail of what to do. And you have to get everything exactly right, since the computer will blindly follow your program exactly as written. How, then, do people write any but the most simple programs? It\u2019s not a big mystery, actually. It\u2019s a matter of learning to think in the right way. A program is an expression of an idea. A programmer starts with a general idea of a task for the computer to perform. Presumably, the programmer has some idea of how to perform the task by hand, at least in general outline. The problem is to flesh out that outline into a complete, unambiguous, step-by-step procedure for carrying out the task. Such a procedure is called an \u201calgorithm.\u201d (Technically, an algorithm is an unambigu\u00aeous, step-by-step procedure that terminates after a finite number of steps; we don\u2019t want to count procedures that go on foreve\u00aer.) An algorithm is not the same as a program. A program is written in some particular programming language. An algorithm is more like the idea behind the program, but it\u2019s the idea of the steps the program will take to perform its task, not just the idea of the task itself. The steps of the algorithm don\u2019t have to be filled in in complete detail, as long as the steps are unambiguous and it\u2019s clear that carrying out the steps will accomplish the assigned task. An algorithm can be expressed in any language, including English. Of course, an algorithm can only be expressed as a program if all t\u00aehe details have been filled in. So, where do algorithms come from? Usually, they have to be developed, often with a lot of thought and hard work. Skill at algorithm development is something that comes with practice, but there are techniques and guidelines that can help. I\u2019ll talk here about some techniques and guidelines that are relevant to \u201cprogramming in the small,\u201d and I will return to the subject several times in later chapters. Pseudocode and Stepwise Refinement- When programming in the small, you have a few basics to work with: variables, assignment statements, and input/output routines. You might also have some subroutines, objects, or other building blocks that have already been written by yo\u00aeu or someone else. (Input/output routines fall into this class.) You can build sequences of these basic instructions, and you can also combine them into more complex control structures such as while loops and if statements. Suppose you have a task in mind that you want the computer to perform. One way to proceed is to write a description of the task, and take that description as an outline of the algorithm you want to develop. Then you can refine and elaborate that description, gradually adding steps and detail, until you ha\u00aeve a complete algorithm that can be translated directly int\u00aeo programming language. This method is called stepwise refinement, and it is a type of top-down design. As you proceed through the stages of stepwise refinement, you can write out descriptions of your algorithm in pseudocode informal instructions that imitate the structure of programming languages without the complete detail and perfect syntax of actual program code.",
    "page29": "Coding, Testing, Debugging - It would be nice if, having developed an algorithm for your program, you could relax, press a button, and get a perfectly working program. Unfortunately, the process of turning an algorithm into Java source code doesn\u2019t always go smoothly. And when you do get to the stage of a working program, it\u2019s often only working in the sense that it does something. Unfortunately not what you want it to do. After program design comes coding: translating the design into a program written in Java or some other language. Usually, no matter how careful you are, a few syntax errors will creep in from somewhere, and the Java compiler will reject your program with some kind of error message. Unfortunately, while a compiler w\u00aeill always detect syntax errors, it\u2019s not very good about telling you exactly what\u2019s wrong. Some\u00aetimes, it\u2019s not even good about telling you where the real error is. A spelling error or missing \u201c{\u201d on line 45 might cause the compiler to choke on line 105. You can avoid lots of errors by making sure that you really unde\u00aerstand the syntax rules of the language and by following some basic programming guidelines. For example, I never type a \u201c{\u201d without typing the matching \u201c}\u201d. Then I go back and fill in the\u00ae statements between the braces. A missing or extra brace can be one of the hardest errors to find in a large program. Always, always indent your program nicely. If you change the program, change the indentation to match. It\u2019s worth the trouble. Use a consistent naming scheme, so you don\u2019t have to struggle to remember whether you called that variable interestrate or interestRate. In general, when the compiler gives multiple error messages, don\u2019t try to fix the second error message from the compiler until you\u2019ve fixed the first one. Once the compiler hits an error in your program, it can get confused, and the rest of the error messages might just be guesses. Maybe the best advice is: Take the time to understand the error before you try to fix it. Programming is not an experimental science. When your program compiles wit\u00aehout error, you are still not done. You have to test the program to make sure \u00aeit works correctly. Remember that the goal is not to get the right output for the two sample inputs that the professor gave in class. The goal is a program that will work correctly for all reasonable inputs. Ideally, when faced with an unreasonable input, it will respond by gently chid\u00aeing the user rather than by crashing. Test your program on a wide variety of inputs. Try to find a set of inputs that will test the full range of functionality that you\u2019ve coded into your program. As you begin writing larger programs, write them in stages and test each stage along the way. You might even have to write some extra code to do the testing for example to call a subroutine that you\u2019ve just written. You don\u2019t want to be faced, if you can avoid it, with 500 newly written lines of code that have an error in there somewhere. The point of testing is to find bugs semantic err\u00aeors that show up as incorrect behavior rather than as compilation errors. And the sad fact is that you will probably find them. Again, you can minimize bugs by careful design and careful coding, but no one has found a way to avoid them altogether. Once you\u2019ve detected a bug, it\u2019s time for debugging. You have to track down the cause of the bug in the program\u2019s source code and eliminate it. Debugging is a skill that, like other aspects of programming, requires practice to master. So don\u2019t be afraid of bugs. Learn from them. One essential debugging skill is the ability to read source code the ability to put aside preconceptions about what you think it does and to follow it the way the computer does mechanically, step-by-step to see what it really does. This is hard. I can still remember the time I spent hours looking for a bug only to find that a line of code that I had looked at ten times had a \u201c1\u201d where it should have had an \u201ci\u201d, or the time when I wrote a subroutine named WindowClosing which would have done exactly what I wanted except that the computer was looking for windowClosing (with a lower case \u201cw\u201d). Sometimes it can help to have someone who doesn\u2019t share your preconceptions look at your code. Often, it\u2019s a problem just to find the part of the program that contains the error. Most programming environments come with a debugger, which is a program that can help you find bugs. Typically, your program can be run under the control of the debugger. The debugger allows you to set \u201cbreakpoints\u201d in your program. A breakpoint is a point in the program where the debugger will pause the program so you can look at the values of the program\u2019s variables. The idea is to track down exactly when things start to go wrong during the program\u2019s execution. The debugger w\u00aeill also let you execute your program one line at a time, so that you can watch what happens in detail once you know the general area in the program where the bug is lurking.",
    "page30": "I will confess that I only rarely use debuggers myself. A more traditional approach to debugging is to insert debugging statements into yo\u00aeur program. These are output statements that print out information about the state of the program. Typically, a debugging statement would say something like System.out.println(\\\"At start of while loop, N = \\\"+ N); You need to be able to tell from the output where in your program the output is coming from, and you want to know the value of important variables. Sometimes, y\u00aeou will find that the computer isn\u2019t even getting to a part of the program that you think it should be executing. Remem\u00aeber that the goal is to find the first point in the program where the state is not what you expect it to be. That\u2019s where the bug is. And finally, remember the golden rule of debugging: If you are absolutely sure that everything in your program is right, and if it still doesn\u2019t work, then one of the things that you are absolutely sure of is wrong. The while and do..while Statements Statements in Java can be either simple statements or compound statements. Simple statements, such as assignment statements and subroutine call statements, are the basic building blocks of a program. Compound statements, such as while loops and if statements, are used to organize simp\u00aele statements into complex structures, which are called control structures because they control the order in which the statements are executed. The next five sections explore the details of control structures that are available in Java, starting with the while statement and the do..while statement in this section. At the same time, we\u2019ll look at examples of programming with each control structure and apply the techniques for designing algorithms that were introduced in the previous section. The while Statement The while statement was already introduced in Section 3.1. A while loop has the form while ( boolean-expression ) statement The statement can, of course, be a block statement consisting of several statements grouped together between a pair of braces. This statement is called the body of the loop. The body of the loop is repeated \u00aeas long as the boolean-expression is true. This boolean expression is called the continuation condition, or more simply the test, of the loop. There are a few points that might need some clarification\u00ae. What happens if the condition is false in the first place, before the body of the loop is executed even once? In that case, the body of the loop is never executed at all. The body of a while loop can be executed any number of times, including zero. What happen\u00aes if the condition is true, but it becomes false somewhere in the middle of the loop body? Does the loop end as soon as this happens? It doesn\u2019t, because the computer continues executing the body of the loop until it gets to the end. Only then does it jump back to the beginning of the loop and test the condition, and only then can the loop end. Let\u2019s look at a typical problem that can be solved using a while loop: finding the average of a set of positive integers\u00ae entered by the user. The average is the sum of the integers, divided by the number of integers. The program will ask the user to enter one integer at a time. It will keep count of the number of integers entered, and it will keep a running total of all the numbers it has read so far. Here is a pseudocode algorithm for the program: Let sum = 0 Let count = 0 while there are more integers to process: Read an integer Add it to the sum Count it Divide sum by count to get the average Print out the average But how can we test whether there are more integers to process? A typical solution is to tell the user to type in zero after all the data have been entered. This will work because we are assuming that all the data are positive numbers, so zero is not a legal data value. The zero is not itself part of the data to be averaged. It\u2019s just there to mark the end of the real data. A data value used in this way is sometimes called a sentinel value. So now the test in the while loop becomes \u201cwhile the input integer is not zero\u201d. But there is another problem! The first time the test is evaluated, before the body of the loop has eve\u00aer been executed, no integer has yet been read. There is no \u201cinput integer\u201d yet, so testing whether the input integer is zero doesn\u2019t make sense. So, we have to do something before the while loop to make sure that the test makes sense. Setting things up so that the test in a while loop makes sense the first time it is executed is called priming the loop. In this case, we can simply read the first integer before the beginning of the loop. Here is a revised algorithm:\u00ae Let sum = 0 Let count = 0 Read an integer while the integer is not zero: Add the integer to the sum Count it Read an integer Divide sum by count to get the average Print out the average",
    "page31": "Notice that I\u2019ve rearranged the body of the loop. Since an integer is read before the loop, the loop has to begin by processing that integer. At the end of the loop, the computer reads a new integer. The computer then jumps back to the beginning of the loop and tests the integer that it has just read. Note that when the computer finally reads the sentinel value, the loop ends before the sentinel value is processed. It is not added to the sum, and it is not counted. This is the way it\u2019s supposed to work. The sentinel is not part of the data. The original algorithm, even if it could have been made to work without priming, was incorrect since it would have summed and counted all the integers, including the sentinel. (Since the sentinel is zero, the sum would still be correct, but the count would be off by one. Such so-called off-by-one errors are very common. Counting turns out to be harder than it looks!) We can easily turn the algorithm into a complete program. Note that the program cannot use the statement \u201caverage = sum/count;\u201d to compute the average. Since sum and count are both variables of type int, the value of sum/count is an integer. The average should be a real number. We\u2019ve seen this problem before: we have to convert one of the int values to a double to force the computer to compute the quotient as a real number. This can be done by type-casting one of the variables to type double. The type cast \u201c(double)sum\u201d converts the value of sum to a real number, so in the program the average is computed as \u201cavera\u00aege = ((double)sum) / count;\u201d. Another solution in this case would have been to declare sum to be a variable of type double in the first place. One other issue is addressed by the program: If the user enters zero as the first input value, there are no data to process. We can test for this case by checking whether count is still equal to zero after the while loop. This might seem like a minor point, but a careful programmer should cover all the bases. The do-while Statement- Sometimes it is more convenient to test the continuation condition at the end of a loop, instead of at the beginning, as is done in the while loop. The do-while statement is very similar to the while statement, except that the word \u201cwhile,\u201d along with the condition that it tests, has been moved to the end. The word \u201cdo\u201d is added to mark the beginning of the loop. A do..while statement has the form do statemen i while ( boolean-expression ); or, since, as usual, the statement can be a block, do { statements } while ( boolean-expression ); Note the semicolon, \u2019;\u2019, at the very end. This semicolon is part of the statement, just as the semicolon at the end of an assignment statement or declaration is part of the statement. Omitting it is a syntax error. (More generally, every statement in Java ends either with a semicolon or a right brace, \u2019}\u2019.) To execute a do loop, the computer first executes the body of the loop that is, the statement or statements inside the loop and then it evaluates the boolean expression. If the value of the expression is true, the computer returns to the beginning of the do loop and repeats the process; if the value is false, it ends the loop and continues with the next part of the program. Since the condition is not tested until the end of the lo\u00aeop, the body of a do loop is always executed at least once. For example, consider the following pseudocode for a game-playing program. The do loop makes sense here instead of a while loop because with the do loop, you know there will be at least one game. Also, the test that is used at the end of the loop wouldn\u2019t even make sense at the\u00ae beginning. do { Play a Game Ask user if he wants to play another game Read the user\u2019s response } while ( the user\u2019s response is yes ); Let\u2019s convert this into proper Java code. Since I don\u2019t want to talk about game playing at the moment, let\u2019s say that we have a class named Checkers, and that the Checkers class contains a static member subroutine named playGame() that plays one game of checkers against the user. Then, the pseudocode \u201cPlay a game\u201d can be expressed as the subroutine call statement \u201cCheckers.playGame();\u201d. We need a variable to store the user\u2019s response. The TextIO class makes it convenient to use a boolean variable to store the answer to a yes/no question. The input function TextIO.getlnBoolean() allows the user to enter the value as \u201cyes\u201d or \u201cno\u201d. \u201cYes\u201d is considered to be true, and \u201cno\u201d is considered to be false. So, the algorithm can be coded as boolean wantsToContinue; // True if user wants to play again. do { Checkers.playGame(); TextIO.put(\\\"Do you want to play again? \\\"); wantsToContinue = TextIO.getlnBoolean(); } while (wantsToContinue == true); When the value of the boolean variable is set to false, it is a signal that the loop should end. When a boolean variable is used in this way as a signal that is set in one part of the program and tested in another part it is sometimes called a flag or flag variable (in the sense of a signal flag).",
    "page32": "By the way, a more-than-usually-pedantic programmer would sneer at the test \u201cwhile (wantsToContinue == true)\u201d. This test is exactly equivalent to \u201cwhile (wantsToContinue)\u201d. Testing whether \u201cwantsToContinue == true\u201d is true amounts to the same thing as testing whether \u201cwantsToContinue\u201d is true. A little less offensive is an expression of the form \u201cflag == false\u201d, where flag is a boolean variable. The value of \u201cflag == false\u201d is exactly the same as the value of \u201c!flag\u201d, where ! is the boolean negation operator. So you can write \u201cwhile (!flag)\u201d instead of \u201cwhile (flag == false)\u201d, and you can write \u201cif (!flag)\u201d instead of \u201cif (flag == false)\u201d. Although a dowhile statement is sometimes more convenient than a while statement, having two kinds of loops does not make the language more powerful. Any problem that can be solved using dowhile loops can also be solved using only while statements, and vice versa. In fact, if doSomething represents any block of program code, then do { doSomething } while ( boolean-expression ); break and continue- The syntax of the while and do..while loops allows you to test the continuation condition at either the beginning of a loop or at the end. Sometimes, it is more natural to have the test in the middle of the loop, or to have several tests at different places in the same loop. Java provides a general method for breaking out of the middle of any loop. It\u2019s called the break statement, which takes the form break; When the computer executes a break statement in a loop, it will immediately jump out of the loop. It then continues on to whatever follows the loop in the program. Consider for example: while (true) { // looks like it will run forever! TextIO.put(\\\"Enter a positive number: \\\"); N = TextIO.getlnInt(); if (N > 0) // input is OK; jump out of loop break; TextIO.putln(\\\"Your answer must be > 0.\\\"); } // continue here after break If the number entered by the user is greater than zero, the break statement will be executed and the computer will jump out of the loop. Otherwise, the computer will print out \u201cYour answer must be > 0.\u201d and will jump back to the start of the loop to read another input value. (The first line of this loop, \u201cwhile (true)\u201d might look a bit strange, but it\u2019s perfectly legitimate. The condition in a while loop can be any boolean-valued expression. The computer evaluates this expression and checks whether the value is true or false. The boolean literal \u201ctrue\u201d is just a boolean expression that always evaluates to true. So \u201cwhile (true)\u201d can be used to write an infinite loop, or one that will be terminated by a break statement.) A break statement terminates the loop that immediately encloses the break statement. It is possible to have nested loops, where one loop statement is contained inside another. If you use a break statement inside a nested loop, it will only break out of that loop, not out of the loop that contains the nested loop. There is something called a labeled break statement that allows you to specify which loop you want to break. This is not very common, so I will go over it quickly. Labels work like this: You can put a label in front of any loop. A label consists of a simple identifier followed by a colon. For example, a while with a label might look like \u201cmainloop: while...\u201d. Inside this loop you can use the labeled break statement \u201cbreak mainloop;\u201d to break out of the labeled loop. For example, here is a code segment that checks whether two strings, s1 and s2, have a character in common. If a common character is found, the value of the flag variable nothingInCommon is set to false, and a labeled break is is used to end the processing at that point. boolean nothingInCommon; nothingInCommon = true; \u00ae// Assume s1 and s2 have no chars in common. int i,j; // Variables for iterating through the chars in s1 and s2. i = 0; bigloop: while (i < s1.length()) { j = 0; while (j < s2.length()) { if (s1.charAt(i) == s2.charAt(j)) { // s1 and s2 have a common char. nothingInCommon = false; break bigloop; // break out of BOTH loops } j++; // Go on to the next char in s2. } i++; //Go on to the next char in s1. } The continue statement is related to break, but less commonly used. A continue statement tells the computer to skip the rest of the current iteration of the loop. However, instead of jumping out of the loop altogether, it jumps back to the beginning of the loop and continues with the next iteration (including evaluating the loop\u2019s continuation condition to see whether any further iterations are required). As with break, when a continue is in a nested loop, it will continue the loop that directly contains it; a \u201clabeled continue\u201d can be used to continue the containing loop instead. break and continue can be used in while loops and dowhile loops. They can also be used in for loops, which are covered in the next section. In Section 3.6, we\u2019ll see that break can also be used to break out of a switch statement. A break can occur inside an if statement, but in that case, it does not mean to break out of the if. Instead, it breaks out of the loop or switch statement that contains the if statement. If the if statement is not contained inside a loop or switch, then the if statement cannot legally contain a break. A similar consider\u00aeation applies to continue statements inside i\u00aefs.",
    "page33": "The for Statement We turn in this section to anot\u00aeher type of loop, the for statement. Any for loop is equivalent to some while loop, so the language doesn\u2019t get any additional power by having for statements. But for a certain type of problem, a for loop can be easier to construct and easier to read than the corresponding while loop. It\u2019s quite possible that in real programs, for loops actually outnumber while loops. For Loops The for statement makes a common type of while loop easier to write. Many while loops have the general form: initialization while ( continuation-condition ) { statements update } The initialization, continuation condition, and updating have all been combined in the first l\u00aeine of the for loop. This keeps everything involved in the \u201ccontrol\u201d of the loop in one place, which helps makes the loop easier to read and understand. The for loop is executed in exactly the same way as the original code: The initialization part is executed once, before the loop begins. The continuation condition is executed before each execution of the loop, and the loop ends when this condition is false. The update part is executed at the end of each exe\u00aecution of the loop, just before jumping back to check the condition. The formal syntax of the for statement is as follows: for ( initialization; continua\u00aetion-condition; update) statement or, using a block statement: for ( initialization; continuation-condition; update) { statements } The continuation-condition must be a boolean-valued expression. The initialization can be any expression, but is usually an assignment statement. The update can also be any expression, but is usually an increment, a decrement, or an assignment statement. Any of the three can be empty. If the continuation condition is empty, it is treated as if it were \u201ctrue,\u201d so the loop will be repeated forever or until it ends for some other reason, such as a break statement. (Some people like to begin an infinite loop with \u201cfor (;;)\u201d instead of \u201cwhile (true)\u201d. Usually, \u00aethe initialization part of a \u00aefor statement assigns a value to some variable, and the update changes the value of that variable with an assignment statement or with an increment or decrement operation. The value of the variable is tested in the continuation condition, and the loop ends when this condition evaluates to false. A variable used in this way is called a loop control variable. In the for statement given above, the loop control variable is years. Certainly, the most common type of for loop is the counting loop, where a loop control variable takes on all integer values between some minimum and some maximum value. A counting loop has the form for ( variable  i = min ; variable  i<= max ; variable i++ ) { statements } where min and max are integer-valued expressions (usually constants). The variable takes on the values min, hmini+1, hmini+2, . . . , max. The value of the loop control variable is often used in the body of the loo\u00aep. The for loop at the beginning of this section is a counting loop in which the loop control variable, years, takes on the values 1, 2, 3, 4, 5. Here is an even simpler example, in which the numbers 1, 2, . . . , 10 are displayed on standard output. for ( N = 1 ; N <= 10 ;\u00ae N++ ) System.out.println( N ); For various reasons, Java programmers like to start counting at 0 instead of 1, and they tend to use a \u201c<\u201d in the condition, rather than a \u201c<=\u201d. The following variation of the above loop prints out the ten numbers 0 and so on. Using < instead of <= in the test, or vice versa, is a common source of off-by-one errors in programs. You should always stop and think, Do I want the final value to be processed or not? It\u2019s easy to count down from 10 to 1 instead of counting up. Just start with 10, decrement the loop control variable instead of incrementing it, and continue as long as the variable is greater than or equal to one. Perhaps it is worth stressing one more time that a for statement, like any statement, n\u00aeever occurs on its own in a real program. A statement must be inside the main routine of a program or inside some other subroutine. And that subroutine must be defined inside a class. I should also remind you that every variable must be declared before it can be used, and that includes the loop control variable in a for statement. In all the examples that you have seen so far in this section, the loop control variables should be declared to be of type int. It is not required that a loop con\u00aetrol variable be an integer. Here, for example, is a for loop in which the variable, ch, is of type char, using the fact that the ++ operator can be applied to characters as well as to numbers",
    "page34": "Nested for Loops- Control structures in J\u00aeava are statements that contain statements. In particular, control structures can contain control structures. You\u2019ve already seen several examples of if statements inside loops, and one example of a while loop inside another while, but any combination of one control structure inside another is possible. We say that one structure is nested inside another. You can even have multiple levels of nesting, such as a while loop inside an if statement inside another while loop. The syntax of Java does not set a limit on the number of levels of nesting. As a practical matter, though, it\u2019s difficult to understand a program that has more than a few levels of nesting. Introduction to Exceptions and trycatch In addition to the control structures\u00ae that determine the normal flow of control in a program,\u00ae Java has a way to deal with \u201cexceptional\u201d cases that throw the flow of control off its normal track. When an error occurs during the execution of a program, the default behavior is to terminate the program and to print an error message. However, Java makes it possible to \u201ccatch\u201d such errors and program a response different from simply letting the program crash. This is done with the trycatch statement. The term exception is used to refer to the type of error that one might want to handle with a trycatch. An exception is an exception to the normal flow of control in the program. The term is used in preference to \u201cerror\u201d because in some cases, an exception might not be considered to be an error at all. You can sometimes think of an exception as just another way to organize a program. Exceptions in Java are represented as objects of type Exception. Actual exceptions are defined by subclasses of Exception. Different subclasses\u00ae represent different types of exceptions We will look at only two types of exception in this section: NumberFormatException and IllegalArgumentException. A NumberFormatException can occur when an attempt is made to convert a string into a number. Such conversions are done by the functions Integer.parseInt and Integer.parseDouble. Consider the function call Integer.parseInt(str) where str is a variable of type String. If the value of \u00aestr is the string \\\"42\\\", then the function call will correctly convert the st\u00aering into the int 42. However, if the value of str is, say, \\\"fred\\\", t\u00aehe function call will fail because \\\"fred\\\" is not a legal string representation of an int value. In this case, an exception of type NumberFormatException occurs. If nothing is done to handle the exception, the program will crash. An IllegalArgumentException c\u00aean occur when an illegal value is passed as a parameter to a subroutine. For example, if a subroutine requires that a parameter be greater than or equal to zero, an IllegalArgumentException might occur when a negative value is passed to the subroutine. How to respond to the illegal value is up to the person who wrote the subroutine, so we can\u2019t simply say that every illegal parameter value will result in an IllegalArgumentException. However, it is a co\u00aemmon response. One case \u00aewhere an IllegalArgumentException can occur is in the valueOf function of an enumerated type. Recall from Subsection 2.3.3 that this function tries to convert a string into one of the values of the enumerated type. If the string that is passed as a parameter to valueOf is not the name of one of the enumerated type\u2019s values, then an IllegalArgumentException occurs. For example, given the enumerated type Toss.valueOf(\\\"HEADS\\\") correctly returns the value Toss.HEADS, while Toss.valueOf(\\\"FEET\\\") results in an IllegalArgumentException. When an exception occurs, we say that the exception is \u201cthrown\u201d. For example, we say that Integer.parseInt(str) throws an exception of type NumberFormatException when the value of str is illegal. When an exception is thrown, it is possible to \u201ccatch\u201d the exception and prevent it from crashing the program. This is done with a trycatch statement. The hexception-class-namei could be NumberFormatException, IllegalArgumentException, or some other exception class. When the computer executes this statement, it executes the statements in the try part. If no error occurs during the execution of hstatements-1i, then the computer just skips over the catch part and proceeds with the rest of the program. However, if an exception of type hexception-class-namei occurs during the execution of hstatements-1i, the computer immediately jumps to the catch part and executes hstatements-2i, skipping any remaining statements in hstatements-1i. During the execution of hstatements-2i, the hvariablenamei represents the exception object, so that you can, for example, print it out. At the end of the catch part, the computer proceeds with the rest of the program; the exception has been caught and handled and does not crash the program. Note that only one type of exception is caught; if some other type of exception occurs during the execution of hstatements-1i, it will crash the program as usual.",
    "page35": "Exceptions in TextIO- When TextIO reads a numeric value from the user, it makes sure that the user\u2019s response is legal, using a technique similar to the while loop and try..catch in the previous example. However, TextIO can read data from other sources besides the user. When it is reading from a file, there is no reasonable way for TextIO to recover from an illegal value in the input, so it responds by throwing an exception. To keep things simple, TextIO only throws exceptions of type IllegalArgumentException, no matter what type of error it encounters. For example, an exception will occur if an attempt is made to read from a file after all the data in the file has already been read. In TextIO, the exception is of type IllegalArgumentException. If you have a better response to file errors than to let the program crash, you can use a try..catch to catch excep\u00aetions of type IllegalArgumentException. For example, suppose that a file contains nothing but real numbers, and we want a program that will read the \u00aenumbers and find their sum and their average. Since it is unknown how many numbers are in the file, there is the question of when to stop reading. One approach is simply to try to keep reading indefinitely. When the end of the file is reached, an exception occurs. This exception is \u00aenot really an error it\u2019s just a way of detecting the end of the data, so we can catch the exception and finish up the program. We can read the data in a while (true) loop and break out of the loop when an exception occurs. This is an example of the somewhat unusual technique of using\u00ae an exception as part of the expected flow of control in a program. To read from the file, we need to know the file\u2019s name. To make the program more general, we can let the user enter the file name, instead of hard-coding a fixed file name in the program. However, it is possible that the user will enter the name of a file that does not exist. When we use TextIO.readfile to open a file that does not exist, an exception of type IllegalArgumentException occurs. We can catch this exception and ask the user to enter a different file name\u00ae. Here is a complete program that uses all these ideas Introduction to GUI Programming- For the past two chapters, you\u2019ve been learning the sort of programming that is done inside a single subroutine. In the rest of the text, we\u2019ll be more concerned with the larger scale structure of programs, but the material that you\u2019ve already learned will be an important foundation for everything to come. In this section, before moving on to programming-in-the-large, we\u2019ll take a look at how programming-in-the-small can be used in other contexts besides text-based, command-linestyle programs. We\u2019ll do this by taking a short, introductory look at applets and graphical programming. An applet is a Java program that runs on a Web page. An applet is not a stand-alone application, and it does not have a main() routine. In fact, an applet is an object rather than a class. When Java first appeared on the scene, applets were one of its major appeals. Since then, they have become less important, although they can still be very useful. When we study GUI programming in Chapter 6, we will concentrate on stand-alone GUI programs rather than on applet\u00aes, but applets are a good place to start for our first look at the subject. When an applet is placed on a Web page, it is a\u00aessigned a rectangular area on the page. It is the job of the applet to draw the contents of that rectangle. When the region needs to be drawn, the Web page calls a subroutine in the applet to do so. This is not so different from what happens with stand-alone programs. When such a program needs to be run, the system calls the main() routine of the program. Similarly, when an applet needs to be drawn, the Web page calls the paint() routine of the applet. The programmer specifies what happens when these routines are called by filling in the bodies of the routines. Programming in the small! Applets can do other things besides draw themselves, such as responding when the user clicks the mouse on the applet. Each of the applet\u2019s behaviors is defined by a subroutine. The programmer specifies how the applet behaves by filling in the bodies of the appropriate subroutines. A very simple applet, which does nothing but draw itself, can be defined by a class that contains nothing but a paint() routine. The source code for the class would then have the form where name-of-applet is an identifier that names the class, and the statements are the code that actually draws the applet. This looks similar to the definition of a stand-alone program, but there are a few things here that need to be explained, starting with the fir\u00aest two lines. When you write a program, there are certain built-in classes that are available for you to use. These built-in classes include System and Math. If you want to use one of these classes, you don\u2019t have to do anything special. You just go ahead and use it. But Java also has a large number of standard classes that are there if you want them but that are not automatically available to your program. (There are just too many of them.) If you want to use these classes in your program, you have to ask for them first. The standard classes are grouped into so-called \u201cpackages.\u201d Two of these packages are called \u201cj\u00aeava.awt\u201d and \u201cjava.applet\u201d. The directive \u201cimport java.awt.*;\u201d makes all the classes from the package jav\u00aea.awt available for use in your program. The java.awt package contains classes related to graphical user interface programming, including a class called Graphics. The Graphics class is referred to in the paint() routine above. The java.applet package contains classes specifically related to applets, including the class named Applet.",
    "page36": "The first line of the class definition above says that the class \u201cextends Applet.\u201d Applet is a standard class that is defined in the java.applet package. It defines all the basic properties and behaviors of applet objects. By extending the Applet class, the new class we are defining inherits all those properties and behaviors. We only have to define the ways\u00ae in which our class differs from the basic Applet c\u00aelass. In our case, the only difference is that our applet will draw itself differently, so we only have to define the paint() routine that does the drawing. This\u00ae is one of the main advantages of object-oriented programming. (Actually, in the future, our applets will be defined to extend JApplet rather than Applet. The JApplet class is itself an extension of Applet. The Applet class has existed since the original version of Java, while JApplet is part of the newer \u201cSwing\u201d set of graphical user interface components. For the moment, the distinction is not important.) One more thing needs to be mentioned and this is a po\u00aeint where Java\u2019s syntax gets unfortunately confusing. Applets are objects, not classes. Instead of being static members of a class, the subroutines that define the applet\u2019s behavior are part of the applet object. We say that they are \u201cnon-static\u201d subroutines. Of course, objects are related to classes because every object is described by a class. Now here is the part that can get confusing: Even though a non-static subroutine is not actually part of a class (in the sense of being part of the behavior of the class), it is nevertheless defined in a class (in the sense that the Java code that defines the subroutine is part of the Java code that defines the class). Many objects can be described by the same class. Each object has its own non-static subroutine. But the common definition of those subroutines the actual Java source code is physically part of the class that describes all the objects. To put it briefly: static subroutines in a class definition say what the class does; non-static subroutines say what all the objects described by the class do. An applet\u2019s paint() routine is an example of a non-static subroutine. A stand-alone program\u2019s main() routine is an example of a static subroutine. The distinction doesn\u2019t really matter too much at this point: When working with stand-alone programs, mark everything with the reserved word, \u201cstatic\u201d; leave it out when working with applets. However, the distinction between static and non-static will become more important later in the course. Let\u2019s write an applet that draws something. In order to write an applet that draws something, you need to know what subroutines are available for drawing, just as in writing textoriented programs you need to know what subroutines are available for reading and writing text. In Java, the built-in drawing subroutines are found in objects of the class Graphics, one of the classes in the java.awt package. In an applet\u2019s paint() routine, you can use the Graphics object g for drawing. (This object is provided as a parameter to the paint() routine when that routine is called.) Graphics objects contain many subroutines. I\u2019ll mention just three of them here. g.setColor(c), is called to set the color that is used for drawing. The parameter, c is an object belonging to a class named Color, another one of the classes in the java.awt package. About a dozen standard colors are available as\u00ae static member variables in the Color cla\u00aess. These standard colors include Color.BLACK, Color.WHITE, Color.RED, Color.GREEN, and Color.BLUE. For example, if you want to draw in red, you would say \u201cg.setColor(Color.RED);\u201d. The specified color is used for all subsequent drawing operations up until the next time setColor is called. g.drawRect(x,y,w,h) draws the outline of a rectangle. The parameters x, y, w, and h must be integer-valued expressions. This subroutine draws the outline of the rectangle whose top-left corner is x pixels from the left edge of the applet and y pixels down from the top of the applet. The width of the rectangle is w pixels, and the height is h pixels. g.fillRect(x,y,w,h) is similar to drawRect except that it fills in the inside of the rectangle instead of just drawing an outline. The applet first fills its entire rectangular area with red. Then it changes the drawing color to black and draws a sequence of rectangles, where each rectangle is nested inside the previous one. The rectangles can be drawn with a while loop. Each time through the loop, the rectangle gets smaller and it moves down and over a bit. We\u2019ll need variables to hold the width and height of the rectangle and a variable to record how far the top-left corner of the rectangle is inset from the edges of the applet. The while loop ends when the rectangle shrinks to nothing. In general outline, the algorithm for drawing the applet is Set the drawing color to red (using the g.setColor subroutine) Fill in the entire applet (using the g.fillRect subroutine) Set the drawing color to black Set the top-left corner inset to be 0 Set the rectangle width and height to be as big as the applet while the width and height are greater than zero: draw a rectangle (using the g.drawRect subroutine) increase the inset decrease the width and the height",
    "page37": "When you write an applet, you get to build on the work of the people who wrote the Applet class. The Applet class provides a framework on which you can hang your own work. Any programmer can create additional frameworks that can be used by other programmers as a basis for writing specific types of applets or stand-alone programs. I\u2019ve written a small framework that makes it possible to write applets that display simple animations. One example that we will consider is an animated version of the nested rectangles applet from earlier in this section. You can see the applet in action at the bottom of the on-line version of this page. A computer animation is really just a sequence of still images. The computer displays the images one after the other. Each image differs a bit from the preceding image in the sequence. If the differences are not too big and if the sequence is displayed quickly enough, the eye is tricked into perceiving continuous motion. In the example, rectangles shrink continually towards the center of the applet, while new rectangles appear at the edge. The perpetual motion is, of course, an illusion. If you think about it, you\u2019ll see that the applet loops through the same set of images over and over. In each image, there is a gap between the borders of the applet and the outermost rectangle. This gap gets wider and wider until a new rectangle appears at the border. Only it\u2019s not a new rectangle. What has really happened is that the applet has started over again with the first image in the sequence.The \u201cimport java.awt.*;\u201d is required to get access to graphics-related classes such as Graphics and Color. You get to fill in any name you want for the class, and you get to fill in the statements inside the subroutine. The drawFrame() subroutine will be called by the system each time a frame needs to be drawn. All you have to do is say what happens when this subroutine is called. Of course, you have to draw a different picture for each frame, and to do that you need to know which frame you are drawing. The class SimpleAnimationApplet2 provides a function named getFrameNumber() that you can call to find out which frame to draw. This function returns an integer value that represents the frame number. If the value returned is 0, you are supposed to draw the first frame; if the value is 1, you are supposed to draw the second frame, and so on. In the sample applet, the thing that differs from one frame to another is the distance between the edges of the applet and the outermost rectangle. Since the rectangles are 15 pixels apart, this distance increases from 0 to 14 and then jumps back to 0 when a \u201cnew\u201d rectangle appears. The appropriate value can be computed very simply from the frame number, with the statement \u201cinset = getFrameNumber() % 15;\u201d. The value of the expression getFrameNumber() % 15 is between 0 and 14. When the frame number reaches 15 or any multiple of 15, the value of getFrameNumber() % 15 jumps back to 0. Drawing one frame in the sample animated applet is very similar to drawing the single image of the StaticRects applet, as given above. The paint() method in the StaticRects applet becomes, with only minor modification, the drawFrame() method of my MovingRects animation applet. I\u2019ve chosen to make one improvement: The StaticRects applet assumes that the applet is 300 by 160 pixels. The MovingRects applet will work for any applet size.\u00ae To implement this, the drawFrame() routine has to know how big the applet is. There are two functions that can be called to get this information. The function getWidth() returns an integer value representing the width of the applet, and the function getHeight() returns the height.One way to break up a complex program into manageable pieces is to use subroutines. A subroutine consists of the instructions for carrying out a certain task, grouped together and given a name. Elsewhere in the program, that name can be used as a stand-in for\u00ae the whole set of instructions. As a computer executes a program, whenever it encounters a subroutine name, it executes all the instructions necessary to carry out the task associated with that subroutine. Subroutines can be used over and over, at different places in the program. A subroutine can even be used inside another subroutine. This allows you to write simple subroutines and then use them to help write more complex subroutines, which can then be used in turn in other subroutines. In this way, very complex programs can be built up step-by-step, where each step in the construction is reasonably simple.Black Boxes A subroutine consists of instructions for performing some task, chunked together and given a name. \u201cChunking\u201d allows you to deal with a potentially very complicated task as a single concept. Instead of worrying about \u00aethe many, many steps that the computer might have to go though to perform that task, you just need to remember the name of the subroutine. Whenever you want your program to perform the task, you just call the subroutine. Subroutines are a major tool for dealing with complexity.",
    "page38": "A subroutine is sometimes said to be a \u201cblack box\u201d because you can\u2019t see what\u2019s \u201cinside\u201d it (or, to be more precise, you usually don\u2019t want to se\u00aee inside it, because then you would have to deal with all the complexity that the subroutine is meant to hide). Of course, a black box that has no \u00aeway of interacting with the rest of the world would be pretty useless. A \u00aeblack box needs some kind of interface with the rest of the world, which allows some interaction between what\u2019s inside the box and what\u2019s outside. A physical black box might have buttons on the outside that you can push, dials that you can set, and slots that can be used for passing information back and forth. Since we are trying to hide\u00ae complexity, not create it, we have the first rule of black boxes The interface of a black box should be fairly straight forward, well-defined, and easy to understand Are there any examples of black boxes in the real world? Yes; in fact, you are surrounded by them. Your television, your car, your VCR, your refrigerator.  You can turn your television on and off, change channels, and set the volume by using elements of the television\u2019s interface dials, remote control, don\u2019t forg\u00aeet to plug in the power without understanding anything about how the thing actually works. The same goes for a VCR, although if the stories are true about how hard people find it to set the time on a VCR, then maybe the VCR violates the simple interface rule. Now, a black box does have an inside the code in a subroutine that actually performs the task, all the electronics inside your television set. The inside of a black box is called its implementation. The second rule of black boxes is that To use a black box, you shouldn\u2019t need to know anything about its implementation; all you ne\u00aeed to know is its interface. In fact, it should be possible to change the implementation, as long as the behavior of the box, as seen from the outside, remains unchanged. For example, when the insides of TV sets went from using vacuum tubes to using transistors, the users of the sets didn\u2019t even need to know about it or even know what it means. Similarly, it should be possible to rewrite the inside of a subroutine, to use more efficient code, for example, without affecting the programs that use that subroutine. Of course, to ha\u00aeve a black box, someone must have designed and built the implementation in the first place. The black box idea works to the advantage of the implementor as well as of the user of the black box. After all, the black box might be used\u00ae in an unlimited number of different situations. The implementor of the black box doesn\u2019t need to know about any of that. The implementor just needs to make sure that the box performs \u00aeits assigned task and interfaces correctly with the rest of the world. This is the third rule of black boxes. The implementor of a black box should not need to know anything about the larger systems in which the box will be used. In a way, a black box divides the worl\u00aed into two parts: the inside (implementation) and the outside. The interface is at the boundary, connecting those two parts. By the way, you should not think of an interface as just the physical connection between the box and the rest of the world. The interface also includes a specification of what the box does and how it can be controlled by using the elements of the physical interface. It\u2019s not enough to say that a TV set has a power switch; you need to specify that the power switch is used to turn the TV on and off! To put this in computer science terms, the interface of a subroutine has a semantic as well as a syntactic component. The syntactic part of the interface tells you just what you have to type in order to call the subroutine. The semantic component specifies exactly what task the subroutine will accomplish. To write a legal program, you need to know the syntactic specification of the subroutine. To understand the purpose of the subroutine and to use it effectively, you need to know the subroutine\u2019s semantic specification. I will refer to both parts of the interface syntactic and semantic collectively as the contract of the subroutine.",
    "page39": "The contract of a subroutine says, essentially, \u201cHere is what you have to do to use me, and here is what I will do for you, guaranteed.\u201d When you write a subroutine, the comments that you write for the subroutine should make the contract very clear. (I should admit that in practice, subroutines\u2019 contracts are often \u00aeinadequately specified, much to the regret and annoyance of the programmers who have to use them.) For the rest of this chapter, I turn from general ideas about black boxes and subroutines in general to the specifics of writing and using subroutines in Java. But keep the general ideas and principles in mind. They are the reasons that subroutines exist in the first place, and they are your guidelines for using them. This should be especially clear in Section 4.6, where I will discuss subroutines as a tool in program development. You should keep in mind that subroutines are not the only example of black boxes in programming. For example, a class is also a black box. We\u2019ll see that a class can have a \u201cpublic\u201d part, representing its interface, and a \u201cprivate\u201d part that is entirely inside its hidden implementation. All the principles of black boxes apply to classes as well as to subroutines. Static Su\u00aebroutines and Static Variables- Every subroutine in Java must be defined in\u00aeside some class. This makes Java rather unusual among programming languages, since most languages allow fre\u00aee-floating, independent subroutines. One purpose of a class is to group together related subroutines and variables. Perhaps the designers of Java felt that everything must be related to something. As a less philosophical motivation, Java\u2019s designers wanted to place firm controls on the ways things are named, since a Java program potentially has access to a huge number of subroutines created by many different programmers. The fact that those subroutines are grouped into named classes (and classes are grouped into named \u201cpackages\u201d) helps control the confusion that might result from so many different names. A subroutine that is a member of a class is often called a m\u00aeethod, and \u201cmethod\u201d is the term that most people prefer for subroutines in Java. I will start using the term \u201cmethod\u201d occasionally; however, I will continue to prefer the more general term \u201csubroutine\u201d for static subroutines. I will use the term \u201cmethod\u201d most often to refer to non-static subroutines, which belong to objects rather than to classes. This chapter will deal with static subroutines almost exclusively. Subroutine Definitions- It will take us a while most of the chapter to get through what all this means in detail. Of course, you\u2019ve already seen examples of subroutines in previous chapters, such as the main() routine of a program and the paint() routine of an applet. So you are familiar with the general format. The statements between the braces, { and }, in a subroutine definition make up the body of the subroutine. These statements are the inside, or implementation part, of the \u201cblack box\u201d, The modifiers that can occur at the beginning of a subroutine definition are words that set certain characteristics of the subroutine, such as whether it is static or not. The modifiers that you\u2019ve seen so far are \u201cstatic\u201d and \u201cpublic\u201d. There are only about a half-dozen possible modifiers altogether. If the subroutine is a function, whose job is to compute some value, then the return-type is used to specify the type of v\u00aealue that is returned by the function. We\u2019ll be looking at functions an\u00aed return types in some detail in Section 4.4. If the subroutine is not a function, then the return-type is replaced by the special value void, which indicates that no value is returned. The term \u201cvoid\u201d is meant to indicate that the return value is empty or non-existent. Finally, we come to the parameter-list of the method. Parameters are part of the interface of a subroutine. They represent information that is passed into the subroutine from outside, to be used by the subroutine\u2019s internal computations. For a concrete example, imagine a class named Television that includes a method named changeChannel(). The immediate question is: What channel should it change to? A parameter can be used to answer this question. Since the channel number is an integer, the type of the parameter would be int, and the declaration of the changeChannel() method might look like public void changeChannel(int channelNum){statement} This declaration specifies that changeChannel() has a parameter named channelNum of type int. However, channelNum does not yet have any particular value. A value for channelNum is provided when the subroutine is called; for example: changeChannel(17); The parameter list in a subroutine can be empty, or it can consist of one or more parameter declarations of the form htypei hparameter-namei. If there are several declarations, they are separated by commas. Note that each declaration can name only one parameter. For example, if you want\u00ae two parameters of type double, you have to say \u201cdouble x, double y\u201d, rather than \u201cdouble x, y\u201d.",
    "page40": "the modifiers are public and static, the return type is void, the subroutine name is main, and the parameter list is \u201cString[] args\u201d. The only question might be about \u201cString[]\u201d, which has to be a type if it is to match the syntax of a parameter list. In fact, String[] represents a so-called \u201carray type\u201d, so the syntax is valid. (The parameter, args, represents information provided to the program when the main() routine is called by the system. In case you know the term, the information consists of any \u201ccommand-line arguments\u201d specified in the command that the user typed to run the program.) You\u2019ve already had some experience with filling in the implementation of a subroutine. In this chapter, you\u2019ll learn all about writing your own complete subroutine definitions, including the interface part. Calling Subroutines- When you define a subrout\u00aeine, all you are doing is telling the computer that the subroutine exists and what it does. The subroutine doesn\u2019t actually get executed until it is called. (This is true even for the main() routine in a class even though you don\u2019t call it, it is called by the system when the system runs your program.) For example, the playGame() method given as an example above could be called using the following subroutine call statement: playGame(); This statement could occur anywhere in the same class that includes the definition of playGame(), whether in a main() method or in some other subroutine. Since playGame() is a public method, it can also be called from other classes, but in that case, you have to tell the computer which class it comes from. Since playGame() is a static method, its full name includes the name of the class in which it is defined. Let\u2019s say, for example, that playGame() is defined in \u00aea class named Poker. Then to call playGame() from outside the Poker class, you would have \u00aeto say Poker.playGame(); The use of the class name here tells the computer which class to look in to find the method. It also lets you distinguish between Poker.playGame() and other potential playGame() methods defined in other classes, such as Roulette.playGame() or Blackjack.playGame(). More generally, a subroutine call statement for a static subroutine takes the form subroutine-namei(parameters); if the subroutine that is being called is in the same class, or class-name.subroutine-namei(parameters); if the subroutine is defi\u00aened elsewhere, in a different class. (Non-static methods belong to objects rather than classes, and they are called using object names instead of class names. More on that later.) Note that the parameter list can be empty, as in the playGame() example, but the parentheses must be there even if there is nothing between them. Subroutines in Programs- It\u2019s time to give an example of what a complete program looks like, when it includes other subroutines in addition to the main() routine. Let\u2019s write \u00aea program that plays a guessing game with the user. The computer will choose a random number between 1 and 100, and the user will try to guess it. The computer tells the user whether the guess is high or low or correct. If the user gets the number after six guesses or fewer, the user wins the game. After each game, the user has the option of continuing with another game. Since playing one game can be thought of as a single, coherent task, it makes sense to write a subroutine that will play one guessing game with the user. The main() routine will use a loop to call the playGame() subroutine over and ov\u00aeer, as many times as the user wants to play. We approach the problem of designing the playGame() subroutine the same way we write a main() routine: Start with an outline of the algorithm and apply stepwise refinement. Here is a short pseudocode algorithm for a guessing game program. Pick a random number while the game is not over: Get the user\u2019s guess Tell the user wheth\u00aeer the guess is high, low, or correct. The test for whether the game is over is complicated, since the game ends if either the user makes a correct guess or the number of guesses is six. As in many cases, the easiest thing to do is to use a \u201cwhile (true)\u201d loop and use break to end the loop whenever we find a reason to do so. Also, if we are going to end the game after six guesses, we\u2019ll have to keep track of the number of guesses that the user has made. Filling out the algorithm gives.",
    "page41": "Let computersNumber be a random number between 1 and 100 Let guessCount = 0 while (true): Get the user\u2019s guess Count the guess by adding 1 t\u00aeo guess count if the user\u2019s guess equals computersNumber: Tell the user he won break out of the loop if the number of guesses is 6: Tell\u00ae the user he lost break out of the loop if the user\u2019s guess is less than computersNumber: Tell the user the guess was low else if the user\u2019s guess is higher than computersNumber: Tel\u00ael the user the guess was high With variable declarations added and translated into Java, this becomes the definition of the playGame() routine. A random integer between 1 and 100 can be computed as (int)(100 * Math.random()) + 1. I\u2019ve cleaned up the interaction with the user to make it flow better. Member Variables- A class can include other things besides subroutines. In particular, it can also include variable declarations. Of course, you can declare variables inside subroutines. Those are called local variables. However, you can also have variables that are not part of any subroutine. To distinguish suc\u00aeh variables from local variables, we call them member \u00aevariables, since they are members of a class. Just as with subroutines, member variables can be either static or non-static. In this chapter, we\u2019ll stick to static variables. A static member variable belongs to the class itself, and it exists as long as the class exists. Memory is allocated for the variable when the class is first loaded by the Java interpreter. Any assignment statement that assigns a value to the variable changes the content of that memory, no matter where that assignment statement is located in the program. Any time the variable is used in an expression, the value is fetched from that same memory, no matter where the expression is located in the program. This means that the value of a static member variable can be set in one subroutine and used in a\u00aenother subroutine.\u00ae Static member variables are \u201cshared\u201d by all the static subroutines in the class. A local variable in a subroutine, on the other hand, exists only while that subroutine is being executed, and is completely inaccessible from outside that one subroutine. A static member variable that is not declared to be private can be accessed from outside the class where it is defined, as well as inside. When it is used in some other class, it must be referred to with a compound identifier of the form hclass-namei.hvariable-namei. For example, the System class contains the public static member variable named out, and you use this variable in your own classes by referring to System.out. If numberOfPlayers is a public static member variable in a class named Poker, then subroutines in the Poker class would refer to it simply as numberOfPlayers, while subroutines in another class would refer to it as Poker.numberOfPlayers. As an example, let\u2019s add a static member variable to the GuessingGame class that we wrote earlier in this section. This variable will be used to keep track of how many games the user wins. We\u2019ll call the variable gamesWon and declare it with the statement \u201cstatic int gamesWon;\u201d. In the playGame() routine, we add 1 to gamesWon if the user wins the game. At the end of the main() routine, we print out the value of gamesWon. It wo\u00aeuld be impossible to do the same thing with a local variable, since we need access to the same variable from both subroutines. When you declare a local variable in a subroutine, you have to assign a value to that variable before you can do anything with it. Member variables, on the other hand are automatically initialized with a default value. For numeric variables, the default value is zero. For boolean variables, the default is false. And for char variables, it\u2019s the unprintable character that has Unicode code number zero. (For objects, such as Strings, the default initial value is a special value called null, which we won\u2019t encounter officially until later.) Since it is of type int, the static member variable gamesWon automatically gets assigned an initial value of zero. This happens to be the correct initial value for a variable that is being used as a counter. You can, of course, assign a different value to the variable at the beginning of the main() routine if you are not satisfied with the default initial value. Parameters- If a subroutine is a black box, then a parameter p\u00aerovides a mechanism for passing information from the outside world into the box. Parameters are part of the interface of a subroutine. They allow you to customize the behavior of a subroutine to adapt it to a particular situation. As an analogy, consider a thermostat a black box whose task it is to keep your house at a certain temperature. The thermostat has a parameter, namely the dial that is used to set the desired temperature. The thermostat always performs the same task: maintaining a constant temperature. However, the exact task that it performs that is, which temperature it maintains is customized by the setting on its dial.",
    "page42": "Using Parameters- As an example, let\u2019s go back to the \u201c3N+1\u201d problem that was discussed in Subsection 3.2.2. (Recall that a 3N+1 sequence is computed according to the rule, \u201cif N is odd, multiply by 3 and add 1; if N is even, divide by 2; continue until N is equal to 1.\u201d For example, starting from N=3 we get the sequence: 3, 10, 5, 16, 8, 4, 2, 1.) Suppose that we want to write a subroutine to print out such sequences. The subroutine will always perform the same task: Print out a 3N+1 sequence. But the exact sequence it prints out depends on the starting value of N. So, the starting value of N would be a parameter to the subrouti\u00aene. The parameter list of this subroutine, \u201c(int startingValue)\u201d, specifies that the subroutine has one parameter, of type int. Within the body of the subroutine, the parameter name can be used in the same way as a variable name. However, the parameter gets its initial value from outside the subroutine. When the subroutine is called, a value must be provided for this parameter in the subroutine call statement. This value will be assigned to the parameter, startingValue, before the body of the subroutine i\u00aes executed. For example, the subroutine could be called using the subroutine call statement \u201cprint3NSequence(17);\u201d. When the computer executes this statement, the computer assigns the value 17 to startingValue and then executes the statements in the subroutine. This prints the 3N+1 sequence starting from 17. If K is a variable of type int, then when the computer executes the subroutine call statement \u201cprint3NSequence(K);\u201d, it will take the value of the variable K, assign that value to startingValue, and execute the body of the subroutine. The class that contains print3NSequence can contain a main() routine (or other subroutines) that call print3NSequence. For example, here is a main() program that prints out 3N+1 sequences for various starting values specified by the user Formal and Actual Parameters Note that the term \u201cparameter\u201d is used to refer to two different, but related, concepts. There are parameters that are used in the definitions of subroutines, such as startingValue in the above example. And there are parameters that are used in subroutine call statements, such as the K in the statement \u201cprint3NSequence(K);\u201d. Parameters in a subroutine definition are called formal parameters or dummy parameters. The parameters that are passed to a subroutine when it is called are called actual parameters or arguments. When a subroutine is called, the actual parameters in the subroutine call statement are evaluated and the values are assigned to the \u00aeformal parameters \u00aein the subroutine\u2019s definition. Then the body of the subroutine is execut\u00aeed. A formal parameter must be a name, that is, a simple identifier. A formal parameter is very much like a variable, and like a variable it has a specified type such as int, boolean, or String. An actual parameter is a value, and so it can be specified by any expression, provided that the expression computes a value of the correct type. The type of the actual parameter must be one that could legally be assigned to the formal parameter with an assignment statement. For example, if the formal parameter is of type double, then it would be legal to pass an int as the actual parameter since ints can legally be assigned to doubles. When you call a subroutine, you must provide one actual parameter for each formal parameter in the subroutine\u2019s definition. Consider, for example, a subroutine. static void doTask(int N, double x, boolean test) { // statements to perform the task go here } This subroutine might be called with the statement doTask(17, Math.sqrt(z+1), z >= 10); When the computer executes this statement, it has essentially the same effect as the block of statements: { int N; // Allocate memory locations for the formal parameters. double x; boolean test; N = 17; // Assign 17 to the first formal parameter, N. x = Math.sqrt(z+1); // Compute Math.sqrt(z+1), and assign it to // the second formal parameter, x. test = (z >= 10); // Evaluate \\\"z >= 10\\\" and assign the resulting // true/false value to the third formal // parameter, test. // statements to perform the task go here } Beginning programming students often find parameters to be surprisingly confusing. Calling a subroutine that already exists is not a problem the idea of providing information to the subroutine in a parameter is clear enough. Writing the subroutine definition is another matter. A common mistake is to assign values to the formal parameters at the beginning of the subroutine, or to ask the user to input their values. This represents a fundamental misunderstanding. When the statements in the subroutine are executed, the formal parameters will already have values. The values come from the subroutine call statement. Remember t\u00aehat a subroutine is not independent. It is called by some other routine, and it is the calling routine\u2019s responsibility to provide appropriate values for the parameters.",
    "page43": "(There are a few technical differences between this and \u201cdoTask(17,Math.sqrt(z+1),z>=10);\u201d besides the amount of typing because of questions about scope of variables and what happens when several variables or parameters have the same name.) Beginning programming students often find parameters to be surprisingly confusing. Calling a subroutine that already exists is not a problem the idea of providing information to the subroutine in a parameter is clear enough. Writing the subroutine definition is another matter. A common mistake is to assign values to the formal parameters at the beginning of the subroutine, or to ask the user to input their values. This represents a fundamental misunderstanding. When the sta\u00aetements in the subroutine are executed, the formal parameters will already have values. Th\u00aee values come from the subroutine call statement. Remember that a subroutine is not independent. It is called by some other routine, and it is the calling routine\u2019s responsibility to provide appropriate values for the parameters. Overloading In order to call a subroutine legally, you need to know its name, you need to know how many formal parameters it has, and you need to know the type of each parameter. This information is called the subroutine\u2019s signature. The signature of the subroutine doTask, used as an example above, can be expressed as as: doTask(int,double,boolean). Note that the signature does not include the names of the \u00aeparameters; in fact, if you just want to use the subroutine, you don\u2019t even need to know what the formal parameter names are, so t\u00aehe names are not part of the interface. Java is somewhat unusual in that it allows two different subroutines in the same class to have the same name, provided that their signatures are different. (The language C++ on which Java is based also has this feature.) When this happens, we say that the name of the subroutine is overloaded because it has several different meanings. The computer doesn\u2019t get the subroutines mixed up. It can tell which one you want to call by the number and types of the actual parameters that you provide in the subroutine call statement. You have already seen overloading used in the TextIO class. This class includes many different methods named putln, for example. These methods all have different si\u00aegnatures, such as: putln(int) putln(double) putln(String) putln(char) putln(boolean) putln() The computer knows which of these subroutines you want to use based on the type of the actual parameter that \u00aeyou provide. TextIO.\u00aeputln(17) calls the subroutine with signature putln(int), while TextIO.putln(\\\"Hello\\\") calls the subroutine with signature putln(String). Of course all these different subroutines are semantically related, which is why it is acceptable programming style to use the same name for them all. But as far as the computer is concerned, printing out an int is very different from printing out a String, which is different from printing out a boolean, and so forth so that each of the\u00aese operations requires a different method. Note, by the way, that the signature does not include the subroutine\u2019s return type. It is illegal to have two subroutines in the same class that have the same signature but that have different return types. For example, it would be a syntax error for a class to contain two methods defined as: int getln() { ... } double getln() { ... } So it should be no surprise that in the TextIO class, the methods for reading different types are not all named getln(). In a given class, there can only be one routine that has the name getln and has no parameters. So, the input routines in TextIO are distinguished by having different names, such as getlnInt() and getlnDouble(). Java 5.0 introduced another complication: It is possible to have a single subroutine that takes a variable number of actual parameters. You have already used subroutines that do this the formatted output routines System.out.printf and TextIO.putf. When you call these subroutines, the number of parameters in the subroutine call can be arbitrarily large, so it would be impossible to have \u00aedifferent subroutines to handle each case. Unfortunately, writing the definition of such a subroutine requires\u00ae some knowledge of arrays, which will not be covered until Chapter 7. When we get to that chapter, you\u2019ll learn how to write subroutines with a variable number of parameters. For now, we will ignore this complication.",
    "page44": "Throwing Exceptions- I have been talking about the \u201ccontract\u201d of a subroutine. The contract says what the subroutine will do, provided that the caller of the subroutine provides acceptable values for subroutine\u2019s parameters. The question arises, though, what should the subroutine do when the caller violates the contract by providing bad parameter values? We\u2019ve already seen that some subroutines respond to bad parameter values by throwing exceptions. For example, the contract of the built-in subroutine Double.pa\u00aerseDouble says that the parameter should be a string representation of a number of type double; if this is true, then the subroutine will convert the string into the equivalent numeric value. If the caller violates the contract by passing an invalid string as the actual parameter, the subroutine responds by throwing an exception of type NumberFormatException. Many subroutines throw IllegalArgumentExceptions in response to bad parameter values. You might want to take this response in your own subroutines. This can be done with a throw statement. An exception is an object, and in order to throw an exception, you must create an exception object. You \u00aewon\u2019t officially learn how to do this until Chapter 5, but for now, you can use the following syntax for a throw statement that throws an IllegalArgumentException throw new IllegalArgumentException( error-message ); where error-message is a string that describes the error that has been detected. (The word \u201cnew\u201d in this statement is what creates the object.) To use this statement in a subroutine, you would check whether the values of the parameters are legal. If not, you would throw the exception. For example, consider the print3NSequence subroutine from the beginning of this section. The parameter of print3NSequence is supposed to be a positive integer. We can modify the subroutine definition to make it throw an exception when this condition is violated Global and Local Variables I\u2019ll finish this section on parameters by noting that we now have three different sorts of variables that can be used inside a subroutine: local variables declared in the subroutine, formal parameter names, and static member variables that are declared outside the subroutine but inside the same class as the subroutine. Local variables have no connection to the outside world; they are purely part of the internal working of the subroutine. Parameters are used to \u201cdrop\u201d values into the subroutine when it is called, but once the subroutine starts executing, parameters act much like local variables. Changes made inside a subroutine to a formal parameter have no effect on the rest of the program (at least if the type of the parameter is one of the primitive types things are more complicated in the case of objects, as we\u2019ll see later). Things are different when a subroutine uses a variable that is defined outside the subroutine. That variable exists independently of the subroutine, and it is accessible to other parts of the program, as well as to the subroutine. Such a variable is said to be gl\u00aeobal to the subroutine, as opp\u00aeosed to the local variables defined inside the subroutine. The scope of a global variable includes the entire class in which it is defined. \u00aeChanges made to a global variable can have effects that extend outside the subroutine where the changes are\u00ae made. You\u2019ve seen how this works in the last example in the previous section, where the value of the global variable, gamesWon, is computed inside a subroutine and is used in the main() routine. It\u2019s not always bad to use global variables in subroutines, but you should realize that the global variable then has to be considered part of the subroutine\u2019s interface. The subroutine uses the global variable to communicate with the rest of the program. This is a kind of sneaky, back-door communication that is less visible than communication done through parameters, and it risks violating the rule that the interface of a black box should be straightforward and easy to understand. So before you use a global variable in a subroutine, you should consider whether it\u2019s really necessary. I don\u2019t advise you to take an absolute stand against using global variables inside subroutines. There is at least one good reason to do it: If you think of the class as a whole as being a kind of black box, it can be very reasonable to let the subroutines inside that box be a little sneaky about communicating with each other, if that will make the class as a whole look simpler from the outside.",
    "page45": "Return Values A subroutine that returns a value is called a function. A given function can only return a value of a specifi\u00aeed type, called the return type of the function. A function call generally occurs in a position where the computer is expecting to find a value, such as the right side of an assignment statement, as an actual parameter in a subroutine call, or in the middle of some larger expression. A boolean-valued function can even be used as the test condition in an if, while, for or do..while statement. (It is also legal to use a function call as a stand-alone statement, just as if it were a regular subroutine. In this case, the computer ignores the value computed by the subroutine. Sometimes this makes sense. For example, the function TextIO.getln(), with a return type of String, reads and returns a line of input typed in by the user. Usually, the line that is returned is assigned to a variable to be used later in the program, as in the statement \u201cname = TextIO.getln();\u201d. However, this function is also useful as a subroutine call statement \u201cTextIO.getln();\u201d, which still reads all input up to and including the next carriage return. Since the return value is not assigned to a variable or used in an expression, it is simply discarded. So, the effect of the subroutine call is to read and discard some input. Sometimes, discarding unwanted input is exactly what you need to do.) The return statement You\u2019ve already seen how functions such as Math.sqrt() and TextIO.getInt() can be used. What you haven\u2019t seen is how to write functions of your own. A functio\u00aen takes the same form as a regular subroutine, except that you have to specify the value that is to be returned by the subroutine. This is done with a return statement, which has the following syntax: return expression ; Such a return statement can only occur inside the definition of a function, and the type of the hexpressioni must match the return type that was specified for the function. (More exactly, it must be legal to assign the expression to a variable whose type is specified by the return type.) When the computer executes this return statement, it evaluates the expression, terminates execution of the function, and uses the value of the expression as the returned value of the function. For example, consider the function definition static double pythagoras(double x, double y) { // Co\u00aemputes the length of the hypotenuse of a right // triangle, where the sides of the triangle are x and y. return Math.sqrt( x*x + y*y ); } Suppose the computer executes the statement \u201ctotalLength = 17 + pythagoras(12,5);\u201d. When it gets to the term pythagoras(12,5), it assigns the actual parameters 12 and 5 to the formal parameters x and y in the function. In the body of the function, it evaluates Math.sqrt(12.0*12.0 + 5.0*5.0), which works out to 13.0. This value is \u201creturne\u00aed\u201d by the function, so the 13.0 essentially replaces the function call in the statement \u201ctotalLength = 17 + pythagoras(12,5);\u201d. The return value is added to 17, and the result, 30.0, is stored in the variable, totalLength. The effect is the same as if the statement had been \u201ctotalLength = 17 + 13.0;\u201d. Note that a return statement does not have to be the last statement in the function definition. At any point in the function where you know the value that you want to return, you can return it. Returning a value will end the function immediately, skipping any subsequent statements in the function. However, it must be the case that the function definitely does return some value, no matter what path the execution of the function takes through the code. You can use a return statement inside an ordinary subroutine, one with decla\u00aered return type \u201cvoid\u201d. Since a void subroutine does not return a value, the return statement does not include an expression; it simply takes the form \u201creturn;\u201d. The effect of this statement is to terminate execution of the subroutine and return control back to the point in the program from which the subroutine was called. This can be convenient if you want to terminate execution somewhere in the middle of the subroutine, but return statements are optional in non-function subroutines. In a function, on the other hand, a return statement, with expression, is always required.",
    "page46": "APIs, Packages, and Javadoc- As computers and their user interfaces have become easier to use, they have also become more complex for programmers to deal with. You can write programs for a simple console-style user interface using just a few subroutines that write output to the console and read the user\u2019s typed replies. A modern graphical user interface, with windows, buttons, scroll bars, menus, text-input boxes, and so on, might make things easier for the user, but it forces the programmer to cope with a hugely expanded array of possibilities. The programmer sees this increased complexity in the form of great numbers of subroutines that are provided for managing the user interface, as well as for other purposes. Toolboxes- Someone who wants to program for Macintosh computers and to produce programs that look and behave the way users expect them to must deal with the Macintosh Toolbox, a collection of well over a thousand different subroutines. There are routines for opening and closing windows, for drawing geometric figures and text to windows, for adding buttons to windows, and for responding to mouse clicks on the window. There are other routines for creating menus and for reacting to user selections from menus. Aside from the user interface, there are routines for opening files and\u00ae reading data from them, for communicating over a network, for sending output to a printer, for handling communication between programs, and in general for doing all the standard things that a computer has to do. Microsoft Windows provides its own set of subroutines for programmers to use, and they are quite a bit different from the subroutines used on the Mac. Linux has several different GUI toolboxes for the programmer to choose from. The analogy of a \u201ctoolbox\u201d is a good one to keep in mind. Every programming project involves a mixture of innovation and reuse of existing tools. A programmer is given a set of tools to work with, starting with the set of basic tools that are built into the language: things like variables, assignment statements, if statements, and loops. To these, the programmer can add\u00ae existing toolboxes full of routines that have already been written for performing certain tasks. These tools, if they are well-designed, can be used as true black boxes: They can be called to perform their assigned tasks without worrying about the particular steps they go through to accomplish those tasks. The innovative part of programming is to take all these tools and apply them to some particular project or problem (word-processing, keeping track of bank accounts, processing image data from a space probe, Web browsing, computer games). This is called applications programming. A software toolbox is a kind of black box, and it presents a certain interface to the programmer. This interface is a specification of what routines are in the toolbox, what parameters they use, and what tasks they perform. This information constitutes the API , or Applications Programming Interface, associated with the toolbox. The Macintosh API is a specification of all the routines available in the Macintosh Toolbox. A company that makes some hardware devices say a card for connecting a computer to a network \u00aemight publish an API for that device consisting of a list of routines that programmers can call in order to communicate with and control the device. Scientists who write a set of routines for doing some kind of complex computation such as solving \u201cdifferential equations,\u201d say would provide an API to allow others to use those routines without understanding the details of the computations they perform.",
    "page47": "The Java programming language is supplemented by a large, standard API. You\u2019ve seen part of this API already, in the form of mathematical subroutines such as Math.sqrt(), the String data type and its associated routines, and the System.out.print() routines. The standard Java API includes routines for working with graphical user interfaces, for network communication, for reading and writing files, and more. It\u2019s tempting to think of these routines as being built into the Java language, but they are technically subroutines that have been written and made available for use in Java programs. Java is platform-independent. That is, the same program can run on platforms as diverse as Macintosh, Windows, Linux, and others. The same Java API must work on all these platforms. But notice that it is the interface that is platform-independent; the implementation varies from one platform to another. A Java system on a particular computer includes implementations of all the standard API routines. A Java program includes only calls to those routines. When the Java interpreter \u00aeexecutes a program and encounters a call to one of the standard routines, it will \u00aepull up and execute the implementation of that routine which is appropriate for the particular platform on which it is running. This is a very powerful idea. It means that you only need to learn one API to program for a wide variety of platforms. Java\u2019s Standard Packages Like all subroutines in Java, the routines in the standard API are grouped into classes. To provide larger-scale organization, classes in Java can be grouped into packages, which were introduced briefly in Subsection 2.6.4. You can have even higher levels of grouping, since packages can also contain other packages. In fact, the entire standard Java API is implemented in several packages. One of these, which is named \u201cjava\u201d, contains several non-GUI packages as well as the original AWT graphics user interface classes. Another package, \u201cjavax\u201d,\u00ae was added in Java version 1.2 and contains the classes used by the Swing graphical user interface and other additions to the API. A package can contain both classes and other packages. A package that is contained in another package is sometimes called a \u201csub-package.\u201d Both the java package and the javax package contain sub-packages. One of the sub-packages of java, for example, is called \u201cawt\u201d. Since awt is co\u00aentained within java, its full name is actually java.awt. This package contains classes that represent GUI components such as buttons and menus in the AWT, the older of t\u00aehe two Java GUI toolboxes, which is no longer widely used. However, java.awt also contai\u00aens a number of classes that form the foundation for all GUI programming, such as the Graphics class which provides routines for drawing on the screen, the Color class which represents colors, and the Font class which represents the fonts that are used to display characters on the screen. Since these classes are contained in the package java.awt, their full names are actually java.awt.Graphics, java.awt.Color, and java.awt.Font. (I hope that by now you\u2019ve gotten the hang of how this naming thing works in Java.) Similarly, javax contains a sub-package named javax.swing, which includes such classes as javax.swing.JButton, javax.swing.JMenu, and javax.swing.JFrame. The GUI classes in javax.swing, together with the foundational classes in java.awt, are all part of the API that makes it possible to program graphical user interfaces in Java. The java package includes several other sub-packages, such as java.io, which provides facilities for input/output, java.net, which deals with network communication, and java.util, which provides a variety of \u201cutility\u201d classes. The most basic package is called java.lang. This package contains fundamental classes such as String, Math, Integer, and Double. It might be helpful to look at a graphical representation of the levels of nesting in the java package, its sub-packages, the classes in t\u00aehose sub-packages, and the subroutines in those classes. This is not a complete picture, since it shows only a very few of the many items in each element. The official documentation for the standard Java 5.0 API lists 165 different packages, including sub-packages, and it lists 3278 classes in these packages. Many of these are rather obscure or very specialized, but you might want to browse through the documentation to see what is available. As I write this, the documentation for the complete API can be found at http://java.sun.com/j2se/1.5.0/docs/api/index.html Even an expert programmer won\u2019t be familiar with the entire API, or even a majority of it. In this book, you\u2019ll only encounter several dozen classes, and those will be sufficient for writing a wide variety of programs.",
    "page48": "Using Classes from Packages Let\u2019s say that you want to use the class java.awt.Color in a program that you are writing. Like any class, java.awt.Color is a type, which means that you can use it to declare variables and parameters and to specify the return type of a function. One way to do this is to use the full name of the class as the name of the type. For example, suppose that you want to declare a variable named rectColor of type java.awt.Color. You could say: java.awt.Color rectColor; This is just an ordinary variable declaration of the form \u201chtype-namei hvariable-namei;\u201d. Of course, using the full name of every class can get tiresome, so Java makes it possible to avoid using the full name of a class by importing the class. If you put import java.awt.Color; at the beginning of a Java source code file, then, in the rest of the file, you can abbreviate the full name java.awt.Color to just the simple name of the class, Color. Note that the import lin\u00aee comes at the start of a file and is not inside any class. Although it is sometimes referred to as a statement, it is more properly called an import directive since it is not a statement in the usual sense. Using this import directive would allow you to say Color rectColor; to declare the variable. Note that the only effect of the import directive is to allow you to use simple class names instead of full \u201cpackage.class\u201d names; you aren\u2019t really importing anything substantial. If you leave out the import directive, you can still access the class you just have to use its full name. There is a shortcut for importing all the classes from a given package. You can import all the classes from java.awt by saying import java.awt.*; The \u201c*\u201d is a wildcard that matches every class in the package. (However, it does not match sub-packages; you cannot import the entire contents of all the sub-packages of the java package by saying import java.*.) Some programmers think that using a wildcard in an import statement is bad style, since it can make a large number of class names available that you are not going to use and might not eve\u00aen know about. They think it is better to explicitly import each individual class that you want to use. In my own programming, I often use wildcards to import all the classes from the most relevant packages, and use individual imports when I am using just one or two classes from a given package. In fact, any Java program that uses a graphical user interface is likely to use many classes from the java.awt and java.swing packages as well as from another package named java.awt.event, and I usually begin such programs with import java.awt.*; import java.awt.event.*; import javax.swing.*; A program that works with networking might include the line \u201cimport java.net.*;\u201d, while one that reads or writes files m\u00aeight use \u201cimport java.io.*;\u201d. (But when you start importing lots of packages in this way, you have to be careful about one thing: It\u2019s possible for two classes that are in different packages to have the same name. For example, both the java.awt package and the java.util package contain classes named List. If you import both java.awt.* and java.util.*, the simple name List will be ambiguous. If you try to declare a variable of type List, you\u00ae will get a compiler error message about an ambiguous class name. The solution is simple: Use the full nam\u00aee of the class, either java.a\u00aewt.List or java.util.List. Another solution, of course, is to use import to import the individual classes you need, instead of importing entire packages.) Because the package java.lang is so fundamental, all the classes in java.lang are automatically imported into every program. It\u2019s as if every program began with the statement \u201cimport java.lang.*;\u201d. This is why we have been able to use the class name String instead of java.lang.String, and Math.sqrt() instead of java.lang.Math.sqrt(). It would still, however, be perfectly legal to use the longer forms of the names. Programmers can create new packages. Suppose that you want some classes that you are writing to be in a package named utilities. Then the source code file that defines those classes must begin with the line package utilities; This would come even before any import directive in that file. Furthermore, as mentioned in Subsection 2.6.4, the source code file would be placed in a folder with the same name as the package. A class that is in a package automatically has access to o\u00aether classes in the same package; that is, a class doesn\u2019t have to import the package in which it is defined. In projects that define large numbers of classes, it makes sense to organize those classes into packages. It also makes sense for programmers to create new packages as toolboxes that provide functionality and API\u2019s for dealing with areas not covered in the standard Java API. (And in fact such \u201ctoolmaking\u201d programmers often have more prestige than the applications programmers who use their tools.) However, I will not be creating any packages in this textbook. For the purposes of\u00ae this book, you need to know about packages mainly so that you will be able to import the standard packages. These packages are always available to the programs that you write. You might wonder where the standard classes are actually located. Again, that can depend to some extent on the version of Java that you are using, but in the standard Java 5.0, they are stored in jar files in a subdirectory of \u00aethe main Java installation directory. A jar (or \u201cJava archive\u201d) file is a single file that can contain many classes. Most of the standard classes can be found in a jar file named classes.jar. In fact, Java programs are generally distributed in the form of jar files, instead of as individual class files. Although we won\u2019t be creating packages explicitly, every class is actually part of a package. If a class is not specifically placed in a package, then it is put in something called the default package, which has no name. All the examples that you see in this book are in the default package.",
    "page49": "Javadoc- To use an API effectively, you need good documentation for it. The documentation for most Java APIs is prepared using a system called Javadoc. For example, this syst\u00aeem is used to prepare the documentation for Java\u2019s standard packages. And almost everyone who creates a toolbox in Java publishes Javadoc documentation for it. Javadoc documentation is prepared from special comments that are placed in the Java source code file. Recall that one type of Java comment begins with and ends with. A Javadoc comment takes the same form, but it begins with /** rather than simply /*. You have already seen comments of this form in some of the examples in this book, such as this subroutine This subroutine prints a 3N+1 sequence to standard output, using startingValue as the initial value of N. It also prints the\u00ae number of terms in the sequence. The value of the parameter, startingValue, must be a positive integer. Note that the Javadoc comment is placed just before the subroutine that it is commenting on. This rule is always followed. You can have Javadoc comments for subroutines, for member variables, and for classes. The Javadoc comment always immediately precedes the thing it is commenting on. Like any comment, a Javadoc comment is ignored by the computer when the file is compiled. But there is a tool called javadoc that reads Java so\u00aeurce code files, extracts any Javadoc comments that it finds, and creates a set of Web pages containing the comments in a nicely formatted, interlinked form. By default, javadoc will only collect information about public classes, subroutines, and member variables, but it allows the option of creating documentation for non-public things as well. If javadoc doesn\u2019t find any Javadoc comment for something, it will construct one, but the comment will contain only basic information such as the name and type of a member variable or the name, return type, and parameter list of a subroutine. This is syntactic information. To add information about semantics and pragmatics, you have to write a Javadoc comment. As an example, you ca\u00aen look at the documentation Web page for TextIO. The documentation page was created by applying the javadoc tool to the source code file, TextIO.java. If you have downloaded the on-line version of this book, the documentation can be found in\u00ae the TextIO Javadoc directory, or you can find a link to it in the on-line version of this section. In a Javadoc comment, the *\u2019s at the start of each line are optional. The javadoc tool will remove them. In addition to normal text, the comment can contain certain special codes. For one thing, the comment can contain HTML mark-up commands. HTML is the language that is used to create web \u00aepages, and Javadoc comments are meant to be shown on web pages. The javadoc tool will copy any HTML commands in the comments to the web pages that it creates. You\u2019ll learn some basic HTML in Section 6.2, but as an example, you can add to indicate the start of a new paragraph. (Generally, in the absence of HTML commands, blank lines and extra spaces in the comment are ignored. descriptions can extend over several lines. The description ends at the next tag or at the end of the comment. You can include a @param tag for every parameter of the subroutine and a @throws for as many types of exception as you want to docume\u00aent. You should have a @return tag only for a non-void subroutine. These tags do not have to be given in any particular order. Here is an example that doesn\u2019t do anything exciting but that does use all three types of doc tag This subroutine computes the area of a rectangle, given its width and its height. The length and the width should be positive numbers. param width the length of one side of the rectangle param height the length the second side of the rectangle return the area of the rectangle throws IllegalArgume\u00aentException if eith\u00aeer the width or the height is a negative number. I will use Javadoc comments for some of my examples. I encourage you to use them in your own code, even if you don\u2019t plan to generate Web page documentation of your work, since it\u2019s a standard format that other Java programmers will be familiar with. If you do want to create Web-page documentation, you need to run the javadoc tool. This tool is available as a command in the Java Development Kit\u00ae that was discussed You can use javadoc in a command line interface similarly to the way that the javac and java commands are used. Javadoc can also be applied in the Eclipse integrated development environment that was also discussed Just right-click the class or package that you want to document in the Package Explorer, select \u201cExport,\u201d and select \u201cJavadoc\u201d in the window that pops up. I won\u2019t go into any of the details here; see the documentation.",
    "page50": "More on Program Design- Understanding how programs work is one thing. Designing a program to perform some particular task is another th\u00aeing altogether. In Section 3.2, I discussed how pseudocode and stepwise refinement can be used to methodically develop an algorithm. We can now see how subroutines can fit into the process. Stepwise refinement is inherently a top-down process, but the process does have a \u201cbottom,\u201d that is, a point at which you stop refining the pseudocode algorithm and translate what you have directly into proper programming language. In the absence of subroutines, the process would not bottom out until you get down to the lev\u00aeel of assignment statements and very primitive input/output operations. But if you have subroutines lying around t\u00aeo perform certain useful tasks, you can stop refining as soon as you\u2019ve managed to express your algorithm in terms of those tasks. This allows you to add a bottom-up element to the top-down approach of stepwise refinement. Given a problem, you might start by writing some subroutines that perform tasks relevant to \u00aethe problem domain. The subroutines become a toolbox of ready-made tools that you can integrate into your algorithm as you develop it. (Alternatively, you might be able to buy or find a software toolbox written by someone else, containing subroutines that you can use in your project as black boxes.) Subroutines can also be helpful even in a strict top-down approach. As you refine your algorithm, you\u00ae are free at any point to take any sub-task in the algorithm and make it into a subroutine. Developing that subroutine then becomes a separate problem, which you can work on separately. Your main algorithm will merely call the subroutine. This, of course, is just a way of breaking your problem down into separate, smaller problems. It is still a top-down app\u00aeroach because the top-down analysis of the problem tells you what subroutines to write. In the bottom-up approach, you start by writing or obtaining subroutines that are relevant to the problem domain, and you build your solution to the problem on top of that foundation of subroutines.  Preconditions and Postconditions When working with subroutines as building blocks, it is important to be clear about how a subroutine interacts with the rest of the program. This interaction is specified by the contract of the subroutine, as discussed in Section 4.1. A convenient way to express the contract of a subroutine is in terms of preconditions and postconditions. The precondition of a subroutine is something that must be true when the subrout\u00aeine is called, if the subroutine is to work correctly. For example, for the built-in function Math.sqrt(x), a precondition is that the parameter, x, is greater than or\u00ae equal to zero, since it is not possible to take the square root of a negative number. In terms of a contract, a precondition represents an obligation of the caller of the subroutine. If you call a subroutine without meeting its precondition, then there is no reason to expect it to work properly. The program might crash or give incorrect results, but you can only blame yourself, not the subroutine. A postcondition of a subroutine represents the other side of the contract. It is something that will be true after the subroutine has run (assuming that its preconditions were met and that there are no bugs in the subroutine). The postcondition of the function Math.sqrt() is that the square of the value that is returned by this function is equal to the parameter that is provided when the subroutine is called. Of course, this will only be true if the preconditiion that the parameter is greater than or equal to zero is met. A postcondition of the built-in subroutine System.out.print() is that the value of the parameter has been displayed on  screen. Preconditions most often give restrictions on the acceptable values of parameters, as in the example of Math.sqrt(x). However, they can also refer to global variables that are used in the subroutine. The postcondition of a subroutine specifies the task that it performs. For a function, the postcondition should specify the value that the function returns. Subroutines are often described by comments that explicitly specify their preconditions and postconditions. When you are given a pre-written subroutine, a statement of its preconditions and postconditions tells you how to use it and what it does. When you are assigned to write a subroutine, the preconditions and postconditions give you an exact specification of what the subroutine is expected to do. I will use this approach in the example that constitutes the rest of this section. The comments are given in the form of Javadoc comments, but I will explicitly label the preconditions and postconditions. (Many computer scientists think that new doc tags @precondition and postcondition should be added to the Javadoc system for explicit labeling of preconditions and postconditions, but that has not yet been done.)",
    "page51": "The Truth About Declarations- Names are fundament\u00aeal to programming, as I said a few chapters ago. There are a lot of details involved in declaring and using names. I have been avoiding some of those details. In this section, I\u2019ll reveal most of the truth (although still not the full truth) about declaring and using variables in Java. The material in the subsections \u201cInitialization in Declarations\u201d and \u201cNamed Constants\u201d is particularly important, since I will be using it regularly in future chapters. Named Constants- Sometimes, the value of a variable is not supposed to change after it is initialized. For example, in the above example where interestRate is initialized to the value 0.05, it\u2019s quite possible that that is meant to be the value throughout the entire program. In this case, the programmer is probably defining the variable, interestRate, to give a meaningful name to the otherwise meaningless number, 0.05. It\u2019s easier to understand what\u2019s going on when a program says \u201cprincipal += principal*interestRate;\u201d rather than \u201cprincipal += principal*0.05;\u201d. In Java, the modifier \u201cfinal\u201d can be applied to a variable declaration to ensure that t\u00aehe value stored in the variable cannot be changed after the variable has been initialized. For example, if the member variable interestRate is declared with final static double interestRate = 0.05; then it would be impossible for the value of interestRate to change anywhere else in the program. Any assignment statement that tries to assign a value to interestRate will be rejected by the computer as a syntax error when the program is compiled. It is legal to apply the final modifier to local variables and even to formal parameters, but it is most useful for member variables. I will often refer to a static member variable that is declared to be final as a named constant, since its value remains constant for the whole time that the program is running. The readability of a program can be greatly enhanced by using named constants to give meaningful names to important quantities in the program. A recommended style rule for named constants is to give them names that consist entirely of upper case letters, wi\u00aeth underscore characters to separate words if necessary. For example, the preferred style for the interest rate constant would be final static double INTEREST RATE = 0.05; This is the style that is generally used in Java\u2019s standard classes, which define many named constants. For e\u00aexample, we have already seen that the Math class contains a variable Math.PI. This variable is declared in the Math class as a \u201cpublic final static\u201d variable of type double. Similarly, the Color class contains named constants such as Color.RED and Color.YELLOW which are public final static variables of type Color. Many named constants are created just \u00aeto give meaningful names to be used as parameters in subroutine calls. For example, the standard class named Font contains named constants Font.PLAIN, Font.BOLD, and Font.ITALIC. These constants are used for specifying different styles of text when calling various subroutines in the Font class. Enumerated type constants are also examples of named constants. The enumerated type definition enum Alignment { LEFT, RIGHT, CENTER } defines the constants Alignment.LEFT, Alignment.RIGHT, and Alignment.CENTER. Technically, Alignment is a class, and the three constants are public final static members of that class. Naming and Scope Rules- When a variable declaration is executed, memory is allocated for that variable. The variable name can be used in at least some part of the program source code to refer to that memor\u00aey or to the data that is stored in the memory. The portion of the program source code where the variable name is valid is called the scope of the variable. Similarly, we can refer to the scope of subroutine names and formal parameter names. For static member subroutines, scope is straightforward. The scope of a static subroutine is the entire source code of the class in which it is defined. That is, it is possible to call the subroutine from any point in the class, including at a point in the source code before the point where the definition of the subroutine appears. It is even possible to call a subroutine from within itself. This is an example of something called \u201crecursion,\u201d a fairly advanced topic that we will return to later.",
    "page52": "Whereas a subroutine represents a single task, an object can encapsulate both data (in the form of instance variables) and a number of different tasks or \u201cbehaviors\u201d related to that data (in the form of instance methods). Therefore objects provide another, more sophisticated type of structure that can be used to help manage the complexity of large programs. This chapter covers the creation and use of objects in Java. Section 5.5 covers the central ideas of object-oriented programming: inheritance and polymorphism. However, in this textbook, we will generally use these ideas in a limited form, by creating independent classes and building on existing classes rather than by designing entire hierarchies of classes from scratch. Object-oriented programming (OOP) represents an attempt to make programs more closely model the way people think about and deal with the world. In the older styles of programming, a programmer who is faced with some problem must identify a computing task that needs to be performed in order to solve the problem. \u00aeProgramming then consists of finding a sequence of instructions that will accomplish that task. But at the heart of objectoriented programming, instead of tasks we find objects entities that have behaviors, that hold information, and that can interact with one another. Programming consists of designing a set of objects that somehow model the problem at hand. Software objects in the program can represent real or abstract entities in the problem domain. This is supposed to make the design of the program more natural and hence easier to get right and easier to understand. To some extent, OOP is just a change in point of view. We can think of an object in standard programming terms as nothing more than a set of variables together with some subroutines for manipulating those variables. In fact, it is possible to use object-oriented techniques in any programming language. However, there is a big difference between a language that makes OOP possible and one that actively supports it. An object-oriented programming language such as Java includes a number of fe\u00aeatures that make it very different from a standard language. In order to make effective use of those features, you have to \u201corient\u201d your thinking correctly. Objects, Classes, and Instances- Objects are closely related to classes. We have already been working with classes for several chapters, and we have seen that a class can contain variables and subroutines. If an object is also a collection of variables and subroutines, how do they differ from classes? And why does it require a different type of thinking to understand and use them effectively? In the one section where we worked with objects rather than classes, Section 3.8, it didn\u2019t seem to make much difference: We just left the word \u201cstatic\u201d out of the subroutine definitions! I have said that classes \u201cdescribe\u201d objects, or more exactly that the non-static portions of classes describe objects. But it\u2019s probably not very clear what this means. The more usual terminology is to say that objects belong to classes, but this might not be much clearer. (There is a real shortage of English words to properly distinguish all the concepts involved. An object certainly doesn\u2019t \u201cbelong\u201d to a class in the same way that a member variable \u201cbelongs\u201d to a class.) From the point of view of programming, it is more exact to say that classes are used to create objects. \u00aeA class is a kind of factory for co\u00aenstructing objects. The non-static parts of the class specify, or describe, what variables an\u00aed subroutines the objects will contain. This is part of the explanation of how objects differ from classes: Objects are created and destroyed as the program runs, and there can be many objects with the same structure, if they are created using the same class.",
    "page53": "Consider a simple class whose job is to group together a few static member variables. For example, the following class could be used to store information about the person who is using the program: class UserData { static String name; static int age; } In a program that uses this class, there is only one copy of each of the variables UserData.name and UserData.age. There can only be one \u201cuser,\u201d since we only have memory space to store data about one user. The class, UserData, and the\u00ae variables it contains exist as long as the program runs. Now, consider a similar class that includes non-static variables: class PlayerData { String name; int age; } In this case, there is no such variable as PlayerData.name or PlayerData.age, since name and age are not static members of PlayerData. So, there is nothing much in the class at all except the potential to create objects. But, it\u2019s a lot of potential, since it can be used to create any number of objects! Each object will have its own variables called name and age. There can be many \u201cplayers\u201d because we can make new objects to represent new players on demand. A program might use this class to store information about multi\u00aeple players in a game. Each player has a name and an age. When a player joins the game, a new PlayerData object can be created to represent that player. If a player leaves the game, the PlayerData object that represents that player can be destroyed. A system of objects in the program is being used to dynamically model what is happening in the game. You can\u2019t do this with \u201cst\u00aeatic\u201d variables! In Section 3.8, we worked with applets, which are objects. The reason they didn\u2019t seem to be any different from classes is because we were only working with one applet in each class that we looked at. But one class can be used to make many applets. Think of an appl\u00aeet that scrolls a message across a Web page. There could be several such applets on the same page, all created from the same class. If the scrolling\u00ae message in the applet is stored in a non-static variable, then each applet will have its own variable, and each applet can show a different message. The situation is even clearer if you think about windows, which, like applets, are objects. As a program runs, many windows might be opened and closed, but all those windows can belong to the same\u00ae class. Here again, we have a dynamic situation where multiple objects are created and destroyed as a program runs. An object that belongs to a class is said to be an instance of that class. The variables that the object contains are called \u00aeinstance variables. The subroutines that the object contains are called instance methods. (Recall that in the context of object-oriented programming, method is a synonym for \u201csubroutine\u201d. From now on, since we are doing object-oriented programming, I will prefer the term \u201cmethod.\u201d) For example, if the PlayerData class, as defined above, is used to create an object, then that object is an instance of the PlayerData class, and name and age are instance variables in the object. It is important to remember that the class of an object determines the types of the instance variables; however, the actual data is contained inside the individual objects, not the \u00aeclass. Thus, each object has its own set of data.",
    "page54": "An applet that scrolls a message across a Web page might include a subroutine named scroll(). Since the applet is an object, this subroutine is an instance method of the applet. The source code for the method is in the class that is used to create the applet. Still, it\u2019s better to think of the instance method as belonging to the object, not to the class. The non-static subroutines in the class merely specify the instance me\u00aethods that every object created from the class will contain. The scroll() methods in two different applets do the same thing in the sense that they both scroll messages across the screen. But there is a real difference betw\u00aeeen the two scroll() methods. The messages that they scroll can be different. You might say that the method definition in the class specifies what type of behavior\u00ae the objects will ha\u00aeve, but the specific behavior can vary from object to object, depending on the values of their instance variables. As you can see, the static and the non-static portions of a class are very different things and serve very different purposes. Many classes contain only static members, or only non-static. However, it is possible to mix static and non-static members in a single cla\u00aess, and we\u2019ll see a few examples later in this chapter where it is reasonable to do so. You should distiguish between the source code for the class, and the class itself. The source code determines both the class and the objects that are created from that class. The \u201cstatic\u201d definitions in the source code specify the things that are part of the class itself, wh\u00aeereas the non-static definitions in the source code specify things that will become part of every instance object that is created from the class. By the way, static member variables and static member subroutines in a class are sometimes called class variables and class methods, since they belong to the class itself, rather than to instances of that class. Fundamentals of Objects So far, I\u2019ve been talking mostly in generalities, and I haven\u2019t given you much idea what you have to put in a program if you want to work with objects. Let\u2019s look at a specific example to see how it works. Consider this extremely simplified version of a Student class, which could be\u00ae used to store information about students taking a course. public class Student { public String name; // Student\u2019s name. public double t\u00aeest1, test2, test3; // Grades on three tests. public double getAverage() { // compute average test grade return (test1 + test2 + test3) / 3; } } // end of class Student None of the members of this class are declared to be static, so the class exists only for creating objects. This class definition says that any object that is an instance of the Student class will include instance variables named name, test1, test2, and test3, and it will include an instance method named getAverage(). The names and tests in different objects will generally have different values. When called for a particular student, the method getAverage() will compute an av\u00aeerage using that student\u2019s test grades. Different students can have\u00ae different averages. (Again, this is what it means to say that an instance method belongs to an individual object, not to the class.) In Java, a class is a type, similar to the built-in types such as int and boolean. So, a class name can be used to specify the type of a variable in a declaration statement, the type of a formal parameter, or the return type of a function. For example, a program could define a variable named std of type Student with the statement Student std; However, declaring a variable does not create an object! This is an important point, which is related to this Very Important Fact In Java, no variable can ever hold an object. A variable can only hold a reference to an object.",
    "page55": "You should think of objects as floating around indep\u00aeendently in the computer\u2019s memory. In fact, there is a special portion of memory called the heap where objects live. Instead of holding an object itself, a variable holds the information necessary to find the object in memory. This information is called a reference or pointer to the object. In effect, a reference to an object is the address of the memory l\u00aeocation where the object is stored. When you use a variable of class type, the computer uses the reference in the variable to find the actual object. In a program, objects are created using an operator called new, which creates an object and returns a reference to that object. For example, assuming that std is a variable of type Student, declared as above, the assignment statement std = new Student(); would create a new object which is an instance of the class Student, and it would store a reference to that object in the variable std. The value of the variable is a reference to the object, not the object itself. It is not quite true, then, to say that the object is the \u201cvalue of\u00ae the variable std\u201d (though some\u00aetimes it is hard to avoid using this terminology). It is certainly not at all true to say that the object is \u201cstored in the variable std.\u201d The proper terminology is that \u201cthe variable std refers to the object,\u201d and I will try to stick to that terminology as much as possible. So, suppose that the variable std refers to an object belonging to the class Student. That object has instance variables name, test1, test2, and test3. These instance variables can be referred to as std.name, std.test1, std.test2, and std.test3. This follows the usual naming convention that when B is part of A, then the full name of B is A.B. For example, a program might include the lines System.out.println(\\\"Hello, \\\" + std.name + \\\". Your test grades are:\\\"); System.out.println(std.test1); System.out.println(std.test2); System.out.println(std.test3); This would output the name and test grades from the object to which std refers. Similarly, std can be used to call the getAverage() instance method in the object by saying std.getAverage(). To print out the student\u2019s average, you could say: System.out.println(\\\"Your average is \\\" + std.getAverage() ); More generally, you could use std.name any place where a variable of type String is legal. You can use it in expressions. You can assign a value to it. You can even use it to call subroutines from the String class. For example, std.name.length() is the number of characters in the student\u2019s name. It is possible for a variable like std, whose type is given by a class, to refer to no object at all. We say in this case that std holds a null reference. The null reference is written in Java as \u201cnull\u201d. You can store a null reference in the variable std by saying std = null; and you could test whether the value of std is null by testing if (std == null)",
    "page56": "If the value of a variable is null, then it is, of course, illegal to refer to instance variables or instance methods through that variable since there is no object, and hence no instance variables to refer to. For example, if the value of the variable std is null, then it would be illegal to refer to std.test1. If your program attempts to use a null reference illegally like this, the result is an error called a null pointer exception. When one object vari\u00aeable is assigned to another, only a reference is copied. The object referred to is not copied. When the assignment \u201cstd2 = std1;\u201d was executed, no new object was created. Instead, std2 was set to refer to the very same object that std1 refers to. This has some consequences that might be surprising. For example, std1.name and std2.name are two different names for the same variable, namely the instance variable in the object that both std1 and std2 refer to. After the string \\\"Mary Jones\\\" is assigned to the variable std1.name, it is also true that the value of std2.name is \\\u00ae\"Mary Jones\\\". There is a potential for a lot of confusion here, but you can help protect yourself from it if you keep telling yourself, \u201cThe object is not in the variable. The variable just holds a pointer to the object.\u201d You can test objects for equality and inequality using the operators == and !=, but here again, the semantics are different from what you are used to. When you make a test \u201cif (std1 == std2)\u201d, you are testing whether the values stored in std1 and std2 are the same. But the values are references to objects, not objects. So, you are testing whether std1 and std2 refer to the same object, that is, whether they point to the same location in memory. This is fine, if its what you want to do. But sometimes, what you want to check is whether the instance variables in the objects have the same values. To do that, you would need to ask whether \u201cstd1.test1 == std2.test1 && std1.test2 == std2.test2 && std1.test3\u00ae == std2.test3 && std1.name.equals(std2.name)\u201d. I\u2019ve remarked previously that Strings are objects, and I\u2019ve shown the strings \\\"Mary Jones\\\" and \\\"John Smith\\\" as o\u00aebjects in the above illustration. A variable of type String can only hold a reference to a string, not the string itself. It could also hold the value null, meaning that\u00ae it does not refer to any string at all. This explains why using the == operator to test strings for equality is not a good idea. Suppose that greeting is a variable of type String, and that the string it refers to is \\\"Hello\\\". Then would the test greeting == \\\"Hello\\\" be true? Well, maybe, maybe not. The variable greeting and the String literal \\\"Hello\\\" each refer to a string that contains the characters H-e-l-l-o. But the strings could still be different objects, that just happen to contain the same characters. The function greeting.equals(\\\"Hello\\\") tests whether \u00aegreeting and \\\"Hello\\\" contain the same characters, which is almost certainly the question you want to ask. The expression greeting == \\\"Hello\\\" tests whether greeting and \\\"Hello\\\" contain the same characters stored in the same memory location.",
    "page57": "When writing new classes, it\u2019s a good idea to pay attention to the issue of access control. Recall that making a member of a class public makes it accessible from anywhere, including from other classes. On the other hand, a private member can only be used in the class where it is defined. In the opinion of many programmers, almost all member variables should be declared private. This gives you complete control over what can be done with the variable. Even if the variable itself is private, you can allow other classes to find out what its value is by providing a public accessor method that returns the value of the variable. For example, if your class contains a private member variable, title, of type String, you can provide a method public String getTitle() { re\u00aeturn title; } that returns the value of title. By convention, the name of an accessor method for a variable is obtained by capitalizing the name of variable and adding \u201cget\u201d in fron\u00aet of the name. So, for the variable title, we get an accessor method named \u201cget\u201d + \u201cTitle\u201d, or getTitle(). Because of this naming convention, accessor methods are more often referred to as getter methods. A getter method provides \u201cread access\u201d to a variable. You might also want to allow \u201cwrite access\u201d to a private variable. That is, you might want to make it possible fo\u00aer other classes to specify a new value for the variable. This is done with a setter method. (If you don\u2019t like simple, Anglo-Saxon words, you can use the fancier term mutator method.) The name of a setter method should consist of \u201cset\u201d followed by a capitalized copy of the variable\u2019s name, and it should have a parameter with the same type as the variable. A setter method for the variable title could be written public void setTitle( String newTitle ) { title = newTitle; } It is actually very common to provide both a getter and a setter method for a private m\u00aeember variable. Since this allows other classes both to see and \u00aeto change the value of the variable, you might wonder why not just make the variable public? The reason is that getters and setters are not restricted to simply reading and writing the variable\u2019s value. In fact, they can take any action at all. For example, a getter method might keep track of the number of times that the variable has been \u00aeaccessed public String getTi\u00aetle() { titleAccessCount++; // Increment member variable titleAccessCount. return title; } and a setter method might check that the value that is being assigned to the variable is legal",
    "page58": "Even if you \u00aecan\u2019t think of any extra chores to do in a getter or setter method, you might change your mind in the future when you redesign and improve your class. If you\u2019ve used a getter and setter from the beginning, you can make the modification to your class without affecting any of the classes that use your class. The private member variable is not part of the public interface of you\u00aer \u00aeclass; only the public getter and setter methods are. If you haven\u2019t used get and set from the beginning, you\u2019ll have to contact everyone who uses your class and tell them, \u201cSorry guys, you\u2019ll have to track down every use that you\u2019ve made of this variable and change your code to use my new get and set methods instead.\u201d A couple of final notes: Some advanced aspects of Java rely on the naming convention for getter and setter methods, so it\u2019s a good idea to follow the convention rigorously. And though I\u2019ve been talking about using getter and setter methods for a variable, you can define get and set methods even if there is no variable. A getter and/or setter method defines a property of the class, that might or might not correspond to a variable. For example, if a class includes a public void instance method with signature setValue(double), then the class has a \u201cproperty\u201d named value of type double, and it has this property whether or not the class has a member variable named value. Constructors and Object Initialization- Object types in Java are very different from the primitive types. Simply declaring a variable whose type is given as a class does not automatically create an object of that class. Objects must be explicitly constructed. For the computer, the process of constructing an object means, first, finding some unused memory in the heap that can be used to hold the object and, second, filling in the object\u2019s instance variables. As a programmer, you don\u2019t care where in memory the object is stored, but you will usually want to exercise some control over what initial values are stored in a new object\u2019s instance variables. In many cases, you will also want to do more complicated initialization or bookkeeping every time an object is created.  Programming with Objects There are several ways in which object-oriented concepts can be applied to the process of\u00ae designing and writing programs. The broadest of these is object-oriented analysis and design which applies an object-oriented methodology to the earliest stages of program development, during which the ov\u00aeerall design of a progra\u00aem is created. Here, the idea is to identify things in the problem domain that can be modeled as objects. On another level, object-oriented pro\u00aegramming encourages programmers to produce generalized software components that can be used in a wide variety of programming projects.",
    "page59": "Some Built-in Classes- Although the focus of object-oriented programming is generally on the design and implementation of new classes, it\u2019s important not to forget that the designers of Java have already provided a large number of reusable classes. Some of these classes are meant to be extended to produce new classes, while others can be used directly to create useful objects. A true mastery of Java requires familiarity with a large number of built-in classes something that takes a lot of time and experience to develop. In the next chapter, we will begin the study of Java\u2019s GUI classes, and you will encounter other built-in classes throughout the remainder of this book. But let\u2019s take a moment to look at a few built-in classes that you might find useful. A string can be built up from smaller pieces using the + operator, but this is not very efficient. If str is a String and ch is a character, then executing the command \u201cstr = str + ch;\u201d involves creating a whole new string that is a copy of str, with the value of ch appended onto the end. Copying the string tak\u00aees some time. Building up a long string letter by letter would require a surprising amount of processing. The class StringBuffer makes it possible to be efficient about building up a long string from a number of smaller pieces. To do this, you must make an object belonging to the StringBuffer class. For example: StringBuffer buffer = new StringBuffer(); (This statement both declares the variable buffer and initializes it to refer to a newly created StringBuffer object. Combining declaration with initialization was covered in Subsection 4.7.1 and works for objects just as it does for primitive types.) Like a String, a StringBuffer contains a sequence of characters. However, it is possible to add new characters onto the end of a StringBuffer without making a copy of the data t\u00aehat it already contains. If x is a value of any type and buffer is the variable defined above, then the command buffer.append(x) will add x, converted into a string representation, onto the end of the data that was already in the buffer. This command actually modifies the buffer, rather than making a copy\u00ae, and that can be done efficiently. A long string can be built up in a StringBuffer using a sequence of append() commands. When the string is complete, the function buffer.toString() will return a c\u00aeopy of the string in the buffer as an ordinary value of type String. The StringBuffer class is in the standard package java.lang, so you can use its simple name without importing it.",
    "page60": "A number of useful classes are collected in the package java.util. For example, this package contains classes for working with collections of objects. We will study these collection classes in\u00ae Chapter 10. Another class in this package, java.util.Date, is used to represent times. When a Date object is constructed without parameters, the result represents the current date and time, so an easy way to display this information is: System.out.println( new Date() ); Of course, to use the Date class in this way, you must make it available by importing it with one of the statements \u201cimport java.util.Date;\u201d or \u201cimport java.util.*;\u201d at the beginning of your program. (See Subsection 4.5.3 for a discussion of packages and import.) I will also mention the class java.util.Random. An object belonging to this class is a source of random numbers (or, more precisely pseudorandom numbers). The stand\u00aeard function Math.random() uses one of these objects behind the scenes to generate its random numbers. An object of type Random can generate random integers, as well as random real numbers. If randGen is created with the command: Random randGen = new Random(); and i\u00aef N is a positive integer, then randGen.nextInt(N) generates a random integer in the range from 0 to N-1. For example, this makes it a little easier to roll a pair of dice. Instead of saying \u201cdie1 = (int)(6*Math.random())+1;\u201d, one can say \u201cdie1 = randGen.nextInt(6)+1;\u201d. (Since you also have to import the class java.util.Random and create the Random object\u00ae, you might not agree that \u00aeit is actually easier.) An object of type Random can also be used to generate so-called Gaussian distributed random real numbers. The main point here, again, is that many problems have already been solved, and the solutions are available \u00aein Java\u2019s standard classes. If you are faced with a task that looks like it shou\u00aeld be fairly common, it might be worth looking through a Java reference to see whether someone has already written a class that you can use. Wrapper Classes and Autoboxing- We have already encountered the classes Double and Integer in Subsection 2.5.7. These classes contain the static methods Double.parseDouble and Integer.p\u00aearseInteger that are used to convert strings to numerical values. We have also encountered the Character c\u00aelass in some examples, and static methods such as Character.isLetter, which can be used to test whether a given value of type char is a letter. There is a similar class for each of the other primitive types, Long, Short, Byte, Float, and Boolean. These classes are called wrapper classes. Although they contain useful static members, they have another use as well: They are used for creating objects that represent primitive type values.",
    "page61": "The class \u201cObject\u201d- We have already seen that one of the major features of object-oriented programming is the ability to create subclasses of a class. The subclass inherits all the properties or behaviors of the class, but can modify and add to what it inherits. In Section 5.5, you\u2019ll learn how to create subclasses. What you don\u2019t know yet is that every class in Java (with just one exception) is a subclass of some other class. If you create a class and don\u2019t explicitly make it a subclass of some other class, then it automatically becomes a subclass of the special class named Object. (Object is the one class that is not a subclass of any other class.) Class Object defines several instance methods that are inherited by every other class. These methods can be used with any object whatsoever. I will mention just one of them here. You will encounter more of them later in the book. The instance method toString() in class Object returns a value of type String that is supposed to be a string representation of the object. You\u2019ve already used this method implicitly, any time you\u2019ve printed out an object or concatenate\u00aed an object onto a string. When you use an object in a context that requires a string, the object is automatically converted to type String by calling its toString() method. The version of toString that is defined in Object just returns the name of the class that the object belongs to, concatenated with a code number called the hash code of the object; this is no\u00aet very useful. When you create a class, you can write a new toString() method for it, which will replace the inherited version. For example, we might add the following method to any of the PairOfDice classes from the previous section.public String toString() { // Return a String representation of a pair of dice, where die1 // and die2 are instance variables containing the numbers that are // showing on the two dice. if (die1 == die2) return \\\"double \\\" + die1; else return die1 + \\\" and \\\" + die2; } If dice refers to a PairOfDice object, then dice.toString() will return strings such as \u201c3 and 6\u201d, \u201c5 and 1\u201d, and \u201cdouble 2\u201d, depend\u00aeing on the numbers showing on the dice. This method would be used automatically to convert dice to type String in a statement such as System.out.println( \\\"The dice came up \\\" + dice ); so this statement might output, \u201cThe dice came up 5 and 1\u201d or \u201cThe dice came up double 2\u201d. You\u2019ll see another example of a toString() method in the next section.",
    "page62": "Object-oriented Analysis and Design- Every programmer builds up a stock of techniques and expertise expressed as snippets of code that can be reused in new programs using the tried-and-true method of cut-and-paste: The old code is physically copied into the new program and then edited to customize it as necessary. The problem is that the editing is error-prone and time-consuming, and the whole enterprise is dependent on the programmer\u2019s ability to pull out that particular piece of code from last year\u2019s project that looks like it might be made to fit. (On the level of a corporation that wants to save money by not reinventing the wheel for each new project, just keeping track of all th\u00aee old wheels becomes a major task.) Well-designed classes are software components that can be reused without editing. A welldesigned class is not carefully crafted to do a particular job in a particular program. Instead, it is crafted t\u00aeo model some particular type of object or a single coherent concept. Since objects and concepts can recur in many problems, a well-designed class is likely to be reusable without modification in a variety of projects. Furthermore, in an object-oriented programming language, it is possible to make subclasses of an existing clas\u00aes. This makes classes even more reusable. If a class needs to be customized, a subclass can be created, and additions or modifications can be made in the subclass without making any changes to the original class. This can be done even if \u00aethe programmer doesn\u2019t have access to the source code of the class and doesn\u2019t know any details of its internal, hidden implem\u00aeentat\u00aeion. The PairOfDice class in the previous section is already an example of a generalized software component, although one that could certainly be improved. The class represents a single, coherent concept, \u201ca pair of dice.\u201d The instance variables hold the data relevant to the state of the dice, that is, the number showing on each of the dice. The instance method represents the behavior of a pair of dice, that is, the ability to be rolled. This class would be reusable in many different pro\u00aegramming projects. On the other hand, the Student class from the previous section is not very reusable. It seems to be crafted to represent students in a particular course where the grade will be based on three tests. If there are more tests or quizzes or papers, it\u2019s useless. If there are two people in the class who have the same name, we are in trouble (one reason why numerical student ID\u2019s are often used). Admittedly, it\u2019s much more difficult to develop a general-purpose student class than a general-purpose pair-of-dice class. But this particular Student class is good mostly as an example in a programming textbook.",
    "page63": "A large programming project goes through a number of stages, starting with specification of the problem\u00ae to be solved, followed by analysis of the problem and design of a program to solve it. Then comes coding, in which the program\u2019s design is expressed in some actual programming language. This is followed by testing and debugging of the program. After that comes a long period of maintenance, which means fixing any new problems that are found in the program and modifying it to adapt it to changing requirements. Together, these stages form what is called the software life cycle. (In the real world, the ideal of consecutive stages is seldom if ever achieved. During the analysis stage, it might turn out that the specifications are incomplete or inconsistent. A problem found during testing \u00aerequires at least a brief return to the coding stage. If the problem is serious enough, it might even require a new design. Maintenance usually involves redoing some of the work from previous \u00aestages) Large, complex programming projects are only likely to succeed if a careful, systematic approach is adopted during all stages of the software life cycle. The systematic approach to programming, using accepted principles of good design, is called software engineering. The software engineer tries to efficiently construct programs that verifiably meet t\u00aeheir specifications and that are easy to modify if necessary. There is a wide range of \u201cmethodologies\u201d that can be applied to help in the systematic design of programs. (Most of these methodologies seem to involve drawing little boxes to represent program components, with labeled arrows to represent relationships among the boxes.) We have been discussing object orientation in progr\u00aeamming languages, wh\u00aeich is relevant to the coding stage of program development. But there are also object-oriented methodologies for analysis and design. The question in this stage of the software life cycle is, How can one discover or invent the overall structure of a program? \u00aeAs an example of a rather simple object-oriented approach to analysis and design, consider this advice: Write down a description of the problem. Underline all the nouns in that description. The nouns s\u00aehould be considered as candidates for becoming classes or objects in the program design. Similarly, underline all the verbs. These are candidates for methods. This is your starting point. Further analysis might uncover the need for more classes and methods, and it might reveal that subclassing can be used to take advantage of similarities among classes. This is perhaps a bit simple-minded, but the idea is clear and the general approach can be effective: Analyze the problem to discover the concepts that are involved, and create classes to represent those concepts. The design should arise from the problem itself, and you\u00ae should end up with a program whose structure reflects the structure of the problem in a natural way.",
    "page64": "Programming Example: Card, Hand, Deck- In this section, we look at some specific examples of object-oriented design in a domain that is simple enough that we have a chance of coming up with something reasonably reusable. Consider card games that are played with a standard deck of playing cards (a so-called \u201cpoker\u201d deck, since it is used in the game of po\u00aeker). Designing the classes In a typical card game, each player gets a hand of cards. The deck is shuffled and cards are dealt one at a time from the deck and added to the players\u2019 hands. In some games, card\u00aes can be removed from a hand, and new cards can be adde\u00aed. The game is won or lost depending on the value (ace, 2, king) and suit (spades, diamonds, clubs, hearts) of the cards that a player receives. If we look for nouns in this description, t\u00aehere are several candidates for objects: game, player, hand, card, deck, value, and suit. Of these, the value and the suit of a card are simple values, and they will just be represente\u00aed as instance variables in a Card object. In a complete program, the other five nouns might be represented by classes. But let\u2019s work on the ones that are most obviously re\u00aeusable: card, hand, and deck. If we look for verbs in the description of a card game, we see that we can shuffle a deck and deal a card from a deck. This gives use us two candidates for instance methods in a Deck class: shuffle() and dealCard(). Cards can be \u00aeadded to and removed from hands. This gives two candidates for instance methods in a Hand class: addCard() and removeCard(). Cards are relatively passive things, but we need to be able to determine their suits and values. We will discover more instance methods as we go along.",
    "page65": "The B\u00aeasic GUI Application- There are two basic types of GUI program in Java: stand-alone applications and applets. An applet is a program that runs in a rectangular area on a Web page. Applets are generally small programs, meant to do fairly simple things, although there is nothing to stop them from being very complex. Applets were responsible for a lot of the initial excitement about Java when it was introduced, since they could d\u00aeo\u00ae things that could not otherwise be done on Web pages. However, there are now easier ways to do many of the more basic things that can be done with applets, and they are no longer the main focus of interest in Java. Nevertheless, there are still some things that can be done best with applets, and they are still fairly common on the Web. We will look at applets in the next section. A stand-alone application is a program that runs on its own, without depending on a Web browser. You\u2019ve been writing stand-alone applications all along. Any class that has a main() routine defines a stand-alone application; running the program just means executing this main() routine. However, the programs that you\u2019ve seen up till now have been \u201ccommandline\u201d programs, where the user and computer interact by typing things back and forth to \u00aeeach other. A GUI program offers a much richer type of user interface, where the user uses a mouse and keyboard to interact with GUI components such as windows, menus, buttons, check boxes, text input boxes, scroll bars, and so on. The main routine of a GUI program creates one or more such components and displays them on the computer screen. Very often, that\u2019s all it does. Once a GUI component has been created, it follows its ow\u00aen programming programming tha\u00aet tells it how to draw itself on the screen and how to respond to events such as being clicked on by \u00aethe user. A GUI program doesn\u2019t have to be immensely complex. We can, for example, write a very s\u00aeimple GUI \u201cHello World\u201d program that says \u201cHello\u201d to the user, but does it by opening a window where the the greeting is displayed: import javax.swing.JOptionPane; public class HelloWorldGUI1 { public static void main(String[] args) { JOptionPane.showMessageDialog( null, \\\"Hello World!\\\" ); } }",
    "page66": "When this program is run, a window appears on the screen that contains the message \u201cHello World!\u201d. The window also contains an \u201cOK\u201d button for the user to click after reading the message. When the user clicks this button, the window closes and the program ends. By the way, this program can be placed in a file named HelloWorldGUI1.java, compiled, and run just like any other Java program. Now, this program is already doing some pretty fancy stuff. It cre\u00aeates a \u00aewindow, it draws the contents of that window, and it handles the event that is generated when the user clicks the button. The reason the program was so easy to write is that all the work is done by showMessageDialog(), a static method in the built-in class JOptionPane. (Note that the source code \u201cimports\u201d the class javax.swing.JOptionPane to make it possible to refer to the JOptionPane cla\u00aess using its simple name. See Subsection 4.5.3 for information about importing classes from Java\u2019s standard packages.) If you want to display a message to the user in a GUI program, this is a good way to do it: Just use a standard class that already knows how to do the work! And in fact, JOptionPane is\u00ae regularly used for just this purpose (but as part of a larger program, usually). Of course, if you want to do anything serious in a GUI program, there i\u00aes a lot more to learn. To give you an idea of the types of things that are involved, we\u2019ll look at \u00aea short GUI program that does the same things as the previous program open a window containing a message and an OK button, and respond to a click on the button by ending the program but does it all by hand instead of by using the built-in JOptionPane class. Mind you, this is not a good way to write the program, but it will illustrate some important aspects of GUI programming in Java. Here is the source code for the program. You are not expected to understand it yet. I will explain how it works below, but it w\u00aeill take the rest of the chapter before you will really understand completely. In this section, you will just get a brief overview of GUI programming.",
    "page67": "JFrame and JPanel- In a Java GUI program, each GUI component in the interface is represented by an object in the program. One of the most fundamental types of component is the window. Windows have many behaviors. They can be opened and closed. They can be resized. They have \u201ctitles\u201d that are displayed in the title bar above the window. And most important, they can contain other GUI components such as buttons and menus. Java, of course, has a built-in class to represent windows. There are actually several different types of window, but the most common type is represented by the JFrame class (which is included in the package javax.swing). A JFrame is an independent window that can, for example, act as the main window of an application. One of\u00ae the most important things to understand is that a JFrame object comes with many of the behaviors of windows already programmed in. In particular, it comes with the basic properties shared by all windows, such as a titlebar and the ability to be opened and closed. Since a\u00ae JFrame comes with these behaviors, you don\u2019t have to program them yourself! This is, of course, one of the central \u00aeideas of objectoriented  programming. What a JFrame doesn\u2019t come with, of course, is content, the stuff that is contained in the window. If\u00ae you don\u2019t add any other content to a JFrame, it will just display a large blank area. You c\u00aean add content \u00aeeither b\u00aey creating a JFrame object and t\u00aehen adding the content to it or by creating a subclass of JFrame and adding the content in the constructor of that subclass. The main program above decla\u00aeres a variable, window, of type JFrame and sets it to refer to a new window object with the statement: JFrame window = new JFrame(\\\"GUI Test\\\");",
    "page68": "The content that is displayed in a JFrame is called its content pane. (In addition to its content pane, a JFrame can also have a menu bar, which is a separate thing that I will talk about later.) A basic JFrame already has a blank content pane; you can either add things to that pane or you can replace the ba\u00aesic content pane entirely. In my sample program, the line window.setContentPane(content) replaces the original blank content pane with a different component. (Remember that a \u201ccomponent\u201d is just a visual element of a graphical user interface). In this case, the new content is a component of type JPanel. JPanel is another of the fundamental classes in Swing. The basic JPanel is, again, just a blank rectangle. There are two ways to make a useful JPanel: The first is to add other components to the panel; the second is to draw something in the panel. Both of these techniques are illustrated in the sample program. In fact, you will find two JPanels in the program: content, which is used to contain other components, and displayPanel, which is used as a drawing surface. Let\u2019s look more\u00ae closely at displayPanel. This variable is of type HelloWorldDisplay, which is a nested static class inside the HelloWorldGUI2 class. (Nested classes were introduced in This class defines just one instance method, paintComponent(), which overrides a method of the same name in the JPanel class. Components and Layout- Another way of using a JPanel is as a container to hold other components. Java has many classes that define GUI components. Before these components can appear on the screen, they must be added to a container. In this program, the variable named content refers to a JPanel that is used as a container, and two other components are added to that container. This is done\u00ae in the statements: content.add(displayPanel, BorderLayout.CENTER); content.add(okButton, BorderLayout.SOUTH); Here, content refers to an object of type JPanel; later in the program, this panel becomes the content pane of the window. The first component that is added to content is displayPanel which, as discussed above, displays the message, \u201cHello World!\u201d. The second is okButton which represents the button that the user clicks to close the window. The variable okButt\u00aeon is of type JButton, the Java class that represents push buttons.",
    "page69": "The \u201cBorderLayout\u201d stuff in these statements has to do with how the two components are arranged in the container. When components are added to a container, there has to be some way of deciding how those components are arranged inside the container. This is called \u201claying out\u201d the components in the container, and the most common technique for laying out components is to use a lay\u00aeout manager. A layout manager is an object that implements some policy for how to arrange the components in a container; different types of layout manager implement different policies. One type of layout manager is defined by the BorderLayout class. In the program, the statement. content.setLayout(new BorderLayout()); creates a new BorderLayout object and tells the content panel to use the new object as its layout manager. Essentially, this line determines how components that are added to the content panel will be arranged inside the panel. We will cover layout managers in much more detail later, but for now all you need to know is that adding okBut\u00aeton in the BorderLayout.SOUTH position puts the button at the bottom of the panel, and putting displayPanel in the BorderL\u00aeayout.CENTER position makes it fill any space that is not taken up by the button. This example shows a general technique f\u00aeor setting up a GUI: Create a container and assign a layout manager to it, create components and add them to the container, and use the container as the content pane of a window or applet. A container is itself a component, so it is possible that some of the components that are added to the top-level container\u00ae are themselves containers, with their own layout managers and components. This ma\u00aekes it possible to build up complex user interfaces in a hierarchical fashion, with containers inside containers inside con\u00aetainers.",
    "page70": "Events and Listeners- The structure of containers and components sets up the physical appearance of a GUI, but it doesn\u2019t say anything about how the GUI behaves. That is, what can the user do to the GUI and how will it respond? GUIs are largely event-driven; that is, the program waits for events that are generated by the user\u2019s actions (or by some other cause). When an event occurs, the \u00aeprogram responds by executing an event-handling method. In order to program the behavior of a GUI, you have to write event-handling methods to respond to the events that you are interested in. The most common technique for handling events in Java is to use event listeners. A listener is an object that includes one or more event-handling methods. When an event is detected by another object, such as a button or menu, the listener object is notified and it responds by running the appropriate event-handling method. An event is detected or generated by an object. Another object, the listener, has the responsibility of responding to the event. The event itself is actually represented by a third object, which carries information about the type of event, when it occurred, and so on. This division of responsibilities makes it easier to organize large programs. As an example, consider the OK button in the sample program. When the user clicks the button, an event is generated. This event is represented by an object belonging to the class ActionEvent. The event that is generated is associated with the button; we say that the button is the source of the event. The listener object in this case is an object belonging to the class ButtonHandler, which is d\u00aeefined as a nested class inside HelloWorldGUI2. private static class ButtonHandler implements ActionListener { public void actionPerformed(ActionEvent e) { System.exit(0); } } This class implemen\u00aets the ActionListener interface a requirement for listener objects that handle events from buttons. The eventhandling method is named actionPerformed, as specified by the ActionListener interface. This method contains the code that is executed when the user clicks the button; in this case, the code is a call to System.exit(), which will terminate the program. There is one more ingredient that is necessary to get the event from the button to the listener object: The listener object must register itself with the button as an event listener. This is done with the statement: okButton.addActionListener(listener);",
    "page71": "This statement tells okButton that when the user clicks the button, the ActionEvent that is generated should be sent to listener. Without this statement, the button has no way of knowing that some other object would like to listen for events from the button. This example shows a general technique for programming the behavior of a GUI: Write classes that include event-handl\u00aeing methods. Create objects that belong to these classes and register them as listeners with the objects that will actually detect or generate the events. When an event occurs, the listener is notified, and the code that you wrote in one of its event-handling methods is executed. At first, this might seem like a very roundabout and complicated way to get things done, but as you gain experience with it, you will find that it is very flexible and that it goes together very well with object oriented pro\u00aegramming. (We will return to events and listeners in much more detail in Section 6.3 and later sections, and I do not expect you to completely understand them at this time.) Applets and HTML- Although stand-alone applications are probably more important than applets at this point in the history of Java, applets are still widely used. They can do things on Web pages that can\u2019t easily be done with other technologies. It is easy to distribute applets to users: The user just has to open a Web page, and the applet is there, with no special installation required (although the user must have an appropriate version of Java installed on their computer). And of course, applets are fun; now that the Web has become such a common part of life, it\u2019s nice to be able to see your work running on a web page. The good news is that writing applets is not much different from writing stand-alone applications. The structure of an applet is essentially the same as the structure of the JFrames that were introduced in the previous section, and events are handled in the same way in both types of program. So, most of what you learn about applications applies to applets, and vice versa. Of course, one difference is that an applet is dependent on a Web page, so to use applets effectively, you have to learn at le\u00aeast a little about creating Web pages. Web pages are written using a language called HTML (HyperText Markup Language). In Subsection 6.2.3, below, you\u2019ll learn how to use HTML to create Web page\u00aes that display applets. Hibernate is an ambitious project that aims to provide a complete solution to the problem of managing persistent data in Java. Today, Hibernate is not only an ORM service, but also a\u00ae collection of data management tools extending well beyond ORM. The Hibernate project suite includes the following:  Hibernate ORM Hibernate ORM consists of a core, a base service for persistence with SQL databases, and a native proprietary API. Hibernate ORM is the foundation for several of the other pro\u00aejects and is the oldest Hibernate project. You can use Hibernate ORM on its own, independent of any framework or any particular runtime environment with all JDKs. It works in every Java EE/J2EE application server, in Swing applications, in a simple servlet container, and so on. As long as you can configure a data source for Hibernate, it works. Hibernate EntityManager This is Hibernate\u2019s implementation of the standard Java Persistence APIs, an optional module you can stack on top of Hibernate ORM. You can fall back to Hibernate when a plain Hibernat\u00aee interface or even a JDBC Connection is needed. Hibernate\u2019s native features are a superset of the JPA persistence features in every respect. \uf0a1 Hibernate Validator Hibernate provides the reference implementation of the Bean Validation (JSR 303) specification. Independent of other Hibernate projects, it provides declarative validation for your domain model (or any other) classes. Hibernate Envers Envers is dedicated to audit logging and keeping multiple \u00aeversions of data in your SQL database. This helps you add data history and audit trails to your application, similar to version control systems you might already be familiar with such as Subversion and Git. Hibernate Search Hibernate Search keeps an index of your domain model data up to date in an Apache Lucene database. It lets you query this database with a powerful and naturally integrated API. Many projects use Hibernate Search in addition to Hibernate ORM, adding full-text search capabilities. If you have a free text search form in your application\u2019s user interface, and you want happy users, work with Hibernate Search. Hibernate Search isn\u2019t covered in this book; you can find more information in Hibernate Search in Action by Emmanuel Bernard (Bernard, 2008). Hibernate OGM The most recent Hibernate project is the object/grid mapper. It provides JPA support for NoSQL solutions, reusing the Hibernate core engine but persisting mapped entities into a key/value-, document-, or graph-oriented data store. Hibernate OGM isn\u2019t covered in this book. Let\u2019s get started with your first Hibernate and JPA project.",
    "page72": "JApplet- The JApplet class (in package javax.swing) can be used as a basis for writing applets in the same way that JFrame is used for writing stand-alone applications. The basic JApplet class represents a blank rectangular area. Since an applet is not a stand-alone application, this area must appear on a \u00aeWeb page, or in some other environment that knows how to display an applet. Like a JFrame, a JApplet contains a content pane (and can contain a menu bar). You can add content to an applet either by adding content to\u00ae its content pane or by replacing the content pane with another component. In my examples, I will generally create a JPanel and use it as a replacement for the applet\u2019s content pane. To create an applet, you will write a subclass of JApplet. The JApplet class defines several instance methods that are unique to applets. These methods are called by the applet\u2019s environment at certain points during\u00ae the applet\u2019s \u00ae\u201clife cycle.\u201d In the JApplet class itself, these methods do nothing; you can override these methods in a subclass. The most important of these special applet methods is public void init() An applet\u2019s init() method is called when the applet is created. You can use the init() method as a place where you can set up the physical structure of the applet and the event handling that will determine its behavior. (You can also do some initialization in the constructor for your class, but there are certain aspects of the applet\u2019s environment that are set up after its constructor is called but before the init() method is called, so there are a few operations that will work in the init() method but will not work in the constructor.) The other applet life-cycle methods are start(), stop(), and destroy(). I will not use these methods for the time being and will not discuss them here except to mention that destroy() is called at the end of the applet\u2019s lifetime and can be used as a place to do any necessary cleanup, such as closing any windows that were opened by the applet.",
    "page73": "Graphics and Painting- Everthing you see on a computer screen has to be drawn there, even the text. The Java API includes a range of classes and methods that are devoted to drawing. In this section, I\u2019ll look at some of the most basic of these. The physical st\u00aeructure of a GUI is built of components. The te\u00aerm component refers to a visual element in a GUI, including buttons, menus, text-input boxes, scroll bars, check boxes, and so on. In Java, GUI components are represented by objects belonging to subclasses of the class java.awt.Component. Most components in the Swing GUI although not top-level components like JApplet and JFramebelong to subclasses of the class javax.swing.JComponent, which is itself a subclass of java.\u00aeawt.Component. Every component is responsible for drawing it\u00aeself. If you want to us\u00aee a standard component, you only have\u00ae to add it to your applet or frame. You don\u2019t have to worry about painting it on the screen. That will happen automatically, since it already knows how to draw itself. Fonts- A font represents a particular size and style of text.\u00ae The same character will appear different in different fonts. In Java, a font is characterized by a font name, a style, and a size. The available font names are system dependent, but you can always use the following four strings as font names: \u201cSerif\u201d, \u201cSansSerif\u201d, \u201cMonospaced\u201d, and \u201cDialog\u201d. (A \u201cserif\u201d is a little decoration on a character, such as a short horizontal line at the bottom of the letter i. \u201cSansSerif\u201d means \u201cwithout serifs.\u201d \u201cMonospaced\u201d\u00ae means that all the characters in the font have the same width. The \u201cDialog\u201d font is\u00ae the one that is typically used in dialog boxes.). Shapes The Graphics class includes a large number of instance methods for drawing various shapes, such as lines, rectangles, and ovals. The shapes are specified using the (x,y) coordinate system described above. They are drawn in the current drawing color of the graphics context. The current drawing color is set to the foreground color of the component when the graphics context is created, but it can be changed at any time using the setColor() method. Here is a list of some\u00ae of the most important drawing methods. With all these commands, any drawing that is done outside the boundaries of the component is ignored. Note that all these methods are in the Graphics class, so they all must be called through an object of type Graphics.",
    "page74": "Mouse Events- Events are central to programming for a graphical user interface. A GUI program doesn\u2019t have\u00ae a main() routine that outlines what will happen when the program is run, in a step-by-step process from beginning to end. Instead, the program must be prepared to respond to various kinds of events that can happen at unpredictable times and in an order that the program doesn\u2019t control. The most basic kin\u00aeds of events are\u00ae generated by the mouse and keyboard. The user can press any key on the keyboard, move the mouse, or press a button on the mouse. The user can do any of these things at any time, and the computer has to respond appropriately. In Java, events are represented by objects. When an event occurs, the system collects all the information relevant to the event and constructs an object to contain that information. Different types of events are represented by objects belonging to different classes. For example, when the user presses one of the buttons on a mouse, an object belonging t\u00aeo a class called Mouse\u00aeEvent is constructed. The object contains information such as the source of the event (that is, the component on which the user clicked), the (x,y) coordinates of the point in the component where the click occurred, and which button on the mouse was pressed. When the user presses a key on the keyboard, a KeyEvent is created. After the event object is constructed, it is passed as a parameter to a designated subroutine.\u00ae By writing that subroutine, the programmer says what should happen when the event occurs. As a Java programmer, you get a fairly high-level view of events. There is a lot of processing that goes on between the time that the user presses a key or moves the mouse and the time that a subroutine in your program is called to respond to the event. Fortunately, you don\u2019t need to know much \u00aeabout that processing. But you should understand this much: Even though your GUI program doesn\u2019t have a main() routine, there is a sort of main routine running somewhere that executes a loop of the form while the program is still running: Wait for the next event to occur Call a subroutine to handle the event This loop is called an event loop. Every GUI program has an event loop. In Java, you don\u2019t have to write the loop. It\u2019s part of \u201cthe system.\u201d If you write a GUI program in some other language, you might have to provide a main routine that runs an event loop. In this section, we\u2019ll look at handling mouse events in Java, and we\u2019ll cover the framework for handling events in general. The next section will cover keyboard-related events and timer even\u00aets. Java also has other types of events, which are produced by GUI components.",
    "page75": "Event Handling- For an event to have any effect, a program must detect the event and react to it. In order to detect an event, the program must \u201clisten\u201d for it. Listening for events is something that is done by an object called an event listener. An event listener object must contain instance methods for handling the events for which it listens\u00ae. For example, if an object is to serve as a listener for events of type MouseEvent, then it must contain the following method (among several others): public void mousePressed(MouseEvent evt) { } The body of the method defines how the object responds when it is notified that a mouse button has been pressed. The parameter, evt, contains information about the event. This information can be used by the listener object to determine its response. The methods that are required in a mouse event listener are specified in an interface named MouseListener. To be used as a listener for mouse events, an object must implement this MouseListener interface. (To review briefly: An interface in Java is just a list of instance methods. A class can \u201cimplement\u201d an interface by doing two things. First, the class must be declared to \u00aei\u00aemplement the interface, as\u00ae in \u201cclass MyListener implements MouseListener\u201d or \u201cclass MyApplet extends JApplet implements MouseListener\u201d. Second, the class must include a definition for each instance method specified in the interface. An interface can be used as the type for a variable or formal parameter. We say that an object implements the MouseListener interface if it belongs to a class that implements the MouseListener interface. Note that it is not enough for the object to include the specified methods. It must also belong to a class that is specifically declared to implement the interface.) Many events in Java are associated with GUI components. For example, when the user presses a button on the mouse, the associated component is the one that the user clicked on. Before a listener object can \u201chear\u201d events associated with a given component, the listener object must be registered with the component. If a MouseListener object, mListener, needs to hear mouse events associated with a Component object, comp, the listener must be registered with the component by calling \u201ccomp.addMouseListener(mListener);\u201d. The addMouseListener() method is an instance method in class Compon\u00aeent, and so can be used with any GUI component object. In our first few examples, we will listen for events on a JPanel that is being used \u00aeas a drawing surface. The event classes, such as MouseEvent, and the listener interfaces, such as MouseListener, are defined in the package java.awt.event. This means that if you want to work with events, you should either include the line \u201cimport java.awt.event.*;\u201d at the beginning of your source code file or import the individual classes and interfaces. Admittedly, there is a large number of details to tend to when you want to use events. To summarize, you must.",
    "page76": "1. Put the import specification \u201cimport java.awt.even\u00aet.*;\u201d (or individual imports) at the beginning of your source code; 2. Declare that some class implements the appropriate listener interface, such as MouseListener; 3. Provide definitions in that class for the subroutines from the interface; 4. Register the \u00aelistener object with the component that will generate the events by calling a method such as addMouseListener() in the component. Any object can act as an event listener, provided that it implements the appropriate interface. A component can listen for the events that it itself generates. A panel can listen for events from components that are contained in the panel. Keyboard Events- In Java, user actions become events in a program. These events are associated with GUI components. When the user presses a button on the\u00ae mouse, the event that is generated is associated with the component that contains the mouse cursor. What about keyboard events? When the user presses a key, what component is associated with the key event that is generated? A GUI uses the idea of input focus to determine the com\u00aeponent associated with keyboard events. At any given time, exactly one interface element on the screen has the input focus, and that is where all keyboa\u00aerd events are directed. If the interface element happens to be a Java component, then the information about the keyboard event becomes a Java object of type KeyEvent, and it is delivered to any listener objects that are listening for KeyEvents associated with that component. The necessity of managing input focus adds an extra twist to working with keyboard events. It\u2019s a good idea to give the user some visual feedback about wh\u00aeich component has the input focus. For example, if the component is the typing area of a word-processor, the feedback is usually in the form of a blinking text cursor. Another common visual clue is\u00ae to draw a brightly colored border around the edge of a component when it has the input focus, as I do in the examples given later in this section.",
    "page77": "A component that wants to have \u00aethe input focus can call the method requestFocus(), which is defined in the Component class. Calling this method does not absolutely guarantee that the component will actually get the input focus. Several components might request the focus; only one will get it. This method should only be used in certain circumstances in any case, since it can be a rude surprise to the user to have the focus suddenly pulled away from a component that the user is working with. In a typical user interface, the user can choose to give the focus to a component by clicking on that component with the mouse. And pressing the tab key will often move the focus from one component to another. Some components do not automatically request the input focus when the user clicks on them. To solve this problem, a program has to register a mouse listener with the component to detect user clicks. In response to a user click, the mousePressed() method should call requestFocus() for the component. This is true, in particular, for the components that are used as drawing surfaces in the examples in this chapter. These components are defined as subclasses of JPanel, and JPanel objects do not receive the input focus automatically. If you want to be able to use the keyboard to interact with a JPanel named drawingSurface, you have to register a listener to listen for mouse events on t\u00aehe drawingSurface and call drawingSurface.requestFocus() in the mousePressed() method of the listener object. As our first example of processing key events, we look at a simple program in which the user moves a square up, down, left, and right by pressing arrow keys. When the user hits the \u2019R\u2019, \u2019G\u2019, \u2019B\u2019, or \u2019K\u2019 key, the color of the square is set to red, green, blue, or black, respectively. Of course, none of these key events are delivered to the program unless it has the input focus. The panel in the program changes its appearance when it has the input focus: When it does,\u00ae a cyan-colored border is drawn around the panel; when it does not, a gray-colored bor\u00aeder is drawn. Also, the panel displays a different message in each case. If the panel does not have the input focus, the user can give the input focus to the p\u00aeanel by clicking on it. The complete source code for this example can b\u00aee found in the file KeyboardAndFocusDemo.java. I will discuss \u00aesome aspects of it below. After reading this section, you should be able to understand th\u00aee source code in its entirety. Here is what the program looks like in its focussed state.",
    "page78": "State Machines- The information stored in an object\u2019s instance varia\u00aebles is said to represent the state of that object. When one of the object\u2019s methods is called, the action taken by the object can depend on its state. (Or, in the terminology we have been using, the definition of the method can look at the instance variables to decide what to do.) Furthermore, the stat\u00aee can change. (That is, the definition of the method can assign new values to the instance variables.) In computer science, there is the idea of a state machine, which is just something that has a state and can change state in response to events or inputs. The response of a state machine to an event or input depends on what state it\u2019s in. An object is a kind of state machine. Sometimes, this point of view can be very useful in designing classes. The state machine point of view can be especially useful in the type of event-oriented programming that is required by graphical user interfaces. When designing a GUI program, you can ask yourself: What information about state do I need to keep track of? What events can change the state of the program? How will my response to a given event depend on t\u00aehe current state? Should the appearance of the GUI be changed to reflect a change in state? How shou\u00aeld the paintComponent() method take the state into account? All this is an alternative to the top-down, ste\u00aep-wise-refinement style of program design, which does not apply to the overall design of an event-oriented program. Basic Components- In preceding sections, you\u2019ve seen how to use a graphics context to draw \u00aeon the screen and how to handle mouse events and keyboard events. In one sense, that\u2019s all there is to GUI programming. If you\u2019re willing to program all the drawing and handle all the mouse and keyboard events, you have nothing more to learn. However, you would either be doing a lot more work than you need to do, or you would be limiting yourself to very simple user interfaces. A typical user interface uses standard GUI components such as buttons, scroll bars, text-input boxes, and menus. These components have already b\u00aeeen written for you, so you don\u2019t have to duplicate the work involved in developing them. They know how to draw themselves, and they can handle the details of processing the mouse and keyboard events that concern them.",
    "page79": "Consider one of the simplest user interface components, a push button. The button has a border, and it displays some text. This text can be changed. Sometimes the button is disabled, so that clicking on it doesn\u2019t have any effect. When it is disabled, its appearance changes. When the user clicks on the push button, the button changes appearance while the mouse button is pressed and changes back when the mouse button is released. In fact, it\u2019s more complicated than that. If the user move\u00aes the mouse outside the push button before releasing the mouse button, the button changes to its regular appearance. To implement this, it is necessary to respond to mouse exit or mouse drag events\u00ae. Furthermore, on many platforms, a button can receive the input focus. The button changes appearance when it has the focus. If the button has the focus and the user presses the space bar, the button is triggered. This means that the button\u00ae must respond to keyboard and focus events as well. Fortunately, you don\u2019t have to program any of this, provided you use an object belonging to the standard class javax.swing.JButton. A JButton object draws itself and processes mouse, keyboard, and focus events on its own. You only hear from the Button when the user triggers it by clicking on it or pressing the space bar while the button has the input focus. When this happens, t\u00aehe JButton object creates an event object belonging to the class java.awt.event.ActionEvent. The event object is sent to any registered listeners to tell them that the button has been pushed. Your program gets only the information it needs the fact that a button was pushed. JTextField and JTextArea- The JTextField and JTextArea classes represent components that contain text that can be edited by the us\u00aeer. A JTextField holds a singl\u00aee line of text, while a JTextArea can hold multiple lines. It is also possible to set a JTextField or JTextArea to be read-only so that the user can read the text that it contains but cannot edit the text. Both classes are subclasses of an abstract class, \u00aeJTextComponent, which defines their common properties. JTextField and JTextArea have many methods in common. The instance method setText(), \u00aewhich takes a parameter of type String, can be used to change the text that is displayed in an input component. The contents of the component can be retrieved by calling its getText() instance method, which returns a value of type String. If you want to stop the user from modifying the text, you can call setEditable(false). Call the same method with a parameter of true to make the input component user-editable again.",
    "page80": "Dialogs- One of the commands in the \u201cColor\u201d menu of\u00ae the MosaicDraw program is \u201cCustom Color\u201d. When the user selects this command, a new window appears wh\u00aeere the user can select a color. This window is an example of a dialog or dialog box. A dialog is a type of window that is general\u00aely used for short, single purpose interactions with the user. For example, a dialog box can be used to display a message to the user, to ask the user a question, to let\u00ae the user select a file to be opened, or to let the user select a color. In Swing, a dialog box is represented by \u00aean object belonging to the class JDialog or to a subclass. The JDialog c\u00aelass is very similar to JFrame and is used in much the same way. Like a frame, a dialog box is a separate window. Unlike a frame, however, a dialog is not com\u00aepletely independent. Every dialog is associated wi\u00aeth a frame (or another dialog), which is called its parent window. The dialog box \u00aeis dependent on its parent. For example, if the parent is closed, the dialog box will also be closed. It is possible to create a dialog box without specifying a parent, but in that case a an invisible frame is created by the system to serve as the parent. Dialog boxes can be either modal or modeless. When a modal dialog is created, its parent frame is blocked. That is, the user will not be able to interact with the parent until the dialog box is closed. Modeless dialog boxes do not block their parents in the same way, so they seem a lot more like independent windows. In practice, modal dialog boxes are easier to use and are much more common than modeless dialogs.",
    "page81": "Creating Jar Files- As the final topic for this chapter, we l\u00aeook again at jar files. Recall that a jar file is a \u201cjava archive\u201d that can contain a number of class files. When creating a program that uses more than one class, it\u00ae\u2019s usually a good idea to place all the classes that are required by the program into a jar file, since then a user will only need that one file to run the program. d\u00aeiscusses how a jar file can be used for an applet. Jar files can also be used for stand-alone applications. In fact, it is possible to make a \u00aeso-called executable jar file. A user can run an executable jar file in much the same way as any other application, usually by double-clicking the icon of the jar file. (The user\u2019s computer must have a correct version of Java installed, and the computer must be configured correctly for this to work. The\u00ae configuration is usually done automatically when Java is installed, at least on Windows and Mac OS.) The question, then, is ho\u00aew to create a jar file. The answer depends on what programming environment you are using. The two basic types of programming environment command line and IDE were discussed in Section 2.6. Any IDE (Integrated Programming Environment) for Java should have a command for creating jar files. In the Eclipse IDE, for example, it\u2019s done as follow\u00aes: In the Package Explorer pane, select the programming project (or just all the individual source code files that you need). Right-click on the selection, and choose \u201cExport\u201d from the menu that pops up. In the window that appears, select \u201cJAR file\u201d and click \u201cNext\u201d. In the window that appears next, enter a name for the jar file in the box labeled \u201cJAR file\u201d. (Click the \u201cBrowse\u201d button next to this box to select the file name using a file dialog box.) The name of the file should end with \u201c.jar\u201d. If you are creating a regular jar file, not an executable one, you can hit \u201cFinish\u201d at this point, and the jar file will be created. You could do this, for example, if the jar file contains a\u00aen applet but no main program. To create an executable file, hit the \u201cNext\u201d button twice to get to the \u201cJar Manifest Specification\u201d screen. At the bottom of this screen is an input box labeled \u201cMain class\u201d. You have to enter the name of the class that contains the main() routine that will be run when the jar file is executed. If you hit the \u201cBrowse\u201d button next to the \u201cMain class\u201d box, you can select the class from a list of classes that contain main() routines. Once you\u2019ve selected the main class, you can click the \u201cFinish\u201d button to create the executable jar file.",
    "page82": "Creating and Using Array\u00aes- When a number of data items are chunked together into a unit, the result is a data s\u00aetructure. Data structures can be very complex, but in many applications, the appropriate data structure consists simply of a sequence of data items. Data structures of this simple variety can be either arrays or records. The term \u201crecord\u201d is not used in Java. A record is essentially the same as a Java object that has instance variables only, but no instance methods. Some other languages, which do not support objects in general, nevertheless do support records. The C programming language, for example, is not object-oriented, but it has \u00aerecords, which in C go by the name \u201cstruct.\u201d The data items in a record in Java, an object\u2019s instance variables are called the fields of the record. Each item is referred to using a field name. In Java, field names are just the names of the instance variables. The distinguishing characteristics of a record are that the data items in the record are referred to by name and that different fields in a record are allowed to be of different types. Arrays- Like a record, an array is a sequence of items. However, where items in a record are referred to by name, the items in an array are numbered, and individual items are referred to by their position number. Furthermore, all the items in an array must be of the same type. The definition of an array is: a numbered sequence of items, which are all of the same type. The number of items in an array is called the length of the array. The position number of an item in an array is called the index of that item. The type of the individual items in an array is called the base type of the array. The base type of an array can be any Java type, that is, one of the primitive types, or a class name, or an interface name. If the base type of an array is int, it is referred to as an \u201carray of ints.\u201d An array with base type String is referred to as an \u201carray of Strings.\u201d However, an array is not, properly speaking, a list of integers or strings or\u00ae other values. It is better thought of as a list of variables of type int, or of type String, or of some other type. As always, there is some potential for confusion between the two uses of a variable: as a name for a memory location and as a name for the value stored in that memory location. Each position in an array acts as a variable. Each position can hold a value of a specified type (the base type of the array). The value can be changed at any time. Values are stored in an array. The array is the container, not the values.",
    "page83": "Java is a general-purpose, robust, secure, and object-oriented programming language. It is a high-level language, I.e., its syntax u\u00aeses English like language. It was developed by Sun Microsystems in the year 1995. It is now maintained and distributed by Oracle. Java has its runtime environment and API; therefore, it is also called a platform. Java is used in a large number of applications over the years. However, it has various advantages and disadvantages given below. Java is a simple programming language since it is easy to le\u00aearn and easy to understand. Its syntax is based on C++, an\u00aed it uses automatic garbage collection; therefore, we don't need to remove the unreferenced objects from memory. Java has also removed the features like explicit pointers, operator overloading, etc., making it easy to read and write. Java uses an obj\u00aeect-oriented paradigm, which makes it more practical. Everything in Java is an object which takes care of both data and behavior. Java uses object-oriented concepts like object, class, inheritance, encapsulation, polymorphism, and abstraction. Java is a secured programming language because it doesn't use Explicit pointers. Also, Java programs run inside the v\u00aeirtual machine sandbox. JRE also provides a classloader, which is used to load the class into JVM dynamically. It separates the class packages of the local file system from the ones that are being imported from the network. Java is a robust programming language since it uses strong memory management. We can also handle exceptions through the\u00ae Java code. Also, we can use type checking to make our code more secure. It doesn't provide explicit pointers so that the programmer cannot access the memory directly from the code. Java code can run on multiple platforms directly, I.e., we need not compile it every time. It is right once, runs anywhere language (WORA) which can be converted into byte code at the compile time. The byte code i\u00aes a platform-independent code that can run on multiple platforms. Java uses a multi-threaded environment in which a bigger task can be converted into various threads and run separately. The main advantage of multi-threadin\u00aeg is that we need \u00aenot provide memory to every running thread.",
    "page84": "The items in an array really, the individual variables that make up the array are more often referred to as the elements of the array. In Java, the elements in an array are always numbered starting from zero. That is, the index of the first element in the array is zero. If the length of the array is N, then the index of the last element in the array is N-1. Once an array has been created, its length cannot be changed. Java arrays are objects. This has several consequences. Arrays are created using a form of the new operator. No variable can ever hold an array; a variable can only refer to an array. Any variable that can refer to an array can also hold the value null, meaning that it doesn\u2019t at the moment refer to anything. Like any object, an array belongs to a class, which like all classes is a subclass of the class Object. The elements of the array are, essentially, instance variables in the array object, except that they are referred to by number rather than by name. Nevertheless, even th\u00aeough arrays are objects, there are differences between arrays and other kinds of objects, and there are a number of special language features in Java for creating and using arrays. Random Access- So far, all my examples of array processing have used sequential access. That is, the e\u00aelements of the array were processed one after the other in the sequence in which they occur in the array. But one of the big advantages of arrays is that they allow random access. That is, every element of the array is equally accessible at any given time. As an example, let\u2019s lo\u00aeok at a well-known problem called the birthday problem: Suppose that there are N people in a room. What\u2019s the chance that there are two people in the room who have the same birthday? (That is, they were born on the same day in the same month, but not necessarily in the same year.) Most people severely underestimate the probability. We",
    "page85": "will actually look at a different version of the question: Suppose you choose people at random and check their birthdays. How many people will you check before you find one who has the same birthday as someone you\u2019ve already checked? Of course, the answer in a particular case depends on random factors, but we can simu\u00aelate the experiment with a computer program and run the program several times to get an idea of how many people need to be checked on average. Dynamic Arrays and ArrayLists- The size of an array is fixed when it is created. In many cases, however, the number of data items that are actually stored in the array varies with time. Consider the following examples: An array that stores the lines of text in a word-processing program. An array that holds the list of computers that are currently downloading a page from a Web site. An array that contains the shapes that have been added to the screen by the user of a drawing program. Clearly, we need some way to deal with cases where the number of data items in an array is not fixed. Partially Full Arrays- Consider an application where the number of items that we want to store in an array changes as the program runs. Since the size of the array can\u2019t actually be changed, a separate counter variable must be used to keep track of how many spaces in the array are in use. (Of course, every space in the array has to contain something; the \u00aequestion is, how many spaces contain useful or valid items?) Consider, for example, a program that reads positive integers entered by the user and stores them for later processing. The program stops reading when the user inputs a number that is less than or equal to zero. The input numbers can be kept in an array, numbers, o\u00aef type int[ ]. Let\u2019s say that no more than 100 numbers will be input. Then the size of the array can be fixed at 100. But the program must\u00ae keep track of how many numbers have actually been read and stored in the array. For this,\u00ae it can use an integer variable, numCount. Each time a number is stored in the array, numCount must be incremen\u00aeted by one. As a rather silly example, let\u2019s write a program that will read the numbers input by the user and then print them in reverse order.",
    "page86": "Dynamic Arrays- In each of the above examples, an arbitrary limit was set on the number of items 100 ints, 10 Players, 100 Shapes. Since the size of an array is fixed, a given array can only hold a certain maximum number of items. In many cases, such an arbitrary limit is undesirable. Why should a program work for 100 data values, but not for 101? The obvious alternative of making an array that\u2019s so big that it will work in any practical case is not usually a good solution to the problem. It means that in most cases, a lot\u00ae of computer memory will be wasted on unused space in the array. That memory might be better used for something else. And what if someone is using a computer that coul\u00aed handle as many data values \u00aeas the user actually wants to process, but doesn\u2019t have enough memory to accommodate all the extr\u00aea space that you\u2019ve allocated for your huge array? Clearly, it would be nice i\u00aef we could increase the size of an array at will. This is not possible, but what is possible is almost as good. Remember that an array variable does not actually hold an array. It just holds a reference to an array object. We can\u2019t make the array bigger, but we can make a new, bigger array object and change the value of the array variable so that it refers to the bigger array. Of course, we also have to copy the contents of the old array into the new array. The array variable then refers to an array object that contains all the data of the old array, with room for additional data. The old array will be garbage collected, since it is no longer in use. ArrrayLists- The DynamicArrayOfInt class could be used in any situation where an array of int with no preset limit on the size is needed. However, if we want to store Shapes instead of ints, we would have to define a new class to do it. That class, probably named \u201cDynamicArrayOfShape\u201d, would look exactly the same as the DynamicArrayOfInt class except that everywhere the type \u201cint\u201d appears, it would be replaced by the type \u201cShape\u201d. Similarly, we could define a Dyna\u00aemicArrayOfDouble class, a DynamicArrayOfPlayer class, and so on. But there is something a little silly about this, since all these classes are close to being identical. It would be nice to be able to write some kind of source code, once and for all, that could be used to generate any of these classes on demand, given the type of value that we want to store. This would be an example of generic programming. Some programming language\u00aes, including C++, have had support for generic programming for some time. With version 5.0, Java introduced true generic programming, but even before that it had something that was very similar: One can come close to generic programming in Java by working with data str\u00aeuctures that contain elements of type Object. We will first consider the almost-generic programming that has been available in Java from the beginning, and then we will look at the change that was introduced in Java\u00ae 5.0.",
    "page87": "In Java, every class is a subclass of the class named Object. This means that every object can be assigned to a variable of type Obje\u00aect. Any object can be put into an array of type Object[ ]. If we defined a DynamicArrayOfObject class, then we could store objects of any type. This is not true generic programming, and it doesn\u2019t apply to the primitive types such as int and double. But it does come close. In fact, there is no need for us to define a DynamicArrayOfObject class. Java already has a standard class named ArrayList that serves much the same purpose. The ArrayList class is in the package java.util, so if you want to use it in a program, you should put the directive \u201cimport java.util.ArrayList;\u201d at the beginning of your source code file. The ArrayList class differs from my DynamicArrayOfInt class in that an ArrayList object always has a definite size, and it is illegal to r\u00aeefer to a position in the ArrayList that lies outside its size. In this, an ArrayList is more like a regular array. However, the size of an ArrayList can be increased at will. The ArrayList class defines many instance methods. Parameterized Types The main difference between true generic programming and the ArrayList examples in the previous subsection is the use of the type Object as the basic type for objects that are stored in a list. This has at\u00ae least two unfortunate consequences: First, it makes it necessary to use type-casting in almost every case when an element is retrieved from that list. Second, since any type of object can legally be added to the list, there is no way for the compiler to detect an attempt to add the wrong type of object to the list; the error will be detected only at run time when the object is retrieved f\u00aerom the list and the attempt to type-cast the object fails. Compare this to arrays. An array of type BaseType[ ] can only hold objects of type BaseType. An attempt to store an object of the wrong type in the array will be detected by the compiler, and there is no need to type-cast items that are retrieved from the array back to type BaseType. To address this problem, Java 5.0 introduced parameterized types. ArrayList is an example: Instead of using the plain \u201cArrayList\u201d type, it is possible to use ArrayList<BaseType>, where BaseType is any object type, that is, the name of a class or of an interface. (BaseType cannot be one of the primitive types.) ArrayList<BaseType> can be used to create lists that can hold only objects of type BaseType.",
    "page88": "Searching and Sorting- Two array processing techniques tha\u00aet are particularly common are searching and sorting. Searching here refers to finding an item in the array that meets some specified criterion. Sorting refers to rearranging all the items in the array into increasing or decreasing order (where the meaning of increasing and decreasing can depend on the context). Sorting and searching are often discussed, in a the\u00aeoretical sort of way, using an array of numbers as an example. In practical situations, though, more interesting types of data are usually involved. For example, the array might be a ma\u00aeiling list, and each element of the array might be an object \u00aecontaining a name and address. Given the name of a person, you might want to look up that person\u2019s address. This is an example of searching, since you want to find the object in the array that con\u00aetains the given name. It would also be useful to be able to sort the array according to various criteria. One example of sorting would be ordering the elements of the array so that the names are in alphabetical o\u00aerder. Another example would be to order the elements of the array according to zip code before printing a set of mailing labels. (This ki\u00aend of sorting can get you a cheaper postage rate on a large mailing.) This example can be generalized to a more abstract situation in which we have an array that contains objects, and we want to search or sort the array based on the value of one of the instance variables in that array. We can use some terminology here that originated in work with \u201cdatabases,\u201d which are just large, organ\u00aeized collections of data. We refer to each of the objects in the array as a record. The instance variables in an object are then called fields of the record. In the mailing list example, each record would contain a name and address. The fields of the record might be the first name, last name, street address, state, city and zip code. For the purpose of searching or sorting, one of the fields is designated to be the key field. Searching then means finding a record in the array that has a specified value in its key field. Sorting means moving the records around in the array so that the key fields of the record are in increasing (or decreasing) order.",
    "page89": "Introduction to Correctness and Robustness- A program is correct if it accomplishes the task that it was designed to perform. It is robust if it can handle illegal inputs and \u00aeother unexpected situations in a reasonable way. For example, consider a program that is designed to read some numbers from the user and then print the same numbers in sorted order. The program is correct if it works for any set of input numbers. It is robust if it can\u00ae also deal with non-numeric input by, for example, printing an error message and ignoring the bad input. A non-robust program might cra\u00aesh or give nonsensical output in the same circumstance. Every program should be correct. (A sorting program that doesn\u2019t sort correctly is pretty useless.) It\u2019s not th\u00aee case that every program needs to be completely robust. It depends on who will use it and how it will be used. For example, a small utility program that you write for your own use \u00aedoesn\u2019t have to be particularly robus\u00aet. The question of correctness is actually more subtle than it might appear. A programmer works from a specification of what the program is supposed to do. The programmer\u2019s work is correct if the program meets its specification. But does that mean tha\u00aet the program itself is correct? What if the specification is incorrect or incomplete? A correct program should be a correct implementation of a complete and correct specification. The question is whether the specification correctly expresses the intention and desires of the pe\u00aeople for whom the program is being written. This is a question that lies largely outside the domain of computer science. Horror Stories- Most computer users have personal experience with programs that don\u2019t work or that crash. In many cases, such problems are just annoyances, but even on a personal computer there can be more serious consequences, such as lost work or lost money. When computers are given more important tasks, the consequences of failure can be proportionately more serious. Just a few years ago, the failure of two multi-million space missions to Mars was prominent in the news. Both failures were probably due to software problems, but in both cases the problem was not with an incorrect program as such. In September 1999, the Mars Climate Orbiter burned up in the Martian atmosphere because data that was expressed in English units of measurement (such as feet and pounds) was entered into a computer program that was designed to use metric units (such as centimeters and grams).",
    "page90": "A few months later, the Mars Polar Lander probably crashed because its software turned off its landing engines too soon. The program was supposed to detect the bump when the spacecraft landed and turn off the engines then. It has been determined that deployment of the landing gear might have jarred the spacecraft enough to activate the program, causing it to turn off the engines when the spacecraft was still in the air. The unpowered spacecraft would then have fallen to the Martian surface. A more robust system would have checked the altitude before turning off the engines! There are\u00ae many equally dramatic stories of problems caused by incorrect or poorly written software. Let\u2019s look at a few incidents recounted in the book Computer Ethics by Tom Forester and Perry Morrison. (This book covers various ethical issues in co\u00aemputing. It, or something like it, is essential reading for any student of computer science.) In 1985 and 1986, one person was killed and several were injured by excess radiation, while undergoing radiati\u00aeon treatments by a mis-programmed computerized radiation machine. In another case, over a ten-year period ending in 1992, almost 1,000 cancer patients received radiation dosages\u00ae that were 30% less than prescribed because of a programming error. In 1985, a computer at the Bank of New York started destroying records of on-going security transactions because of an error in a program. It\u00ae took less than 24 hours to fix the program, but by that time, the bank was out $5,000,000 in overnight interest payments on funds that it had to borrow to cover the problem. The programming of the inertial guidance system of the F-16 fighter plane would have turned the plane upside-down when it crossed the equator, if the problem had not been discovered in simulation. The Mariner 18 space probe was lost because of an error in one line of a program. The Gemini V space capsule missed its scheduled landing target by a hundred miles, because a programmer forgot to take into account the rotation of the Earth. In 1990, AT&T\u2019s long-distance telephone service was disrupted throughout the United States when a newly loaded computer program proved to contain a bug. These are just a few examples. Softwar\u00aee problems are all too common. As programmers, we need to understand why that is true and what can be done about it.",
    "page91": "Java to the Rescue- Part of the problem, according to the inventors of Java, can be traced to programming languages themselves. Java\u00ae was designed to provide some protection against certain types of errors. How can a lan\u00aeguage feature help prevent errors? Let\u2019s look at a few examples. Early programming languages did not require variables to be declared. In such languages, when a variable name is used in a program, the variable is created automatically. Yo\u00aeu m\u00aeight consider this more convenient than having to declare every variable explicitly. But there is an unfortunate consequence: An inad\u00aevertent spelling error might introduce an extra variable that you had no intention of creating. This type of error was responsible, according to one famous story, for yet another lost spacecraft. In the FORTRAN programming language, the command \u201cDO 20 I = 1,5\u201d is the first statement of a counting loop. Now, spaces are insignificant in FORTRAN, so this is equivalent to \u201cDO20I=1,5\u201d. On the other hand, the command \u201cDO20I=1.5\u201d, with a period instead of a comma, is\u00ae an assignment statement that assigns the value 1.5 to the variable DO20I. Supposedly, the inadvertent substitution of a period for a comma in a statement of this type caused a rocket to blow up on take-off. Because FORTRAN doesn\u2019t require variables to be declared, the compiler would be happy to accept the statement \u201cDO20I=1.5.\u201d It would just create a new variable named DO20I. If FORTRAN required variables to be declared, the compiler would have complained that the variable DO20I was undeclared. While most programming languages today do require variables to be declared, there are other features in common programming languages that can cause problems. Java has eliminated some of these features. Some people complain that this makes Java less efficient and less powerful. While there is some justice in this criticism, the increase in security and robustness is probably worth the cost in most circumstances. The best defense against some types of errors is to design a programming language in which the errors are i\u00aempossible. In other cases, where the error can\u2019t be completely eliminated, the language can be designed so that when the error does occur, it will automatically be detected. This will at least prevent the error from causing further harm, and it will alert the programmer that there is a bug that needs fixing. Let\u2019s look at a few cases where the designers of Java have taken these approaches.",
    "page92": "An array is created with a certain number of locations, numbered from zero up to some specified maximum index. It is an error to try to use an array location that is outside of the specified range. In Java, any attempt to do so is detected automatically by the system. In some other languages, such as C and C++, it\u2019s up to the programmer to make sure that the index is within the legal range. Suppose that an array, A, has three locations, A[0], A[1], and A[2]. Then A[3], A[4], and so on refer to memory locations beyond the end of the array. In Java, an attempt to store data in A[3] will be detected. The program will be terminated (unless the error is \u201ccaught\u201d, as discussed in Section 3.7). In C or C++, the computer will just go ahead and store the data in memory that is not part of the array. Since there is no telling what that memory location is being used for, the result will be unpredictable. The consequences could be much more serious than a terminated program. (See, for example, the discussion of bu\u00aeffer overflow errors later in this section.) Pointers are a notorious source of programming errors. In Java, a variable of object type holds either a pointer to an object or the special value null. Any attempt to use a null value as if it were a pointer to an actual object will be detected by the\u00ae system. In some other languages, again, it\u2019s up to the programmer to avoid such null pointer errors. In my old Macintosh computer, a null pointer was actually implemented as if it were a pointer to memory location zero. A program could use a null pointer to change values stored in memory near location zero. Unfortunately, the Macintosh stored important system data in t\u00aehose locations. Changing that data could cause the whole system to crash, a consequence more severe than a single failed program. Another type of pointer error occurs when a pointer value is pointing to an object of the wrong type or to a segment of memory that does not even hold a valid object at all. These types of errors are impossible in Java, which does not allow programmers to manipulate pointers directly. In other languages, it is possible to set a pointer to point, essentially, to any location in memory. If this is done incorrectly, then using the pointer can have unpredictable results.",
    "page93": "Another type of error that cannot occur in Java is a memory leak. In Java, once there are no longer any pointers that refer to an object, that object is \u201cgarbage collected\u201d so that the memory that it occupied can be reused.\u00ae In other languages, it is the programmer\u2019s responsibility to return unused memory to the system. If the \u00aeprogrammer fails to do th\u00aeis, unused memory can build up, leaving less memory for programs and data. There is a story that many common programs for older Windows computers had so many memory leaks that the computer would run out of memory after a few days of use and would have to be restarted. Many programs have been found to suffer from buffer overflow errors. Buffer overflow errors often make the news because they are responsible for many network security problems. When one computer receives data from another computer over a network, that data is stored in a buffer. The buffer is just a segment of memory that has been allocated by a program to hold data that it expects to receive. A buffer overflow occurs when more data is received than will fit in the buffer. The question is, what happens then? If the error is detected by the program or by the networking software, then the only thing that has happened is a failed network data transmission. The real problem occurs when the software does not properly detect buffer overflows. In that case, the software continues to store data in memory even after the buffer is filled, and the extra data goes into some part of memory that was not allocated by the program as part of the buffer. That memory might be in use for some other purpose. It might contain important data. It might even contain part of the program itself. This is where the real security issues come in. Suppose that a buffer overflow causes part of a program to be replaced with extra data received over a network. When the computer goes to execute the part of the program that was replaced, it\u2019s actually executing data that was received from another computer. That data could be anything. It could be a program that crashes the computer or takes it over. A malicious programmer who finds a convenient buffer overflow error in networking software can try to exploit that error to trick other computers into executing his programs.",
    "page94": "For so\u00aeftware writ\u00aeten completely in Java, buffer overflow errors are impossible. The language simply does not provide any way to store data into memory that has not been properly allocated. To do that, you would need a pointer that points to unallocated memory \u00aeor you would have to refer to an array location that lies outside the range allocated for the array. As explained above, neither of these is possible in Java. (However, there could conceivably still be errors in Java\u2019s standard classes, since some of the methods in these classes are actually written in the C programming language rather than in Java.) It\u2019s clear that language design can help prevent errors or detect them when they occur. Doing so involves restricting what a programmer is allowed to do. Or it requires tests, such as checking whether a pointer is null, that take some extra processing time. Some programmers feel that the sacrifice of power and efficiency is too high a price to pay for the extra security. In some applications, this is true. However, there are many situations where safety and security are primary considerations. Java is designed for such situations. Robust Handling of Input- One place where correctness and robustness are important and especially difficult is in the processing of input data, whether that data is typed in by the user, read from a file, or received over a network. Files and networking will be covered in Chapter 11, which will make essential use of material th\u00aeat will be covered in the next two sections o\u00aef this chapter. For now, let\u2019s look at an example of processing user input. Examples in this textbook use my TextIO class for reading input from the user. This class has built-in error handling. For example, the function TextIO.getDouble() is guaranteed to return a legal value of type double. If the user types an illegal value, then TextIO will ask the user to re-enter their response; your program never sees the illegal value. However, this approach can be clumsy and unsatisfactory, especially when the user is entering complex data. In the following example, I\u2019ll do my own error-checking. Sometimes, it\u2019s useful to be able to look ahead at what\u2019s coming up in the input without actually reading it. For example, a program might need to know whether the next item in the input is a number or a word. For this purpose, the TextIO class includes the function TextIO.peek(). This function returns a char which is the nex\u00aet character in the user\u2019s input, but it does not actually read tha\u00aet character. If the next thing in the input is an end-of-line, then TextIO.peek() returns the new-line character, \u2019 \u2019. Often, what we really need to know is the next non-blank character in the user\u2019s input. Before we can test this, we need to skip past any spaces (and tabs). Here is a function that does this. It uses TextIO.peek() to look ahead, and it reads characters until the next character in the input is either an end-of-line or some non-blank character.",
    "page95": "Exceptions and Exception Classes- We have already seen that \u00aeJava (like its cousin, C++) provides a neater, more structured alternative method for dealing with errors that can occur while a program is running. The method is referred to as exception handling. The word \u201cexception\u201d is meant to be more general than \u201cerror.\u201d It includes any circumstance that arises as the program is executed which is meant to be treated as an exception to the normal flow of control of the program. An exception might be an error, or it might just be a special case that you would rather not have clutter up your elegant algorithm. When an exception occurs during the execution of a program, we say that the exception is thrown. When this happens, the normal flow of the program is thrown off-track, and the program is in danger of crashing. However, the crash can be avoided if the exception is caught and handled in some way. An exception can be thrown in one part of a program and caught in a different part. An exception that is not caught will generally cause the program to crash. (More exactly, the thread that throws the exception will crash. In a multithreaded program, it is possible for other threads to continue even after one crashes. We will cover threads in Section 8.5. In particular, GUI pr\u00aeo\u00aegrams are multithreaded, and parts of the program might continue to function even while \u00aeother parts are non-functional because of exceptions.) By the way, since Java programs are executed by a Java interpreter, having a program crash simply means that it terminates abnormally and prematurely. It doesn\u2019t mean that the Java interpreter will crash. In effect, the interpreter catches any exceptions that are not caught by the program. The interpreter responds by terminating the program. In many other programming languages, a crashed program will sometimes crash the entire system and freeze the computer until it is restart\u00aeed. With Java, such system crashes should be impossible which means that when\u00ae they happen, you have the satisfaction of blaming the system rather than your own program. along with the trycatch statement, which is use\u00aed to catch and handle exceptions. However, that se\u00aection did not cover the complete syntax of trycatch or the full complexity of exceptions. In this section, we cover these topics in full detail.",
    "page96": "When an exception occurs, the thing that is actually \u201cthrown\u201d is an object. This object can carry information (in its instance variables) from the point where the exception occurs to the point where it is caught and handled. This information always includes the subroutine call stack, which is a list of the subroutines that were being executed when the exception was thrown. (Since one subroutine can call a\u00aenother, several subroutines can be active at the same time.) Typically, an exception object also includes an error message describing what happened to cause the exception, and it can contain other data as well. All exception objects must belong to a subclass of the standard class java.lang.Throwable. In general, each different type of exception is represented by its own subclass of Throwable, and these subclasses are arranged\u00ae in a fairly complex class hierarchy that shows the relationship among various types of exception. Throwable has two direct subclasses, Error and Exception. These two subclasses in turn have many other predefined subclasses. In addition, a programmer can create new exception classes to represent new types of exception. Most of the subclasses of the class Error represent serious errors within the Java virtual machine that should ordinarily cause program termination because there is no reasonable way to handle them. In general, you should not try to catch and handle such errors. An example is a ClassFormatError, which occurs when the Java virtual machine finds some kind of illegal data in a file that is supposed to contain a compiled Java class. If that class was being loaded as part of the program, then there is really no way for the program to proceed. On the other hand, subclasses of the \u00aeclass Exception represent exceptions that are meant to be caught. In many cases, these are exceptions that might naturally be called \u201cerrors,\u201d but they are errors in the program or in input dat\u00aea that a programmer can anticipate and possibly respond to in some reasonable way. (However, you should avoid the temptation of saying, \u201cWell, I\u2019ll just put a thing here to catch all the errors that might occur, so my program won\u2019t crash.\u201d If you don\u2019t have a reasonable way to respond to the error, it\u2019s best just to let the program crash, because trying to go on will probably only lead to worse things down the road in the worst case, a program that gives an incorrect answer without giving you any indication that the answer might be wrong!)",
    "page97": "Throwing Exceptions- There are times when it makes sense for a program to deliberately throw an exception. This is the case when the program discovers some sort of exceptional or error condition, but there is no reasonable way to handle the error at the point where the problem is discovered. The program can throw an exception in the hope that some other part of the program will catch and handle the exception. This can be done with a throw statement. You have already seen an example of this in Subsection 4.3.5. In this section, we cover the throw statement more fully. The syntax of the throw statement is: throw hexception-objecti ; The hexception-objecti must be an object belonging to one of the subclasses of Throwable. Usually, it will in fact belong to one of the subclasses of Exception. In most cases, it will be a newly constructed object created with the new operator. For example: \u00aethrow new ArithmeticException(\\\"Division by zero\\\"); The parameter in the constructor becomes the error message in the exception \u00aeobject; if e refers to the object, the error message can be retrieved by calling e.getMessage(). (You might find this example a bit odd, because you might expect the system itself to throw an ArithmeticException when an attempt is made to divide by zero. So why should a programmer bother to throw the exception? Recall that if the numbers that are being divided are of type int, then division by zero will indeed throw an ArithmeticException. However, no arithmetic operations with floating-point numbers will ever produce an exception. Instead, the special value Double.NaN is used to represent the result of an illegal operation. In some situations, you might prefer to throw an ArithmeticException when a real number is divided by zero.) An exception can be thrown either by the system or by a throw statement. The exception is processed in exactly the same way in either case. Suppose that the exception is thrown inside a try statement. If that try statement has a catch clause that handles that type of exception, then the computer jumps to the catch clause and executes it. The exception has been handled. After handling the\u00ae exception, the computer executes the finally clause of the try statement, if there is one. It then continues normal\u00aely with the rest of the program, which follows the try statement. If the exception is not immediately caught and handled, the processing of the exception will continue.",
    "page98": "When an exception is thrown during the execution of a subroutine and the exception is not handled in the same subroutine, then that subroutine is terminated (after the execution of any pending finally clauses). \u00aeThen the routine that called that subroutine gets a chance to handle the exception. That is, if the subroutine was\u00ae called inside a try statement that has an appropriate catch clause, then that catch clause will be executed and the program will continue on normally from there. Again, if the second routine does not handle the exception, then it also is terminated and the routine that called it (if any) gets the next shot at the exception. The exception will crash the program only if it passes up\u00ae through the entire chain of subroutine calls without being handled. (In fact, even this is not quite true: In a multithreaded program, only the thre\u00aead in which the exception occurred is terminated.) Mandato\u00aery Exception Handling In the preceding example, declaring that the subroutine root() can throw an IllegalArgumentException is just a courtesy to potential readers of this routine. This is because handling of IllegalArgumentExcept\u00aeions is not \u201cmandatory.\u201d A routine can throw an IllegalArgumentException without announcing the possibility. And a program that calls that routine is free either to catch or to ignore the exception, just as a p\u00aerogrammer can choose either to catch or to ignore an exception of type NullPointerException. For those exception classes \u00aethat require mandatory handling, the situation is different. If a subroutine can throw such an exception, that \u00aefact must be announced in a throws clause in the routine definition. Failing to do so is a syntax error that will be reported by the compiler. On the other hand, suppose that some statement in the body of a subroutine can generate an exception of a type that requires mandatory\u00ae handling. The statement could be a throw statement, which throws the exception directly, or it could be a call to a subroutine that can throw the exception. In either case, the exception must be handled. This can be done in one of two ways: The first way is to place the statement in a try statement that has a catch clause that handles the exception.",
    "page99": "in this case, the exception is handled within the subroutine, so that any caller of the subroutine wil\u00ael never see the exception. T\u00aehe second way is to declare that the subroutine can throw the exception. This is done by adding a \u201cthrows\u201d\u00ae clause to the subroutine heading, which alerts any callers to the possibility that an exception might be generated when the subroutine is executed. The caller will, in turn, be \u00aeforced either to handle the exception in a try statement or to declare the exception in a throws clause in its own header. Exception-handling is mandatory for any exception class that is not a subclass of either Error or RuntimeException. Exceptions that require mandatory handling generally represent conditions that are outside the control of the programmer. For example, they might represent bad input or an illegal action taken by the user. There is no way to avoid such errors, so a robust program has to be prepared to handle them. The design of Java makes it impossible for programmers to ignore the possibility of such errors. Among the exceptions that require mandatory handling are several that can occur when using Java\u2019s input/output routines. This means that you can\u2019t even use these routines unless you understand something about exception-handling. Chapter 11 deals with input/output and uses mandatory exception-handling extensively. Programming with Exceptions- Exceptions can be used to help write robust programs. They provide an organized and structured approach to robustness. Without exceptions, a program can become cluttered with if statements that test for various possible error conditions. With exceptions, it becomes possible to write a clean implementation of an algorithm that will handle all the normal cases. The exceptional cases can be handled elsewhere, in a catch clause of a try statement. When a program encounters an exceptional condition and has no w\u00aeay of handling it immediately, the program can throw an exception. In some cases, it makes \u00aesense to throw an exception belonging to one of Java\u2019s predefi\u00aened classes, such as Illeg\u00aealArgumentException or IOException. However, if there is no standard class that adequately represents the exceptional condition, the programmer can define a new exception class. The\u00ae new class must extend the standard class Throwable or one of its subclasses. In general, if the programmer does not want to require mandatory exception handling, the new class will extend RuntimeException (or one of its subclasses). To create a new exception class that does require mandatory handling, the programmer can extend one of the other subclasses of Exception or can extend Exception itself.",
    "page100": "Here, for example,\u00ae is a class that extends Exception, and therefore requires mandatory exception handling when it is used: public class ParseError extends Exception { public ParseError(String message) { // Create a ParseError object containing // the given message as its error message. super(message); } } The class contains only a constructor that makes it possible to create a ParseError obj\u00aeect containing a given error message. (The statement \u201csuper(message)\u201d \u00aecalls a constructor in the superclass, Exception. See Subsection 5.6.3.) Of course the class inherits the getMessage() and printStackTrace() routines from its superclass. If e refers to an object of type ParseError, then the function call e.getMessage() will retrieve the error message that was specified in the constructor. But the main point \u00aeof the ParseError class is simply to exist. When an ob\u00aeject of type ParseError is thrown, it indicates that a certain type of error has occurred. (Parsing, by the way, refers to figuring \u00aeout the syntax of a string. A ParseError would indicate, presumably, that some string that is being processed by the program does not have the expected form.) A throw statement can be used in a program to throw an error of type ParseError. The constructor for the ParseError object must specify an error message. The ability to throw exceptions is particularly useful in writing general-purpose subroutines and classes that are meant to be used in more than one program. In this case, the person writing the subroutine or class often has no reasonable way of handling the error, since that person has no way of knowing exactly how the subroutine or class will be used. In such circumstances, a novice programmer is often tempted to print an error message and forge ahead, but this is almost never satisfactory since it can lead to unpredictable results down the line. Printing an error message and terminating the program is almost as bad, since it gives the program no chance to handle the error. The program that calls the subroutine or uses the class needs to know that the error has occurred. In languages that do not support exceptions, \u00aethe only alternative is to return some special value or to set the value of some varia\u00aeble to indicate \u00aethat an error has occurred.",
    "page101": "Introduction to Threads- Like people, computers can multitask. That is, they can be working on several different tasks at the same time. A computer that has just a single central processing unit can\u2019t literally do two things at the same time, any more than a person can, but it can sti\u00aell switch its attention back and forth among several tasks. Furthermore, it is increasingly common for computers to have more than one pr\u00aeocessing unit, and such computers can literally work on several tasks simu\u00aeltaneously. It is likely that from now on, most o\u00aef the increase in computing power will come from adding additional \u00aeprocessors to computers rather than from increasing the speed of individual processors. To use the full power of these multiprocessing computers, a programmer must do parallel programming, which means writing a program as a set of several tasks that can be executed simultaneously\u00ae. Even on a single-processor computer, parallel programming techniques can be useful, since some problems can be tackled most naturally by breaking the solution into a set of simultaneous tasks that cooperate to solve the problem. In Java, a single task is called a thread. The term \u201cthread\u201d refers to a \u201cthread of control\u201d or \u201cthread of execution,\u201d meaning a sequence of instructions that are executed on\u00aee after another the thread extends through time, connecting each instruction to the next. In a multithreaded program, there can be many threads of control, weaving through time in parallel and forming the complete fabric of the program. (Ok, enough with the metaphor, already!) Every Java program has at least one thread; when the Java virtual machine runs your program, it creates a thread that is responsible for executing the main routine of the program. This main thread can in turn create other threads that can continue even after the main thread has terminated. In a GUI program, ther\u00aee is at least one additional thread, which is\u00ae responsible for handling events and drawing components on the screen. This GUI thread is created when the first window is opened. So in fact, you have already done parallel programming! When a main routine opens a win\u00aedow, both the main thread and the GUI thread can continue to run in parallel. Of course, parallel programming can be used in much more interesting ways.",
    "page102": "Unfortunately, parallel programming is even more difficult than ordinary, single-threaded programming. When several threads are working together on a problem, a whole new category of errors is possible. This just means that techniques for writing correct and robust programs are even more important for parallel programming than they are for normal programming. (That\u2019s one excuse for having this section in this chapter another is that we will need threads at several points in future chapters, and I didn\u2019t have another place in the book where the topic fits more naturally.) Since threads are a difficult topic, you will probably not fully understand everything in this section the first time through the material. Your understanding should improve as you encounter more examples of threads in future sections. Creating and Running Threads- In Java, a thread is represented by an object belonging to the cla\u00aess java.lang.Thread (or to a subclass of this class). The purpose of a Thread object is to execute a single method. The method is executed in its own thread of control, which can run in pa\u00aerallel with other threads. When the execution of the method is finished, either because the method terminates normally or because of an uncaught exception, the thread stops running. Once this happens, there is no way to restart the thread or to use the same Thread object to start another thread. Operations on Threads- The Thread class includes several useful methods in addition to the start() method that was discussed above. I will mention just a few of them. If thrd is an object of type Thread, then the boolean-valued function thrd.isAlive() can be used to test whether or not the thread is alive. A thread is \u201calive\u201d between the time it is started and the time when it terminates. After the thread has terminated it is said to be \u201cdead\u201d. (The rather gruesome metaphor is also u\u00aesed when we refer to \u201ckilling\u201d or \u201caborting\u201d a thread.)",
    "page103": "The static method Thread.sleep(milliseconds) causes the thread that executes this method to \u201csleep\u201d for the specified number of milliseconds. A sleeping thread is still alive, but it is not running. While a thread is sleeping, the computer will work on any other r\u00aeunnable threads (or on other program\u00aes). Thread.sleep() can be used to insert a pause in the executi\u00aeon of a thread. The sleep method can throw an exception of type InterruptedExcep\u00aetion, which is an exception class that requires mandatory exception handling. In practice, this means that the sleep method is usually used in a trycatch statement that catches the potential InterruptedException: try { Thread.sleep(lengthOfPause); } catch (InterruptedException e) { } One thread can interrupt another thread to wake it up when it is sleeping or paused for some other reason. A Thread, thrd, can be interrupted by calling its method thrd.interrupt(), but you are not likely to do t\u00aehis until you start writing rather advanced applications, and you are not likely to n\u00aeeed to do anything in response to an InterruptedException (except to catch it). It\u2019s unfortunate that you have to worry about it at all, but that\u2019s the way that mandatory exception handling works. Mutual Exclusion with \u201csynchronized\u201d Programming several threads to carry out independent tasks is easy. The real difficulty arises when threads have to interact in some way. One way that threads interact is by sharing resources. When two threads need access to the same resource, such as a variable or a window on the screen, some care must be taken that they don\u2019t try to use the same resource at the same time. Otherwise, the situation could be something like this: Imagine several cooks sharing the use of just one measuring cup, and imagine that Cook A fills the measuring cup with milk, only to have Cook B grab the cup before Cook A has a chance to empty the milk int\u00aeo his bowl. There has to be some way for Cook A to claim exclusive rights to the cup while he performs the two operations: Add-Milk-To-Cup and Empty-Cup-Into-Bowl.",
    "page104": "Wait and Notify- Threads can interact with each other in other ways besides sharing resources. For example, one thread might produce some sort of result that is needed by another thread. This imposes some restriction on the order in which the threads can do their computations. If the second thread gets to the point where it needs the result from the first thread, it might have to stop and wait for the result to be produced. S\u00aeince the second thread can\u2019t continue, it might as well go to sleep. But then there has to be some way to notify the second thread when the result is ready, so that it can wake up and continue its computation. Java, of course, has a way to do this kind of waiting and notification: It has wait() and notify() methods that are defined as instance methods in class Object and so can be used with any object. The reason why wait() and notify() should be associated with objects is not obvious, so don\u2019t worry about it at this point. It does, at least, make it possible to direct different notifications to a different recipients, depending on which object\u2019s noti\u00aefy() method is called. Volatile Variables- And a final note on communication among threads: In general, threads communicate by sharing variables and accessing those variables in syn\u00aechronized methods or synchronized statements. However, synchronization is fairly expensive co\u00aemputationally, and excessive use of it should be avoided. So in some cases, it can make sense for threads to refer to shared variables without synchronizing their access to those variables. However, a subtle problem arises when the value of a shared variable is set is one thread and used in another. Because of the way that threads are implemented in Java, the second thread might not see the changed value of the variable immediately. That \u00aeis, it is possible that a thread will continue to see the old value of the shared variable for some time afte\u00aer the value of the variable has been changed by another thread. This is because threads are allowed to cache shared data. That is, each thread can keep its own local copy of the shared data. When one thread changes the value of a shared variable, the local copies in the caches of other threads are not immediately changed, so the other threads continue to see the old value. When a synchronized method or s\u00aetatement is entered, threads are forced to update their caches to the most current values of the variables in the cache. So, using shared variables in synchronized code is always safe.",
    "page105": "It is still possible to use a shared variable outside of synchronized code, but in that case, the variable must be declared to be volatile. The volatile keyword is a modifier that can be added to a variable declaration, as in private volatile int count; If a variable is declared to be volatile, no thread will keep a local copy of that variable in its cache. Instead, the thread will always use the official, main copy of the variable. This means that any change made to the variable will immediately be available to all threads. This makes it safe for threads to refer to volatile shared variables even outside of synchronized code. (Remember, though, that synchronization is still the only way to prevent race conditions.) When the volatile modifier is applied to an object variable, only the variable itself is declared to be volatile, not the contents of the object that the variable points to. For this reason,\u00ae volatile is generally only used for variables of simple types such as primitive types and enumerated types.Analysis of Algorithms- This chapter has concentrated mostly on correctness of programs. In practice, another issue is also important: efficiency. When analyzing a program in terms of effi\u00aeciency, we want to look at questions such as, \u201cHow long does it take for the program to run?\u201d and \u201cIs there another approach that will get the answer more quickly?\u201d Efficiency will\u00ae always be less important than correctness; if you don\u2019t care whether a program works correctly, you can make it run very quickly indeed, but no one will think it\u2019s much of an achievement! On the other hand, a program that gives a correct answer after ten thousand years isn\u2019t very useful either, so efficiency is often an important issue.The term \u201cefficiency\u201d can refer to efficient use of almost any resource, including time,\u00ae computer memory, disk space, or network bandwidth. In this section, however, we will deal exclusively with time efficiency, and the major question that we want to ask about a program is, how long does it take to perform its task?",
    "page106": "it really makes little sense to classify an individual program as being \u201cefficient\u201d or \u201cinefficient.\u201d It makes more sense to compare two (correct) programs that perform the same task and ask which one of the two is \u201cmore efficient,\u201d that is, which one performs the task more quickly. However, even here there are difficulties. The running time of a program is not well-defined. The run time can be different depending on the number and speed of the processors in the computer on which it is run and, in the case of Java, on the design of the Java Virtual Ma\u00aechine which is used to interpret the program. It can depend on details of the compiler which is used to translate the program from high-level lang\u00aeuage to machine language. Furthermore, the run time of a program depends on the size of the problem which the program has to solve. It takes a sorting program longer to sort 10000 items than it takes it to sort 100 items. When the run times of two programs are compared, it often happens that Program A solves small problems faster than Program B, while Program B solves large problems faster than Program A, so that it is simply not the case that one program is faster than the other in all \u00aecases. the eff\u00aeiciency of programs. The field is known as Analysis of Algorithms. The focus is on algorithms, rather than on programs as such, to avoid having to deal with multiple implementations of the same algorithm written in different languages, compiled with different compilers, and running on different computers. Analysis of Algorithms is a mathematical field that abstracts away from these down-and-dirty details. Still, even though it is a theoretical field, every working programmer should be aware of some of its techniques and results. This section is a very brief introduction to some of those techniques and results. Because this is not a mathematics book, the treatment will \u00aebe rather informal.",
    "page107": "One of the main techniques of analysis \u00aeof algorithms is asymptotic analysis. The term \u201casymptotic\u201d here means basically \u201cthe tendency in the long run.\u201d An asymptotic analysis of an algorithm\u2019s run time looks at the question of how the run time depends on the size of the problem. The analysis is asymptotic because it only considers what happens to the run time as the size of the problem increases without limit; it is not concerned with what happens for problems of small size or, in fact, for problems of any fixed finite size. Only what happens in the long run, as the problem size increase\u00aes without limit, is important. Showing that Algorithm A is asymptotically faster than Algorithm B doesn\u2019t necessarily mean that Algorithm A will run faster than \u00aeAlgorithm B for problems of size 10 or size 1000 or even size 1000000 it only means that if you keep increasing the problem size, you will eventually come to a point where \u00aeAlgorithm A is faster than Algorithm B. An asymptotic analysis is only a first approximation, but in practice it often gives important and useful information. Central to asymptotic analysis is Big-Oh notation. Using this notation, we might say, for example, that an algorithm has a running time that is O(n2 ) or O(n) or O(log(n)). These notations are read \u201cBig-Oh of n squared,\u201d \u201cBig-Oh of n,\u201d and \u201cBig-Oh of log n\u201d (where log is a logarithm function). More generally, we can refer to O(f(n)) (\u201cBig-Oh of f of n\u201d), where f(n) is some function that assigns a positive real number to every positive intege\u00aer n. The \u201cn\u201d in this notation refers to the size of the problem. Before you can even begin an asymptotic analysis, you need some way to measure problem size. Usually, this is not a big issue. For example, if the problem is to sort a list of items, then the problem size can be taken to be the number of items in the list. When the input to an algorithm is a\u00aen integer, as in the case of algorithm that checks whether a given positive integer is prime, the usual measure of the size of a problem is the number of bits in the input integer rather than the integer itself. More generally, the number of bits in the input to a problem is often a good measure of the size of the problem.",
    "page108": "To say that the running time of an algorithm is O(f(n)) means that for large values of the problem size, n, the running time of the algorithm is no bigger than some constant times f(n). (More rigorously, there is a number C and a positive int\u00aeeger M such that whenever n is greater than M, the run time is less than or equal to C*f(n).) The constant takes into account details such as the speed of the computer on which the algorithm is run; if you use a slower computer, you might have to use a bigger constant in the formula, but changing the constant won\u2019t change the basic fact that the run time is O(f(n)). The constant also makes it unnecessary to say whether we are measuring time in seconds, years, CPU cycles, or any other unit of measure; a change from one unit of measure to another is just multiplication by a constant. Note also that O(f(n)) doesn\u2019t depend at all on what happens for small problem sizes, only on what happens in the long run as the problem size increases without limit. To look at a simple example, consider the problem of adding up all the numbers in an array. The problem size, n, is the length of the array. Using A as the name of the array, the algorithm can be expressed in Java as: total = 0; for (int i = 0; i < n; i++) total = total + A[i]; This algorithm performs the same operation, total = total + A[i], n times. The total time spent on this operation is a*n,\u00ae where a is the time it takes to perform the operation once. time spent on this operation is a*n, where a is the time it takes to perform the operation once. Now, this is not the only thing that is done in the alg\u00aeorithm. The value of i is incremented and is compared to n each time through the loop. This adds an additional time of b*n to the run time, for some constant b. Furthermore, i and total both have to be initialized to zero; this adds some constant amount c to the running time. The exact running time would then be (a+b)*n+c, where the constants a, b, and c depend on factors such as how the code is compiled and what computer it is run on.",
    "page109": "Using the fact that c is less than or equal to c*n for any positive integer n, we can say that the run time is less than or equal to (a+b+c)*n. That is, the run time is less than or equal to a constant times n. By definition, this means that the run time for this algorithm is O(n). If this explanation is too mathematical for you, we can just note that for large values of n, the c in the formula (a+b)*n+c is insignificant compared to the other term, (a+b)*n. We say that c is a \u201clower order term.\u201d When doing a\u00aesymptotic analysis, lower order terms can be discarded. A rough, but correct, asymptotic analysis of the algorithm would go something like this: Each iteration of the for loop takes a certain con\u00aestant amount of time. There are n iterations of the loop, so the t\u00aeotal run time is a constant times n, plus lower order terms (to account for the initialization). Disregarding lower order terms, we see that the run time is O(n). Note that to say that an algorithm has run time O(f(n)) is to say that its run time is no bigger than some cons\u00aetant times n (for large values of n). O(f(n)) puts an upper limit on the run time. However, the run time could be smaller, even much smaller. For example, if the run time is O(n), it would also be correct to say that the run time is O(n2 ) or even O(n10). If the run time is less than a constant times n, then it is certainly less than the same constant times n\u00ae 2 or n10 Of course, sometimes it\u2019s useful to have a lower limit on the run time. That is, we want to be able to say that the run time is greater than or equal to some constant times f(n) (for large values of n). The notation for this is \u2126(f(n)), read \u201cOme\u00aega of f of n.\u201d \u201cOmega\u201d is the name of a letter in the Greek alphabet, and \u2126 is the upper case version of that letter. (To be technical, saying that the r\u00aeun time \u00aeof an algorithm is \u2126(f(n)) means that there is a positive number C and a positive integer M such that whenever n is greater than M, the run time is greater than or equal to C*f(n).) O(f(n)) tells you something about the maximum amount of time tha\u00aet you might have to wait for an algorithm to finish; \u2126(f(n)) tells you something about the minimum time.",
    "page110": "The algorithm for adding up the numbers in an array has a run time that is \u2126(n) as well as O(n). \u00aeWhen an algorithm has a run time that is both \u2126(f(n)) and O(f(n)), its run time is said to be \u0398(f(n)), read \u201cTheta of f of n.\u201d (Theta\u00ae is another letter from the Greek alphabet.) To say that the run time of an algorithm is \u0398(f(n)) means that for large values of n, the run time is between a*f(n) and b\u00ae*f(n), where a and b are constants (with b greater than a, and both greater than 0).So far, my analysis has ignored an im\u00aeportant detail. We have looked at how run time depends on the problem size, but in fact the run time usually depends not just on the size of the problem but on the specific data that has to be processed. For example, the run time of a sorting algorithm can depend on the initial order of the items that are to be sorted, and not just on the number of items. To account for this dependency, we can consider either the worst case run time analysis or the average case run time analysis of an algorithm. For a worst case run time analysis, we consider all possible problems of size n and look at the longest possible run time for all such problems. For an average case analysis, we consider all possible problems of size n and look at the average of the run times for all such problems. Usually, the average case analysis assumes that all problems of size n are equally likely\u00ae to be encountered, although this is not always realistic or even possible in the case where there is an infinite number of di\u00aefferent problems of a given \u00aesize. In many cases, the average and the worst case run times are the same to within a constant multiple. This means that as far as asymptotic analysis is concerned, they are the same. That is, if the average case run time is O(f(n)) or \u0398(f(n)), then so is the worst case. However, later in the book, we will encounter a few cases where the average\u00ae and worst case asymptotic analyses differ.",
    "page111": "So, what do you really have to know about analysis of algorithms to read the rest of this book? We will not do any rigorous mathematical analysis, but you should be able to follow informal discussion of simple cases such as the examples that we have looked at in this sect\u00aeion. Most important, though, you should have a feeling for exactly what it means to say that the running time of an algorithm\u00ae is O(f(n)) or \u0398(f(n)) for some common functions f(n). The main point is that these notations do not tell you anything about the actual numerical value of the running time of the algorithm for any particular case. They do n\u00aeot tell you anything at all about the running time fo\u00aer small values of n. What they do tell you is something about the rate of growth of the running time as the size of the problem increases. Suppose you compare two algorithm that solve the same problem. The run time of one algorithm is \u0398(n2), while the run time of the second algorithm is \u0398(n3). What does this tell you? If you want to know which algorithm will be faster for some particular problem of size, say, 100, nothing is certain. As far as you can tell just from the asymptotic analysis, either algorithm could be faster for that particular case or in any particular case. But what you can say for sure is that if you look at larger and larger problems, you will come to a point where the \u0398(\u00aen2) algorithm is faster than the \u0398(n3) algorithm. Furthermore, as you continue to increase the problem size, the relative advantage of the \u0398(n2) algorithm will continue to grow. There will be values of n for which the \u0398(n2) algorithm is a thousand times faster, a million times faster, a billion times faster, and so on. This is because for any positive constants a and b, the function a*n3 grows faster than the function b*n2 as n gets larger. (Mathematically, the limit of the ratio of a*n3 to b*n2 is infinite as n approaches infinity.) This means that for \u201clarge\u201d problems, a \u0398(n2) algorithm will definitely be faster than a \u0398(n3) algorithm. You just don\u2019t know based on the asymptotic analysis alone exactly how large \u201clarge\u201d has to be. In practice, in fact, it is likely that the \u0398(n2) algorithm will be faster even for fairly small values of n, and absent other information you would generally prefer a \u0398(n2) algorithm to a \u0398(n3) algorithm.",
    "page112": "Recursion- At one time or another, you\u2019ve probably been told that you can\u2019t define something in terms of itself. Nevertheless, if it\u2019s done right, defining something at least partially in terms of itself can be a very powerful te\u00aechnique. A recursive definition is one that uses the concept or thing that is being defined as part of the definition. For example: An \u201cancestor\u201d is either a parent or an ancestor of a parent. A \u201csentence\u201d can be, among other things, two sentences joined by a conjunction such as \u201cand.\u201d A \u201cdirectory\u201d is a part of a disk drive that can hold files and directories. In mathematics, a \u201cset\u201d is a collection of elements, which can themselves be sets. A \u201cstatement\u201d in Java can be a while statement, which is made up of\u00ae the word \u201cwhile\u201d, a boolean-valued condition, and a stat\u00aeement. Recursive definitions c\u00aean describe very complex situations with just a few words. A definition of the term \u201cancestor\u201d without using recursion might go something like \u201ca parent, or \u201cand so on\u201d is not very rigorous. (I\u2019ve often thought that recursion is really just a rigorou\u00aes way of saying \u201cand so on.\u201d) You run into the same problem if you try to define a \u201cdirectory\u201d as \u201ca file that is a list of files, where s\u00aeome of the files can be lists of files, where some of those files can be lists of files, and so on.\u201d Trying to describe what a Java statement can look like, without using recursion in the definition, would be difficult and probably pretty comical. Recursion can be used as a programming technique. A recursive subroutine is one that calls itself, either dire\u00aectly or indirectly. To say that a subroutine calls itself directly means that its definition contains a subroutine call statement that calls the subroutine that is being defined. To say that a subroutine calls itself indirectly means that it calls a second subroutine which in turn calls the first subroutine (either directly or indirectly). A recursive subroutine can define a complex task \u00aein just a few lines of code. In the rest of this section, we\u2019ll look at a variety of examples, and we\u2019ll see other examples in the rest of the book.",
    "page113": "Recursive Binary Search- Binary search is used to find a specified value in a sorted list of items (or, if it does not occur in the list, to determine that fact). The idea is to test the element in the middle of the list. If that element is equal to the specified value, you are done. If the specified value is less than the middle element of the list, then you should search for the value in the first half of the list. Otherwise, you should search for the value in the second half of the list. The method used to search for the v\u00aealue in the first or second half of the list is binary search. That is, you look at the mid\u00aedle element in the half of the list that is still under consideration, and either you\u2019ve found the value you are looking for, or you have to apply binary search to one half of the remaining elements. And so on! This is a recursive description, and we can write a recursive subroutine to implement it. Before we can do that, though, there are two considerations that we need to take into account. Each of these illustrates an important general fact about recursive subroutines. First of all, the binary search algorithm begins by looking at the \u201cmiddle element of the list.\u201d But what if the list is empty? If there are no elements in the list, then it is impossible to look at the middle element. In the terminology of Subsection 8.2.1, having a non-empty list is a \u201cprecondition\u201d for looking at the middle element, and this is a clue that we have to modify the algorithm to take this precondition into account. What should we do if we find ourselves searching for a specified value in an empty list? The answer is easy: If the list is empty, we can be sure that the value does not occur in the list, so we can give the answer without any further work. An empty list is a base case for the binary search algorithm. A base case for a recursive algorithm is a case that is handled directly, rather than by applying the algorithm recursively. The binary search algorithm actually has another type of base case: If we find the element we are looking for in the middle of the list, we are done. There is no need for further recursion. The second consideration has to do with the parameters to the subroutine. The problem is phrased in terms of searching for a value in a list. In the original, non-recursive binary search subroutine, the list was given as an array. However, in the recursive approach, we have to able to ap\u00aeply the subroutine recursively to just a part of the original list. Where the original subroutine was designed to search an entire array, the recursive subroutine must be able to search part of an array. The parameters to the subroutine must tell it what part of the array to search. This illustrates a general fact that in order to solve a problem recursively, it is often necessary to generalize the problem slightly.",
    "page114": "Linked Data Structures- Every useful object contains instance variables. When the type of an instance variable is given by a class or interface name, the variable can hold a reference to another object. Such a reference is also called a pointer, and we say that the variable points to the object. (Of course, any variable that can contain a reference to an object can also contain the special value null, which points to nowhere.) When one object contains an instance variable that points to another object, we think of the objects as being \u201clinked\u201d by the pointer. Data structures of great complexity can be constructed by linking objects together. Recursive Linking- Something interesting happens when an object contains an instance variable that can refer to another object of the same type. In that case, the definition of the object\u2019s class is recursive. Such recursion arises naturally in many cases. For ex\u00aeample, consider a class designed to represent employees at a company. Suppose that every employee except the boss has a supervisor, who is another employee of the company. As the while loop is executed, runner points in turn t\u00aeo the original employee, emp, then variable is incremente\u00aed each time runner \u201cvisits\u201d a new employee. The loop ends when ru\u00aenner.supervisor is null, which indica\u00aetes that runner has reached the boss. At that point, count has counted the number of steps between emp and the boss. In this example, the supervisor variable is quite natural and useful. In fact, data structures that are built by linking objects together are so useful that they are a major topic of study in computer science. We\u2019ll be looking at a few typical examples. In this section and the next, we\u2019ll be looking at linked lists. A linked list consists of \u00aea chain of objects of the same type, link\u00aeed together by pointers from one object to the next. This is much like the chain of supervisors between emp and the boss in the above example. It\u2019s also possible to have more complex situations, in which one object can contain links to several other objects.",
    "page115": "Linked \u00aeLists- For most of the examples in the rest of this section, linked lists will be constructed out of objects belonging to the class Node which is defined as follows: class Node { String item; Node next; } The term node is often used t\u00aeo refer to one of the objects in a linked data structure. Objects of type Node can be chained together as shown in the top part of the above picture. Each node holds a String and a pointer to the next node in the list (if any). The last node in such a list can always be \u00aeidentified by the fact that the instance variable next in the last node holds the value null instead of a pointer to another node. The purpose of the chain of nodes is to represent a list of strings. The first string in the list is stored in the first node, the second string is stored in the second node, and so on. The pointers and the node objects are use\u00aed to build the structure, but the data that we are interested in representing is the list of strings. Of course, we could just as easily represent a list of integers or a list of JButtons or a list of any other type of data by changing the type of the item that is stored in each node. Although the Nodes in this example are very simple, we can use them to illustrate the common operations on linked lists. Typical operations include deleting nodes from the list, inserting new nodes into the list, and search\u00aeing for a specified String among \u00aethe items in the list. We will look at subroutines to perform all of these operations, among others. For a linked list to be used in a program, that program needs a variable that refers to the first node in the list. It only needs a pointer t\u00aeo the first node since all the other nodes in the list can be accessed by starting at the first node and following links along the list from one node to the next. In my examples, I will always use a variable named head, of type Node, that points to \u00aeth\u00aee first node in the linked list. When the list is empty, the value of head is n\u00aeull.",
    "page116": "Stacks, Queues, and ADTs- A linked list is a particular type of data structure, made up of objects linked together by pointers. In the previous section, we used a linked list to store an ordered list of Strings, and we implemented insert, delete, and find operations on that list. However, we could easily have stor\u00aeed the list of Strings in an array or ArrayList, instead of in a linked list. We could still have implemented the same operations on the list. The implementations of these operations would have been different, but their interfaces and logical behavior would still be the same. The term abstract data type, or ADT, refers to a set of possible values and a set of operations on those values, without any specification of how the values are to be represented o\u00aer how the operations are to be implemented. An \u201cordered list of strings\u201d can be defined as an abstract data type. Any sequence of Strings that is arranged in increasing order is a possible value of this data type. The operations on the data type include inserting a new string, deleting a string, and finding a string in the list. There are often several different ways to implement the same abstract data type. For example, the \u201cordered list of strings\u201d ADT can be implemented as a linked list or as an array. A program that only depends on the abstract definition of the ADT can use either implementation, interchangeably. In particular, the implementation of the ADT can be changed without affecting the program as a whole. This can make the program easier to debug \u00aeand maintain, so ADTs are an important tool in software engineering. In this section, we\u2019ll look at two common abstract data types, stacks and queues. Both stacks and queues are often implemented as linked lists, but that is not the only possible\u00ae implementation. You should think of the rest of this section partly as a discussion of stacks and queues and partly as a case study in ADTs.",
    "page117": "Stacks- A stack consists of a sequence of items, which should be thought of as piled one on top of the other lik\u00aee a physical stack of boxes or cafeteria trays. Only the top item on the stack is accessible at any given time. It can be removed from the stack with an operation called pop. An item lower down on the stack can only be rem\u00aeoved after all the items on top of it have been popped off the stack. A new item can be added to the top of the stack with an operation called push. We can make a stack of any type of items. If, for example, the items are values of type int, then the push and pop operations can be implemented as instance methods void push (int newItem) Add newItem to top of stack. int pop() Remove the top int from the stack and return it. It is an error to try to pop an item from an empty stack, so it is important to be able to tell whether a stack is empty. We need another stack operation to do the test, implemented as an instance method boolean isEmpty() Returns true if the stack is empty. To get a better handle on the differen\u00aece between stacks and queues, consider the sample program DepthBreadth.java. I suggest that you run the program or try the applet version that can be found in the \u00aeon-line version of this section. The program shows a grid of squares. Initially, all the squares are white. When you click on a white square, the program will gradually mark all the squares in the grid, starting from the one where yo\u00aeu click. To understand how the program does this, think of yourself in the place of the progra\u00aem. When the user clicks a square, you are handed an index card. The location of the square its row and column is written on the card. You put the card in a pile, which then contains just that one card. Then, you repeat the following: If the pile is empty, you\u00ae are done. Otherwise, take an index card from the pile. The index card specifies a square. Look at each horizontal and vertical neighbor of that square. If the neighbor has not already been encountered, write its location on a new index card and put the card in the pile.",
    "page118": "While a square \u00aeis in the pile, waiting to be processed, it is colored red; that is, red squares have been encountered but not yet processed. When a square is taken from the pile and processed, its color changes to gray. Once a square has been colored gray, its color won\u2019t change again. Eventually, all the squares have been processed, and the procedure ends. In the index card analogy, the pile of cards has been emptied. The program can use your choice of three methods: Stack, Queue, and Random. In each case, the same general procedure is used. The only difference is how the \u201cpile\u00ae of index cards\u201d is managed. For a stack, cards are added and removed at the top of the pile. For a queue, cards are added to the bottom of the pile and removed from the top. In the random case, the card to be processed is picked at random from among all the cards in the pile. The order of processing is very different \u00aein these three cases. You should experiment with the program to see how it \u00aeall works. Try to underst\u00aeand how stacks and queues are being used. Try starting from one of the corner squares. While t\u00aehe process is going on, you can click on other white squares, and they will be added to the pile. When you do this with a stack, you should notice that the square you click is processed immediately, and all the red squares that were already waiting for processing have to wait. On the other hand, if you do this with a queue, the square that you click will wait its turn until all the squares that were already in the pile have been processed. Queues seem very natural because they occur so often in real life, but there are times when stacks are appropriate and even essential. For example, consider what happens wh\u00aeen a routine calls a subrou\u00aetine. The first routine is suspended while the subroutine is executed, and it will continue only when the subroutine returns. Now, suppose that the subroutine calls a second subroutine, and the second subroutine calls a third, and so on. Each subroutine is suspended while the subsequent subroutines are executed. The computer has to keep track of all the subroutines that are suspended. It does this with a stack.",
    "page119": "Wh\u00aeen a subroutine is called, an activation record is created for that subroutine. The activation record contains information relevant to the execution of the subroutine, such as its local variables and parameters. The activation record for the subroutine is placed\u00ae on a stack. It will be removed from the stack and destroyed when the subroutine returns. If the subroutine calls another subroutine, the activation record of the second subroutine is pushed onto the stack, on top of the activation record of the fi\u00aerst subroutine. The stack can continue to grow as more subroutines are called, and it shrinks as those subroutines return. Postfix Expressions As another example, stacks can be used to evaluate postfix expressions. An ordinary mathematical expression such as 2+(15-12)*17 is called an infix expression. In an infix expression, an operator comes in between its two operands, as in \u201c2 + 2\u201d. In a postfix expression, an operator comes after its two operands, as in \u201c2 2 +\u201d. The infix expression \u201c2+(15-12)*17\u201d would be written in postfix form as \u201c2 15 12 - 17 * +\u201d. The \u201c-\u201d operator in this expression applies to the two operands that precede it, namely \u201c15\u201d and \u201c12\u201d. The \u201c*\u201d operator applies to the two operands that precede it, namely \u201c15 12 -\u201d and \u201c17\u201d. And the \u201c+\u201d operator applies to \u201c2\u201d and \u201c15 12 - 17 *\u201d. These are the same computations that \u00aeare done in the original infix expression. Now, suppose that we want to process the expression \u201c2 15 12 - 17 * +\u201d, from left to right and find its value. The first item we encounter is the 2, but what can we do with it? At this point, we don\u2019t kno\u00aew what \u00aeoperator, if any, will be applied to the 2 or what the other operand might be. We have to remember the 2 for later processing. We do this by pushing it onto a stack. M\u00aeoving on to the next item, we see a 15, which is pushed onto the stack on top of the 2. Then the 12 is added to the stack. Now, we come to the operator, \u201c-\u201d. This operation applies to the two operands that preceded it in the expression. We have saved those two operands on the stack. So, to process the \u201c-\u201d operator, we pop two numbers from the stack, 12 and 15, and compute 15 - 12 to get the answer 3. This 3 must be remembered to be used in later processing, so we push it onto the stack, on top of the 2 that is still waiting there The next item in the expression is a 17, which is processed by pushing it onto the stack, on top of the 3. To process the next item, \u201c*\u201d, we pop two numbers from the stack. The numbers are 17 and the 3 that represents the value of \u201c15 12 -\u201d.",
    "page120": "These numbers are multiplied, and the result, 51 is pushed onto the stack. The next item in the expression is a \u201c+\u201d operator, which is processed by popping 51 and 2 from the stack, adding them, and pushing the result, 53, onto the stack. Finally, we\u2019ve come to the end of the expression. The number on the stack is the value of the entire expression, so all we have to do is pop the answer from the stack, and we are done! The value of \u00aethe expression is 53. Tree Traversal- Consider any node in a binary tree. Look at that node together with all its descendents (that is, its children, the children of its children, and so on). This set of nodes forms a binary tree, which is called a subtree of the original tree. For example, in the picture, nodes 2, 4, and 5 form a subtree. This subtree is called the left subtree of the root. Similarly, nodes 3 and 6 make up the right subtree of the root. We can consider any non-empty binary tree to be made up of a root node, a left subtree, and a right subtree. Either or both of the subtrees can be empty. This is a recursive definition, matching the recursive definition of\u00ae the TreeNode class. So it should not be a su\u00aerprise that recursive subroutines are often used to process trees. Consider the problem of counting the nodes in a binary tree. (As an exercise, you might try to come up\u00ae with a non-recursive algorithm to do the counting, but you shouldn\u2019t expect to find one.) The heart of problem is keeping track of which nodes remain to be counted. It\u2019s not so easy to do this, and in fact it\u2019s not even possible without an auxiliary data structure such as a stack or queue. With recursion, however, the algorithm is almost trivial. Either the tree is empty or it consists of a root and two subtrees. If the tree is empty, the number of nodes is zero. (This is the base case of the recursion.) \u00aeOtherwise, use recursion to count the nodes in each subtree. Add the results from the subtrees together, and add one to count the root.",
    "page121": "Binary Sort Trees- One of the examples in Section 9.2 was a linked list of strings, in which the strings were kept in increasing order. While a linked list works well for a small number of strings, i\u00aet becomes inefficient for a large number of items. When inserting an item into the list, searching for that item\u2019s position requires looking at, on average, half the items in the list. Finding an item in the list requires a similar amount of time. If the strings are stored in a sorted array instead of in a linked list, then searching becomes more efficient because binary search can be used. However, inserting a new item into the array is still inefficient since it means moving, on average, half of the items in the array to make a space for the new item. A binary tree can be used to\u00ae store an ordered list of strings, or other items, in a way that makes both searching and insertion efficient. A binary tree used in this way is called a binary sort tree. A binary sort tree is a binary tree with the following property: For every node in the tree, the item in that node is greater than every item in the left subtree of that node, and it is less than or eq\u00aeual to all the items in the right subtree of th\u00aeat node. Here for example is a binary sort tree containing items of type String. (In this picture, I haven\u2019t bothered to draw all the pointer variables. Non-null pointers are shown as arrows.)",
    "page122": "Binary sort trees have this useful property: An inorder traversal of the tree will process the items in increasing order. In fact, this is really just another way of expressing the definition. For example, if an inorder traversal is used to print the items in the tree shown above, then the items will be in alphabetical order. The definition of an inorder traversal guarantees that all the items in the left subtree of \u201cjudy\u201d are printed before \u201cjudy\u201d, and all the items in the right subtree of \u201cjudy\u201d are printed after \u201cjudy\u201d. But the binary sort tree property guarantees that the items in the left subtree of \u201cjudy\u201d are precisely those that precede \u201cjudy\u201d in alphabetical order, and all the items in the right subtree follow \u201cjudy\u201d in alphabetical order. So, we know that \u201cjudy\u201d is output in its proper alphabetical position. But the same argument applies to the subtrees. \u201cBill\u201d will be output after \u201calice\u201d and before \u201cfred\u201d and its descendents. \u201cFred\u201d will be output after \u201cdave\u201d and before \u201cjane\u201d and \u201cjoe\u201d. And so on. Suppose that we want to search for a given item in a binary search tree. Compare that item to the root item of the tree. If they are equal, we\u2019re done. If the item we are looking for is less t\u00aehan the root item, then we need to\u00ae search the left subtree of t\u00aehe root the right subtree can be eliminated because it only contains items that are greater than or equal to the root. Similarly, if the item we are looking for is greater than the item in the root, then we only need to look in the right subtree. In either case, the same procedure can then be applied to search the subtree. Inserting a new item is similar: Start by searchin\u00aeg the tree for the position where the new item belongs. When that position is found, create a new node and attach it to the tree at that position.",
    "page123": "Searching and inserting are efficient operations on a binary search tree, provided that the tree is close to being balanced. A binary tree is balanced if for each node, the left subtree of that node contains approximately the same number of nodes as the right subtree. In a perfectly balanced tree, the two numbers differ by at most one. Not all binary trees are balanced, but if the tree is created by inserting items in a random order, there is a high probability that the tree is approximately balanced. (If the order of insertion is not random, however, it\u2019s quite possible for the tree to be very unbalanced.) During a search of any binary sort tree, every comparison eliminates one of two subtrees from further consideration. If the tree is balanced, that means cutting the number of items still under consideration in half. This is exactly the same as the binary search algorithm, and the result, is a similarly efficient algorithm. In terms of asymptotic analysis, searching, inserting, and deleting in a binary search tree have average case run time \u0398(log(n)). The problem size, n, is the number of items in the tree, and the average is taken over all the different orders in which the items could have been \u00aeinserted into the tree. As long the actual insertion order is random, the actual run time ca\u00aen be expected to be close to the average. However, the worst case run time for binary search tree operations is \u0398(n), which is much worse than \u0398(log(n)). The worst case occurs for certain particular insertion orders. For example, if the items are inserted into the tree in order of increasing size, then every item that is inserted moves always to the right as it moves down the tree. The result is a \u201ctree\u201d that looks more like a linked list, since it consists of a linear string of nodes strung together by their right child pointers. Operations on such a tree have the same performance as operations on a linked list. Now, there are data structures that are similar to simple binary sort trees, except that insertion and deletion of nodes are implemented in a way that will always keep the tree balanced, or almost balanced. For these data structures, searching, inserting, and deleting have both average case and worst case run times that are \u0398(log(n)). Here, however, we will look at only the simple versions of inserting and searching. The sample program SortTreeDemo.java is a demonstration of bin\u00aeary sort trees. The program includes subroutines that implement inorder traversal, searching, and insertion. We\u2019ll look at the latter two subroutines below. The main() routine tests the subroutines by letting you type in strings to be inserted into the tree.",
    "page124": "Backus-Naur Form- Natural and artificial languages are similar in that they have a structure known as grammar or syntax. Syntax can be expressed by a set of rules that describe what it means to be a legal sentence or program. For programming languages, syntax rules are often expressed in BNF (Backus-Naur Form), a system that was developed by computer scientists John Backus and P\u00aeeter Naur in the late 1950s. Interestingly, an equivalent system was developed independentl\u00aey at about the same time by linguist Noam Chomsky to describe the grammar of natural language. BNF cannot express all possible syntax rules. For example, it can\u2019t express the fact that a variable must be defined before it is used. Furthermore, it says nothing about the meaning or semantics of the langauge. The pr\u00aeoblem of specifying the semantics of a language even of an artificial programming langauge is one that is still far from being completely solved. However, BNF does express the basic structure of the language, and it plays a central role in the design of translation programs. Files- The data and programs in a computer\u2019s main memory survive only as long as the power is on. For more permanent storage, computers use files, which are collections of data stored on a hard disk, on a USB memory stick, on a CD-ROM, or on some other type of storage device. Files are organized into directories (sometimes called folders). A directory can hold \u00aeother directories, as well as files. Both directories and files have names that are used to identify them. Programs can read data from existing files. They can create new files and can write data to files. In Java, such input and output can be done using streams. Human-readable character data is read from a file using an object belonging to the class FileReader, which is a subclass of Reader. Similarly, data is written to a file in human-readabl\u00aee format through an object of type FileWriter, a subclass o\u00aef Writer. For files that store data in machine format, the appropriate I/O classes are FileInputStream and FileOutputStream. In this section, I will only discuss character oriented file I/O using the FileReader and FileWriter classes. However, FileInputStream and FileOutputStream are used in an exactly parallel fashion. All these classes are defined in the java.io package.",
    "page125": "Word Counting- The final example in this section also deals with storing information about words. The problem here is to make a list of all the words that occur in a file, along with the number of times that each word occurs. The file will be selected by the user. The output of the program will consist of two lists. Each list contains all the words from the file, along with the number of times that the word occurred. One list is sorted alphabetically, and the other is sor\u00aeted according to the number of occurrences, with the most common words at the top and the least common at the bottom. The problem here is a generalization, which asked you to make an alphabetical list of all the words in a file, without counting the number of occurrences. Symbol Tables- We begin with a straightforward but important application of maps. When a compiler reads the source code of a program, it encounters definitions of variables, subroutines, and classes. The names of these things can be used later in the progra\u00aem. The compiler has to remember the name is encountered later in the program. This is a natural application for a Map. The name can be used as a key\u00ae in the map. The value associated to the key is the definition of the name, encoded somehow as an object. A map that is used in this way is called a symbol table. In a compiler, the values in a symbol table can be quite complicated, since the compiler has to deal with names for various sorts of things, and it needs a different type of information for each different type of name. We will keep things simple by looking at a symbol table in another context. Suppose that we want a program that can evaluate expressions entered by the user, and suppose that the expressions can contain variables, in addition to operators, numbers, and parentheses. For this to make sense, we need some way of assigning values to variables. When a variable is used in an expre\u00aession, we need to retrieve the variable\u2019s value. A symbol table can be used to store the data that we need. The keys for the symbol table are va\u00aeriable names. The value associated with a key is the value of that variable, which is of type double.",
    "page126": "Almost all applications require persistent data. Persistence is one of the fundamental concepts in application development. If an information system didn\u2019t preserve data when it was powered off, the system would be of little practical use. Object persistence means individual objects can outlive the application process; they can be saved to a data store and be re-created at a later point in time. When we talk about persistence in Java, we\u2019re normally talking about mapping and storing object instances in a database using SQL. We start by taking a brief look at the technology and how it\u2019s used in Java. Armed with this information, we then continue our discussion of persistence and how it\u2019s implemented in object-oriented applications.  You, like most other software engineers, have probably worked with SQL and relational databases; many of us handle such systems every d\u00aeay. Relational database management systems have SQL-based\u00ae application programming interfaces; hence, we call today\u2019s relational database products SQL database management systems (DBMS) or, when we\u2019re talking about particular systems, SQL databases.  Relational technology is a known quantity, and this alone is sufficient reason for many organizations to choose it. But to say only this is to pay less respec\u00aet than is due. Relational databases are entrenched because they\u2019re an incredibly flex\u00aeible and robust approach to data management. Due to the well-researched theoretical foundation of the relational data model, relational databases can guarantee and protect the integrity of the stored data, among other desirable characteristics. You may be familiar with E.F. Codd\u2019s four-decades-o\u00aeld introduction of the relational model, A Relational Model of Data for Large Shared Data Banks (Codd, 1970). A more recent compendium worth reading, with a focus on SQL, is C. J. Date\u2019s \u00aeSQL and Relational Theory (Date, 2009).   \u00aeRelational DBMSs aren\u2019t specific to Java, nor is an SQL database specific to a particular application. This important principle is known as data independence. In other words, and we can\u2019t stress this important fact enough, data live\u00aes longer than any application does. Relational technology provides a way of sharing data among\u00ae different applications, or among different parts of the same overall system (the data entry application and the reporting application, for example). Relational technology is a common denominator of many disparate systems and technology platforms. Hence\u00ae, the relational data model is often the foundation for the common enterprise-wide representation of business entities.",
    "page127": "Before we go into more detail about the practical aspects of SQL databases, we have to mention an important issue: although marketed as relation\u00aeal, a database system pr\u00aeoviding only an SQL data language interface isn\u2019t really relational and in many ways isn\u2019t even close to the original concept. Naturally, this has led to confusion. SQL practitioners blame the relational data model for shortcomings in the SQL language, and relational data management experts blame the SQL standard for being a weak implementation of the relational model and ideals. A\u00aepplication engineers are stuck somewhere in th\u00aee middle, with the burden of delivering something that works. We highlight some important and significant aspects of this issue throughout this book, but generally we focus on the practical aspects. If you\u2019re interested in more background material, we highly recommend Practical Issues in Database Management: A Reference for the Thinking \u00aePractitioner by Fabian Pascal (Pascal, 2000) and An Introduction to Database Systems by Chris Date (Da\u00aete, 2003) for the theory, concepts, and ideals of (relational) database systems. The latter book is an excellent reference (it\u2019s big) for all questions you may possibly have about databases and data management.  To use Hibernate effectively, you must start with a solid understanding of the relational model and SQL. You need to understand the relational model and topics such as normalization to guarantee the integrity of your data, and you\u2019ll need to use your knowledge of SQL to tune the performance of your Hibernate application. Hibernate automates many repetitive coding tasks, but your knowledge of persistence technology must extend beyond Hibernate itself if you want to take advantage of the full power of modern SQL databases. To dig deeper, consult the bibliography at the end of this book.",
    "page128": "Y\u00aeou\u2019ve probably used SQL for many years and are familiar with the basic operations and statements written in this language. Still, we know from our own experience that SQL is sometimes hard to remember, a\u00aend some terms vary in usage.   Let\u2019s review some of the SQL terms used in this book. You use SQL as a data definition language (DDL) when creating, altering, and dropping artifacts such as tables and constraints in the catalog of the DBMS. When this schema is ready, you use SQL as a data manipulation \u00aelanguage (DML) to perform operations on data, inclu\u00aeding insertions, updates, and deletions. You retrieve data by executing queries with restrictions, projections, and Cartesian products. For efficient reporting, you use SQL to join, aggregate, and group data as necessary. You can even nest SQL statements ins\u00aeide each other a technique that uses subselects. When your business requirements change, you\u2019ll have to modify the database schema again with DDL statements after data has been stored; this is known as schema evolution.   If you\u2019re an SQL veteran and you want to know more about optimization and how SQL is executed, get a copy of the excellent book SQL Tuning, by Dan Tow (Tow, 2003). For a look at the practical side of SQL through the lens of how not to use SQL, SQL Antipatterns: Avoiding the Pitfalls of Database Programming (Karwin, 2010) is a good resource.   Although the SQL database is one part of ORM, the other part, of course, consists of the data in your \u00aeJava application that needs to be persisted to and loaded from the database.",
    "page129": "Using SQL in Java- When you work with an SQL databa\u00aese in a Java application, you issue SQL statements to the database via the J\u00aeava Database Connectivity (JDBC) API. Whether the SQL was written by hand and embedded in the Java code or generated on the fly by Java code, you use the JDBC API to bind arguments when preparing query parameters, executing the query, scrolling through the query result, retrieving values from the result set, and so on. These are low-level data access tasks; as application engineers, we\u2019re more interested in the business problem that requires this data access. What we\u2019d really like to write is code that saves and retrieves instances of our classes, relieving us of this lowlevel drudgery.  Because these data access tasks are often so tedious, we have to ask, are the relational data model and (especially) SQL the right choices for persistence in objectoriented applications? We answer this question unequivocal\u00aely: yes! There are many reasons why SQL databases dominate the computing industry relational database management systems are the only\u00ae proven generic data management technology, and they\u2019re almost always a requirement in Java projects.   Note that we aren\u2019t claiming that rela\u00aetional technology is always the best solution. There are many data management requirements th\u00aeat warrant a completely different approach. For example, internet-scale di\u00aestributed systems (web search eng\u00aeines, content distribution networks, peer-to-peer sharing, instant messaging) have to deal with exceptional transaction volumes. Many of these systems don\u2019t require that after a data update completes, all processes see the same updated data (strong transactional consistency). Users might be happy with weak consistency; after an update, there might be a window of inconsistency before all processes see the updated data. Some scientific applications work with enormous but very specialized dat\u00aeasets. Such systems and their unique challenges typically require equally unique and often custom-made persistence solutions. Generic data management tools such as ACID-compliant transactional SQL databases, JDBC, and\u00ae Hibernate would play only a minor role.",
    "page130": "ORM and JPA- In a nutshell, object/relational mapping is the automated (and transparent) persistence of objects in a Java application to the tables in an SQL database, using metadata that describes the mapping between the classes of the application and the schema of the SQL database. In essence, ORM works by transforming (reversibly) data from one representation to another. Before we move on, you need to understand what Hibernate can\u2019t do for you.  A supposed advantage of ORM is that it shields developers from messy SQL. This view holds that object-oriented developers can\u2019t be expected to understand SQL or relational databases well and that they find SQL somehow offensive. On the contrary, we believe that Java developers must have a sufficient level of familiarity with and appreciation of relational modeling and SQL in order to work with Hibernate. ORM is an advanced technique used by developers who have already done it the hard w\u00aeay. To use Hibernate effectively, you must be able to view and interpret the SQL statements it issues and understand their performance implications Let\u2019s look at some of the benefits of Hibernate: Productivity Hibernate eliminates much of the grunt work (more than you\u2019d expect) and lets you concentrate on the business problem. No matter which application-development strategy you prefer top-down, starting with a domain model, or bottom-up, starting with an existing database schema Hibernate, used together with the appropriate tools, will significantly reduce development time. Maintainability  Automated ORM with Hibernate reduces\u00ae lines of code (LOC), making the system more understandable and easier to refactor. Hibernate provides a buffer between the domain model and the SQL schema, insulating each model from minor changes to the other.  Performance Although hand-coded persistence might be faster in the same sense that assembly code can be faster than Java code, automated solutions like Hibernate allow the use of many optimizations at all times. One example of this is efficient and easily tunable caching in the application tier. This means developers can spend more \u00aeenergy hand-optimizing the few remaining real bottlenecks instead of prematurely optimizing everything.  Vendor independence Hibernate can help mitigate some of the risks associated with vendor lock-in. Even if you plan never to change your DBMS product, ORM tools that support a number of different DBMSs enable a certain level of portability. In addition, DBMS independence helps in development scenarios where engineers use a lightweight local database but deploy for testing and production on a different system.",
    "page131": "The Hibernate approach to persistence was well received by Java developers, and the standard Java Pers\u00aeistence API was designed along similar lines.  JPA became a key part of the simplifications introduced in recent EJB and Java EE specifications. We should be clear up front that neither Java Persistence nor Hibernate are limited to the Java EE environment; they\u2019re general-purpose solutions to the persistence problem that any type of Java (or Groovy, or Scala) application can use.   The JPA specification defines the following:  A facility for specifying mapping metadata how persistent classes and their properties relate to the database schema. JPA relies heavily on Java annotations in domain model classes, but you can also write mappings in X\u00aeML files. APIs for performing basic CRUD operations on instances of persistent class\u00aees, most prominently javax.persistence.EntityManager to store and load data. A language and APIs for specifying queries that refer to classes and properties of classes. This langua\u00aege is the Java Pers\u00aeistence Query Language (JPQL) and looks similar to SQL. The standardized API allows for programmatic creation of criteria queries without string manipulation. How the persistence engine interacts with transactional instances to perform dirty checking, association fetching, and other optimization functions. The latest JPA specification covers some basic caching strategies. Hibernate implements JPA and supports all the standardized mappings, queries, and programming interfaces.",
    "page132": "With object persistence, individual objects can outlive their\u00ae application process, be saved to a data store, and be re-created later. The object/relatio\u00aenal mismatch comes into play when the data store is an SQL-based relational database management system. For instance, a network of objects can\u2019t be saved to a database table; it must be disassembled and persisted to columns of portable SQL data types. A good solution for this problem is object/relational mapping (ORM).  ORM isn\u2019t a silver bullet for all persistence tasks; its job is to relieve the developer of 95% of object persistence work, such as writing complex SQL statements with many table joins and copying values from JDBC result sets to objects or graphs of objects.  A full-featured ORM middleware solution may provide database portability, certain optimization techniques like caching, and other viable functions that aren\u2019t easy to hand-co\u00aede in a limited time with SQ\u00aeL \u00aeand JDBC.  Better solutions than ORM might exist someday. We\u00ae (and many others) may have to rethink everything we know about data management systems and their languages, persistence API standards, and application integration. But the evolution of today\u2019s systems into true relational database systems with seamless object-oriented integration remains pure speculation. We can\u2019t wait, and there is no sign that any of these issues will improve soon (a multibillion-dollar industry isn\u2019t very agile). ORM is the best solution currently available, and it\u2019s a timesaver for developers facing the object/rela\u00aetional mismatch every day.",
    "page133": "Hibernate is an ambitious project that aims to provide a complete solution to the problem of managing persistent data in Java. Today, Hibernate is not only\u00ae an ORM service, but also a collection of data management tools extending well beyond ORM. The Hibernate project suite includes the following:  Hibernate ORM Hibernate ORM consists of a core, a base service for persistence with SQL databases, and a native \u00aeproprietary API. Hibernate ORM is the foundation for several of the other projects and is the oldest Hibernate project. You can use Hibernate ORM on its own, independent of any framework or any particular runtime environment with all JDKs. It works in every Java EE/J2EE application server, in Swing applications, in a simple servlet container, and so\u00ae on. As long as you can configure a data source for Hibernate, \u00aeit works. Hibernate EntityManager This is Hibernate\u2019s implementation of the standard Java Persistence APIs, an optional module you can stack on top of Hibernate ORM. You can fall back to Hibernate when a plain Hibernate interface or even a JDBC Connection is needed. Hibernate\u2019s native features are a superset of the JPA persistence features in every respect. \uf0a1 Hibernate Validator Hibernate provides the reference implementation of the Bean\u00ae Validation (JSR 303) specification. Independent of other Hibernate projects, it provides declarative validation for your domain model (or any other) classes. Hibernate Envers Envers is dedicated to audit logging and keeping multiple versions of data in your SQL database. This helps you add data history and audit trails to your application, similar to version control systems you might already be familiar with such as Subversion and Git. Hibernate Search Hibernate Search keeps an index of your domain model data up to date in an Apache Lucene database. It lets you query this database with a powerful and naturally integrated API. Many projects use Hibernate Search in addition to Hibernate ORM, adding full-text search capabilities. If you have a free text search form in your application\u2019s user interface, and you wan\u00aet happy users, work with Hibernate Search. Hibernate Search isn\u2019t covered in this book; you can find more information in Hibernate Search in Action by Emmanuel Bernard (Bernard, 2\u00ae008). Hibernate OGM The most recent Hibernate project is the object/grid mapper. It provides JPA support for NoSQL solutions, reusing the Hibernate core engine but persisting mapped entities into a key/value-, document-, or graph-oriented data store. Hibernate OGM isn\u2019t covered in this book. Let\u2019s get started\u00ae with your first Hibernate and JPA project.",
    "page134": "The \u201cHello World\u201d example in the previous chapter introduced you to Hibernate; certainly, it isn\u2019t useful for understanding the requirements of real-world applications with complex data models. For the rest of the book, we use a much more sophisticated example application CaveatEmptor, an online auction system to demonstrate Hibernate and Java Persistence. (Caveat emptor means \u201cLet the buyer beware\u201d.) We\u2019ll start our discussion of the application by \u00aeintroducing a layered application architecture. Then, you\u2019ll learn how to identify the business entities of a problem domain. You\u2019ll create a conceptual \u00aemodel of these entities and their attributes, called a domain model, and you\u2019ll implement it in Java by creating persistent classes. We\u2019ll spend some time exploring exactly what these Java classes should look like and where they fit within a typical layered application architecture. We\u2019ll also look at the persistence capabilities\u00ae of the classes and how this aspect influences the design and implementation. We\u2019ll add Bean Validation, which helps to automatically verify the integrity of the domain model data not only for persistent information but all business logic.   We\u2019ll then explore mapping metadata options the ways you tell Hibernate how your persistent classes and their properties relate to database tables and columns. This can be as simple as adding annotations directly in the Java source code of the classes or wr\u00aeiting XML documents that you eventually deploy along with the compiled Java classes that Hibernate accesses at runtime. After reading this chapter, you\u2019ll know how to design the persist\u00aeent parts of your domain model in complex real-world projects, and what mapping metadata option you\u2019ll primarily prefer and use. Let\u2019s start with the example application.",
    "page135": "The example CaveatEmptor application- The \u00aeCaveatEmptor example is an online auction application that demonstrates ORM techniques and Hibernate functionality. You can download the source code for the application from www.jpwh.org. We won\u2019t pay much attention to the user interface in this book (it could be web based or a rich client); we\u2019ll concentrate instead on the data access code. When a design decision about data access code that has consequences for the user interface has to be made\u00ae, we\u2019ll naturally consider both.   In order to understand the design issues involved in ORM, let\u2019s pretend the CaveatEmptor application doesn\u2019t yet exist and that you\u2019re bui\u00aelding it from scratch. Let\u2019s start by looking at the architecture. A layered architecture- W\u00aeith any nontrivial application, it usually makes sense to organize classes by concern. Persistence is one concern; others include presentation, workflow, and business logic. A typical object-oriented architect\u00aeure includes layers of code that represent the concerns. A layered architecture defines interfaces between code that implements the various concerns, allowing changes to be made to the way one concern is implemented without significant disruption to code in the other layers. Layering determines the kin\u00aeds of inter-layer dependencies that occur. The rules are as follows:  Layers communicate from top to bottom. A layer is dependent only on the interface of the layer directly below it. Each layer is unaware of any other layers except for the layer just below it.",
    "page136": "The Cavea\u00aetEmptor domain model The CaveatEmptor site auctions many different kinds of items, from electronic equipm\u00aeent to airline tickets. Auctions proceed according to the English auction strategy: users continue to place bids on an item until the bid period for that item expires, and the highest bidder wins.   In any store, goods are categorized by type and grouped with similar goods into sections and onto shelves. The auction catalog requires some kind of hierarchy of item categories so that a buyer ca\u00aen browse these categories or arbitrarily search by category and item attributes. Lists of items appear in the category browser and search result screens. Selecting an item from a list takes the buyer to an item-\u00aedetail view where an item may have images attached to it.   An auction consists of a sequence of bids, and one is the winning bid. User details include name, address, and billing information.   The result of this analysis, the high-level overview of the domain model, is shown in figure 3.3. Let\u2019s briefly discuss some interesting features of this model.   Each item can be auctioned only once, so you don\u2019\u00aet need to make Item distinct from any auction entities. Instead, you have a single auction item entity named Item. Thus, Bid is associated directly with Item. You model the Address informat\u00aeion of a User as a separate class, a User may have three addresses, for home, billing, and shipping. You do allow the user to have many BillingDetails. Subclasses of an abstract class represent the various billing strategies (allowing future extension).   The application may nest a Category inside another Category, and so on. A recursive association, from the Category entity to itself, expresses this r\u00aeelationship. Note that a single Category may have multiple child categories but at most one parent. Each Item belongs to at least one Category.   This representation isn\u2019t the complete domain model but only classes for which you need persistence capabilities. You\u2019d like to store and load instances of Category, Item, User, and so on. We have simplified this high-level overview a little; we may introduce additional classes later or make minor modifications to them when needed for more complex examples.",
    "page137": "Implementing the domain model- You\u2019ll start with an issue that any implementation must deal with: the separation of concerns. The domain model implementation is usually a central, organizing component; it\u2019s reused heavily whenever you implement new application functionality. For this reason, you should be prepared to go to some lengths to ensure that concerns other than business aspects don\u2019t leak into the domain model implementation. When concerns such as persistence, transaction mana\u00aegement,\u00ae or authorization start to appear in the domain mod\u00aeel classes, this is an example of leakage of concerns. The domain model implementation is such an important piece of code that it \u00aeshouldn\u2019t depend on orthogonal Java APIs. For example, code in the domain model shouldn\u2019t perform JNDI lookups or call t\u00aehe database via the JDBC API, not directly and not through an intermediate abstraction. This allows you to reuse the domain model classes virtually anywhere:  \u00aeThe presentation layer can acces\u00aes instances and attributes of domain model entities when rendering views. The controller components in the business layer can also access the state of domain model entities and call methods \u00aeof the entities to execute business logic. The persistence layer can load and store instances of domain model en\u00aetities from and to the database, preserving their state.",
    "page138": "Most important, preventing leakage of concerns makes it easy to unit-test the domain model without the need for a particular runtime environment or container, or the need for m\u00aeocking any service dependencies. You can write unit tests that verify the correct behavior of your domain model classes without any special test harness. (We aren\u2019t talking about testing \u201cload from the database\u201d and \u201cstore in the database\u201d aspects, but \u201ccalculate the shipping cost and tax\u201d behavior.)  The Java EE standard solves the problem of leaky concerns with metadata, as annotations within your code or externalized as XML descriptors. This approach allows th\u00aee runtime container to implement some predefined cross-cutting concerns security, concurrency, persistence, transactions, and remoteness in a generic way, by intercepting calls to application components.   Hibernate isn\u2019t a Java EE runtime environment, and it\u2019s not an application server. It\u2019s an implementation of just one specification under the Java EE umbrella JPA  and a solution for just one of these concerns: persistence.  JPA defines the entity class as the primary programming artifact. This programming model enables transparent persistence, and a JPA provider such as Hibernate also offers automated persistence. Transparent and a\u00aeutomated persistence- We use transparent to mean a complete separation of concerns between the persistent classes of the domain model and the persistence layer. The persistent classes \u00aeare unaware of and have no dependency on the persistence mechanism. We use automatic to refer to a persistence solution (your annotated domain, the layer, and mechanism) that relieves you of handling low-level mechanical details, such as writing most SQL statements and working with the JDBC API.",
    "page139": "The Item class of the CaveatEmptor domain model, \u00aefor example, shouldn\u2019t have any runtime dependency on any Java Persistence \u00aeor Hibernate API. Furthermore:  JPA doesn\u2019t require that any special superclasses or interfaces be inherited or implemented by persistent classes. Nor are any special classes used to implement attri\u00aebutes and associations. (Of course, the option to use both techniques is always there.) You can reuse persistent classes outside the\u00ae context of persistence, in unit te\u00aests or in the presentation layer, for example. You can create instances in any runtime environment with the regular Java new operator, preserving testability and reusability. In a system with transparent persistence, instances of entities aren\u2019t aware of the underlying data store; they need not even be aware that they\u2019re being persisted or retrieved. JPA externalizes persistence\u00ae conce\u00aerns to a generic persistence manager API. He\u00aence, most of your code, and certainly your complex business logic, doesn\u2019t have to concern itself with the current state of a domain model entity instance in a sing\u00aele th\u00aeread of execution.",
    "page140": "We regard tran\u00aesparency as a requirement because it makes an application easier to build and maintain. Transparent persistence should be one of the primary goals of any ORM solution. Clearly, no automated persistence solution is completely transparent: Every automated persistence layer, including JPA and Hi\u00aebernate, imposes some requirements on the persistent classes. For example, JPA requires that collectionvalued attributes be typed to an interface such as java.util.Set or java.util.List and not to an actual implement\u00aeation such as java.util.Ha\u00aeshSet (this is a good practice anyway). Or\u00ae, a JPA entity class has to have a special attribute, called the database iden\u00aetifier (which is also less of a\u00ae restriction but usually convenient).   You now know why the persistence mechanism should have minimal impact on how you implement a domain model, and that transparent an\u00aed automated persistence are req\u00aeuired. Our preferred programming model to archive this is POJO.",
    "page141": "Around 10 years ago, many developers sta\u00aerted talking about POJO, a back-to-basics approach that essentially revives JavaBeans\u00ae, a component model for UI development, and reapplies it to the other layers of a system. Several revisions of the EJB and \u00aeJPA specifications b\u00aerought us new lightweight entities, and it would be appropriate to call them persistence-capable JavaBeans. Java engineers often use al\u00ael these terms as synonyms for the s\u00aeame basic design approach.   You shouldn\u2019t be too concerned about what terms we use in this book; the ultimate goal is to apply the persistence aspect as transparently as possible to Java classes. Almost any Java class can be persistence-capable\u00ae if you follow some simple practices. Let\u2019s see how this looks in code.   Writing persistence-capable classes Working with fine-grained and rich domain models is a major Hibernate objective. This \u00aeis a reason we work with POJOs. In general, using fine-grained objects means more classes than tables.   A persistence-capable plain-old Java class declares attributes, which represent stat\u00aee, and business methods, which define behavior. Some attributes represent associations to other persistence-capable classes.",
    "page142": "JPA doesn\u2019t require that persistent classes implement java.io.Serializable. But when instances are stored in an HttpSession or passed by value using RMI, serialization is necessary. Although this might not occur in your application, the class wi\u00aell be serializable without any additional work, and there are no downsides to declaring that. (We aren\u2019t going to declare it on every example, assuming that you know when it will be necessary.)   The class can be abstract and, if needed, extend a non-persistent class or implement an interface. It must be a top-level class, not nested within another class. The persistence-capable class and any of \u00aeits methods can\u2019t be final (a requirement of the JPA speci\u00aefication).   Unlike the JavaBeans specificat\u00aeion, which requires no specific constructor, Hibernate (and JPA) require a constructor with no arguments for every persistent class. Alternatively, you might not write a constructor at all; Hibernate will then use the Java default constructor. Hibernate \u00aecalls classes using the Java reflection API on such a no argument constructor to\u00ae create instances. The constructor may not be public, but it has to be at least package-visible if Hibernate will use runtime-generated proxies for performance optimization. Also, consider the requirements of other specifications: the EJB standard requires public visibility on session bean constructors, just like the JavaServer Faces (JSF) specification requires for its managed beans. There are other situations when you\u2019d want a public constructor to create an \u201cempty\u201d state: for example, query-by-example building",
    "page143": "The properties of the POJO implement the attributes of the business entities\u00ae for example, the username of User. You usually implement properties as private or protected member fields, together with public or protected property accessor methods: for each field a method for retrieving its value and a method for settin\u00aeg the value. These methods are known as the getter and setter, respectively. The example POJO in listing 3.1 declares getter and setter methods for the username property.   The JavaBean specification defines the guidelines for naming accessor methods; this allows generic tools like Hibernate to easily discover and manipulate property values. A getter method\u00ae name begins with get, followed by the name of the property (the first letter in uppercase); a setter method name begins with set and similarly is followed by the name of the property. You may begin getter methods for Boolean properties with is instead of get.  Hibernate doesn\u2019t require accessor methods. You can choose how \u00aethe state of an instance\u00ae of your persistent classes should be persisted. Hibernate will either directly access fields or call accessor methods. Your class design isn\u2019t disturbed much by these consideration\u00aes. You \u00aecan make some accessor methods non-public or completely remove them then configure Hibernate to rely on field access for these properties.",
    "page144": "Should property fields and accessor methods be private,  protected, or package visible? Typically, you want to discourage direct access to the internal state of your class, so y\u00aeou don\u2019t make attribute fields public. If you make fields or methods private, you\u2019re effectively declaring that nobody should ever access them; only you\u2019re allowed to do that (or a service like Hibernate). This is a definitive statement. There are often good reasons for someone to access your \u201cprivate\u201d internals usually t\u00aeo fix one of your bugs and you only make people angry if they have to fall back to reflection access in an emergency. Instead, you might assume or know that the engineer who comes after you has access to your code and knows what they\u2019re doing.  The protected visibility then is a more reasonable default. You\u2019re forbidding direct public access, indicating that this particul\u00aear member detail is internal, but allowing access by subclasses if need be. You trust the engineer who creates the subclass. Package visibility is rude: you\u2019re forcing someone to create code in the same package to access member fields and methods; this is extra work for no good reason. Most important, these recommendations for visibility are relevant for environments without security policies and a runtime SecurityManager. If you have to keep your internal code private, make it private.",
    "page145": "Implementing POJO associations- You\u2019ll now see how to associate and create different kinds of relationships between objects: one-to-many, many-to-one, and bidirectional relati\u00aeonships. We\u2019ll look at the scaffolding code needed to create these associations, how to simplify relationship management, and how to enforce the integrity of these relationships.  Shouldn\u2019t bids on an item be stored in a list? The first reaction is often to preserve the order of elements as they\u2019re entered by users, because this may also be the order in which you will show them later. Certainly, in an auction application there has to be some defined order in which the user sees bids for an item for example, highest bid first or newest bid last. You might even work with a java.util.List in your user interface code to sort and display bids of an item. That doesn\u2019t mean this display order should be durable; data integrity isn\u2019t affected by the order in which bids are displayed. Y\u00aeou need to store the amount of each bid\u00ae, so you can find the highest bid, and you need to store a timestamp for each bid when it\u2019s created, so you can find the newest bid. When in\u00ae doubt, keep your system flexible and sort the data when it\u2019s retrieved fr\u00aeom the datastore (in a query) and/or shown to the user (in Java code), not when it\u2019s stored.",
    "page146": "The addBid() method not only reduces the lines of code when dealing with Item and Bid instances, but also enforces the cardinality of the association. You avoid errors that arise from leaving out one of the two required actions. You should always provide this kind of grouping of operations for associations, if possible. If you compare this with the relational model of foreig\u00aen keys in an SQL database, you can easily see how a network and pointer model complicates a simple operation: instead of a declarative constraint, you need procedural code to guar\u00aeantee data integrity.   Because you want addBid() to be the only externally visible mutator method for the bids of an item (possibly in addition to a removeBid() me\u00aethod), you can make the Item#setBids() method private or drop it and configure Hibernate to directly access fields for persistence. Consider making the Bid#setItem() method package-visible, for the same reason.   The Item#getBids() getter method still returns a modifiable collection, so clients can use it to make changes that aren\u2019t reflected on the inverse side. Bids added directly to the collection wouldn\u2019t have a reference to an item an inconsistent state, according to your database constraints. To prevent this, you can wrap the internal collection before returning it from the getter method, with Collections.unmodifiableCollection(c) and Collections.unmodifiableSet(s). The client then gets an exception if it tries to modify the collection; you therefore force every modification to go through the relationship management method that guarantees integrity. Note that in this case you\u2019ll have to configure Hibernate for field access, because the collection",
    "page147": "There are several problems with this appr\u00aeoach. First, Hibernate can\u2019t call this constructor. You need to add a no-argument constructor for Hibernate, and it needs \u00aeto be at least package-visible. Furthermore, because there is no setItem() method, Hibernate would have to be configured to access the item field directly. This means the field can\u2019t be final, so the class isn\u2019t guaranteed to be immutable.   In the examples in this book, we\u2019ll sometimes write scaffolding methods such as the Item#addBid() shown earlier, or we may have additional constructors for required values. It\u2019s up to you how many convenience methods and layers y\u00aeou want to wrap around the persistent association properties and/or fields, but we recommend \u00aebeing consistent and applying the same strategy to all your domain model classes. For the sake o\u00aef readability, we won\u2019t always show convenience methods, special constructors, and other such scaffolding in future c\u00aeode samples and assume you\u2019ll add them according to your own taste and requirem\u00aeents.   You now have seen domain model classes, how to represent their attributes, and the relationships between them. Next, we\u2019ll increase the level of abstraction, adding metadata to the domain model implementation \u00aeand declaring aspects such as validation and persistence rules.",
    "page148": "Domain model metadata- Metadata is data about data, so domain model metadata is information about your domain model. For example, when you use the Java reflection API to discover the names of classes of your domain model or the names of their attributes, you\u2019re accessing domain model metadata.  ORM tools also re\u00aequire metadata, to specify the mapping between classes and tables, properties and columns, associations and foreign keys, Java ty\u00aepes and SQL types, and so on. This object/relational mapping meta\u00aedata governs the transformation between the different type systems and \u00aerelationship representations in objectoriented and SQL systems. JPA has a metadata API, which you can call to obtain details about the persistence a\u00aespects of your domain model, such as the names of persistent entities and attributes\u00ae. First, it\u2019s your job as an engineer to create and maintain this information.  JPA standardizes two metadata options: annotations in Java code and externalized XML descri\u00aeptor files. Hibernate has some extensions for native functionality, also available as annotations and/or XML\u00ae descriptors. Usually we prefer either annotations or XML files as the primary source of mapping metadata. After reading this sec\u00aetion, you\u2019ll have the background information to make an educated decision for your own project.",
    "page149": "We\u2019ll also discuss Bean Va\u00aelidation (JSR 303) and how it provides declarative validation for your domain model (or any other) classes. The reference implementation of this specification is the Hibernate Validator project. Most engineers today prefer Java annotations as the primary mechanism for declaring metadata. Applying Bean Validation rules Mos\u00aet applications contain a multitude of data-integrity checks. You\u2019ve seen what happens when you violate one of the simplest data-integrity constraints: you get a NullPointerException when you expect a value to be availabl\u00aee. Other examples are a string-valued prope\u00aerty that shouldn\u2019t be empty (remember, an empty string isn\u2019t\u00ae null), a string that has to match a particular regular expression pattern, and a number or date value that must be within a ce\u00aertain range.   These business rules affect every layer of an application: The user interface code has to display detailed and localized error messages. The business and persistence \u00aelayers must check input values received from the client before passing them to the datastore. The SQL database has to be the final va\u00aelidator, ultimately guaranteeing the integrity of durable data.",
    "page150": "The idea behind Bean Validation is that declaring rules such as \u201cThis property can\u2019t be null\u201d or \u201cThis number has to be in the given range\u201d is much easier and less error-prone than writing if-then-else procedures repeatedly. Furthermore, declaring these rules on the central component of your application, the domain model implementation, enables integrity checks in every layer of the system. The rules are then available to the presentation and persistence layers. And if you consider how dataintegrity constraints affect not only your Java application code but also your SQL database schema which is a collection of integrity rule\u00aes you might think of Bean Validation constraints as additional ORM metadata You add two more attributes the name of an item and the auctionEnd date when an auction concludes. Both are typical candidates for additional constraints: you want to guarantee that the name is a\u00aelways present and human readable (on\u00aee-character item names don\u2019t make much sense), but it shouldn\u2019t be too long your SQL database will be most efficient with variable-length strings up to 255 characters, and your user interface also has some constraints on visible label space. The ending time of an auction obviously should be in the future. If you don\u2019t provide an error \u00aemessage, a default message will be used. Messages can be keys to external properties files, for internationalization.",
    "page151": "The validation engine will access th\u00aee fields directly if you annotate the fields. If you prefer calls through accessor methods, annotate the getter \u00aemethod with validation constraints, not the setter. Then constraints are part of the class\u2019s API and included in its Javadoc, making the domain model implementation easier to un\u00aederstand. Note that this is independent from access by the JPA provider; that is, Hibernate Validator may call accessor methods,\u00ae whereas Hibernate ORM may call fields\u00ae directly.   Bean Validation isn\u2019t limited to the built-in annotations; you can create your own constraints and annotations. With a custom constraint, you can even u\u00aese class-level annotations and validate several attribute values at the same time on an instance of the class. The following test code shows how you can manually check the integrity of an Item ins\u00aetance. We\u2019re not going to explain this code in detail but offer it for you to explore. You\u2019ll rarely write this kind of validation code; most of the time, this aspect is automatically handled by your user interface and persistence framework. It\u2019s therefore important to look for Bean Validation integration when selecting a UI framework. JSF version 2 and newer automatically integrates with Bean Validation, for example.   \u00aeHibernate, as required from any JPA provider, also automatically integrates with Hibernate Validator if the libraries are available on the classpath and offers the following features:",
    "page152": "You don\u2019t have to manually validate instances before passing them to Hibernate for storage. \uf0a1 Hibernate recognizes constraints on persistent domain model classes and triggers validation before database insert or update operations. When validation fails, Hibernate throws a ConstraintViolationException, containing the failure details, to the code calling persistence-management operations. \uf0a1 The Hibernate toolset for automatic SQL schema generation understands many constraints and generates SQL DDL-equivalent constraints for you. For example, an @NotNull annotation translates into an SQL NOT NULL constraint, and an @Size(n) rule defines the number of characters in a VARCHAR(n)-typed column. You can control this behavior of Hibernate with the <validation-mode> element in your persistence.xml configuration file. The default mode is AUTO, so Hibernate will only validate if it finds\u00ae a Bean Validation provider (such as Hibernate Validator) on the classpath of the running application. With mode CALLBACK, validation will always occur, and you\u2019ll get a deployment error if you forget to bundle a Bean Validation provider. The NONE mode disables automatic validation by the JPA provider. You\u2019ll see Bean Validation annotations again later in this book; you\u2019ll also find them in the example code bundles. At this point we could write much more about Hibernate Validator, but we\u2019d only repeat what is already available in the pr\u00aeoject\u2019s excellent reference guide. Have a look, and \u00ae\u00aefind out more about features such as validation groups and the metadata API for discovery of constraints.   The Java Persistence and Bean Validation standards embrace annotations aggressively. The expert groups have been aware of the advantages of XML deployment descriptors in certain situations, especially for configuration metadata that changes with each deployment.",
    "page153": "This part is all about actual ORM, from classes and properties to tables and columns. Chapter 4 starts with regular class and property mappings and explains how you can map fine-grained Java domain models. Next, in chapter 5, you\u2019ll see how to map basic properties and embeddable components, and how to control mapping between Java and SQL types. \u00aeIn chapter 6, you\u2019ll map inheritance hierarchies of \u00aeentities to the database using four basic inheritance-mapping strategies; y\u00aeou\u2019ll also map polymorphic associations. Chapter 7 is all about mapping collections and entity associations: you map persistent collections, collections of basic and embeddable types, and simple many-to-one and one-to-many entity associations. Chapter 8 dives deeper with advanced entity association mappings like mapping one-to-one entity associations, \u00aeone-to-many mapping options, and many-to-many and ternary entity relationships. Finally, you\u2019ll find chapter 9 most interesting if you need to introduce Hibernate in an existing application, or if you have to work with legacy database schemas and handwritten SQL. We\u2019ll also talk about customized SQL DDL for schema generation in this chapter.  After reading this part of the book, you\u2019ll be ready to create even the most complex mappings quickly and with the right strategy. You\u2019ll understand how the problem of inheritance mapping can be solved and how to map collections and associations. You\u2019ll also be able to tune and customize Hibernate for integration with any existing database schema or application.",
    "page154": "Fine-grained domain models- A major objective of Hibernate is support for fine-grained and rich domain models. It\u2019s o\u00aene reason we work with POJOs. In crude terms, fine-grained means more classes than tables.   For example, a user may have a home address in your domain model. In the database, you may have a single US\u00aeERS table with the columns HOME_STREET, HOME_CITY, and HOME_ZIPCODE. (Remember the problem of SQL types we discussed in section 1.2.1?)   In the d\u00aeomain model, you could use the same approach, representing the address as three string-valued properties of the User class. But it\u2019s much better to model this using an Address class, where User has a homeAddress property. This domain model achieves improved cohesion and greater code reuse, and it\u2019s more understandable than SQL with inflexible type systems.  JPA emphasizes the usefulness o\u00aef fine-grained classes for implementing type safety and behavior. For example, many people model an email address as a\u00ae string-valued property of User. A more sophisticated approach is to define an EmailAddress class, which adds higher-level semantics and behavior it may pr\u00aeovide a prepareMail() method (it s\u00aehouldn\u2019t have a sendMail() method, because you don\u2019t want your domain model classes to depend on the mail subsystem).   This granularity problem leads us to a distinction of central importance in ORM. In Java, all classes are of equal standing all instances have\u00ae their own identity and life cycle. When you introduce persistence, some instanc\u00aees may not have their own identity and life cycle but depend on others. Let\u2019s walk thro\u00aeugh an example",
    "page155": "Distinguishing entities and value types- You may find it helpful to add stereotype (a UML extensibility mechanism) information to your UML class diagrams so you can immediately recognize entities and value types. This practice also forces you to think about this distinction for all your classes, which is a first step to an optimal mapping and well-performing persistence layer.  The Item and User classes are obvious entities. They each have their own identity, their instances have \u00aereferences from many other instances (shared reference\u00aes), and they have independent lifespans.   Marking the Address as a value type is also easy: a single User instance references a particular Address instance. You know this because the association has been created as a composition, where the User instance has been made fully responsible for the life cycle of the referenced Address instance. Therefore, Address instances can\u2019t be referenced by anyone else and don\u2019t need their own identity.   The Bid class could be a problem. In object-oriented modeling, this is marked as a composition (the association between Item and Bid with the diamond). Thus, an Item is the owner of its Bid instances and holds a collection of references. At first, this seems reasonable, because bids in an auction system are useless when the item they were made for is gone.   But wh\u00aeat if a future extension of the domain model requires a \u00aeUser#b\u00aeids collection,\u00ae containing all bids made by a particular User? Right now, the association between Bid and User is unidirectional; a Bid has a bid\u00aeder reference. What if this was bidirectional?",
    "page156": "Mapping entities with identity- Mappi\u00aeng entities with identity requires you to understand Java identity and equality before we can walk through an entity class example and its mapping. After that, we\u2019ll be able to dig in deeper and select a primary key, configure key generators, and finally go through identifier generator strategies. First, it\u2019s vita\u00ael to understand the difference between Java object identity and object equality before we di\u00aescuss terms like database identity and the way JPA manages identity.  Understanding Java iden\u00aetity and equality- Java developers understand the difference between Java object identity and equality. Object identity (=) is a notion defined by the Java virtual machine. Two references are identical if they point to the same memory location.  On the other hand, object equality is a notion defined by a class\u2019s equals() method, sometimes also referred to as equivalence. Equivalence means two different (non-identical) instances have the same value the same state. Two different instances of String are equal if they repr\u00aeesent the same sequence of characters, even though each has its own \u00aelocation in the memory space of the virtual machine. (If you\u2019re a Java guru, we acknowledge that String is a special case. Assume we used a different class to make the same point.) Persistence complicates this picture. With object/relational p\u00aeersistence, a persistent instance is an in-memory representation of a particular row (or rows) of a database table (or tables). Along with Java identity and equality, we define database identity",
    "page157": "Objects are identical if they occupy the same memory location in the JVM. This can be checked with the a = b operator. This concept is known as object identity. \uf0a1 Objects are equal if they have th\u00aee same state, as defined by the a.equals(Object b) method. Classes that don\u2019t explicitly\u00ae override this method inherit the implementation defined by java.lang.Object, which compares object identity with ==. This concept is known as object equality. \uf0a1 Objects stored in a relational database are identical if they share the same table and primary key value. This concept, mapped into the Java space, is known as database identity. We now need to look at how datab\u00aease identity relates to object identity and how to express database identity in the mapping metadata. As an example, you\u2019ll map an entity of a domain model.  Every entity class has to have an @Id property; it\u2019s how JPA exposes database identi\u00aety to the application. We don\u2019t show the identifier property in our diagrams; we assume that each entity class has one. In our examples, we always name the identifier\u00ae property id. This is a good practice for your own project; use the same identifier property name for all your domain model entity classes. If you specify nothing else, this property maps to a primary key column\u00ae named ID of the ITEM table in your database s\u00aechema.",
    "page158": "Hibernate will use the field to access the identifier property value when loading and storing items, not getter or sett\u00aeer methods. Because @Id is on a fi\u00aeeld, Hibernate will now enable every field of the class as a persis\u00aetent property by default. The rule in JPA is this: if @Id is on a field, the JPA provider will access fields of the class directly and consider al\u00ael fields part of the persistent state by default. You\u2019ll see how to override this later in this chapter in our experience, field access is often the best choice, because it gives you more freedom for accessor method design.   Should you have a (pub\u00aelic) getter method fo\u00aer the identifier property? Well, the application of\u00aeten uses database identifiers as a convenient handle to a particular instance, even outside the persistence layer. For example, it\u2019s common for web applications to display the results of a search screen to the user as a list of summaries. When the user selects a particular element, the application may need to retrieve the selected item, and it\u2019s common to use a lookup by identifier for this purpose you\u2019ve probably already used identifiers this way, even in applications that rely on JDBC. Should you have a setter method? Primary key values never change, so you shouldn\u2019t allow modification of the identifier property value. Hibernate won\u2019t update a primary key column, and you shouldn\u2019t expose a public identifier setter method on an entity.   The Java type o\u00aef the identifier pr\u00aeoperty, java.lang.Long in the previous \u00aeexample, depends on the primary key column type of the ITEM table and how key values are produced. This brings us to the @GeneratedValue annotation and primary keys in general.",
    "page159": "Selecting a primary key-Must primary keys be immutable? The relational model defines that a candidate key must be unique and irreducible (no subset of the key attributes has the uniqueness property). Beyond that, pickin\u00aeg a candidate key as the primary key is a matter of taste. But Hibernate expects a cand\u00aeidate key to be immutable when used as the primary key. Hibernate doesn\u2019t support updating \u00aeprimary key values with an API; if you try to work around this requirement, you\u2019ll run into problems with Hibernate\u2019s caching and dirty-checking engine. If your database schema relies on updatable primary keys (and maybe uses ON UPDATE CASCADE foreign key constraints), you must change the schema before it will work with Hibernate. The database identifier of an entity is mapped to some table primary key, so let\u2019s first get some background on primary keys without worrying about mappings. Take a step back and think about how you identify entities.   A candidate key is a column or set of columns that you could use to identify a particular row in a table. To become the primary key, a candidate key must satisfy the following requirements:  \u00aeThe value of any candidate key column is never null. You can\u2019t identify something with data that is unknown, and there are no nulls in the relational model. Some SQL products allow defining (composite) primary keys with nullable columns, so you must be careful. The value of the candidate key column(s) is a unique value for any row. The value of the candidate key column(s) never changes; it\u2019s immutable.",
    "page160": "If a table has only one identifying attribute, it becomes, by definition, the primary key. But several columns or combinations of columns may satisfy these properties for a particular table; you choose between candidate keys to decide the best \u00aeprimary key for the table. You should declare candidate keys not chosen as the primary key as unique keys in the database if their value is indeed unique (but maybe not immutable).   Many legacy SQL data models use natural primary keys. A natural key is a key with business meaning: an attribute or combination of attributes that is unique by virtue of its business semantics. Examples of natural keys are the US Social Security Number and Australian Tax File Number. Distinguishing natural keys is simple: if a candida\u00aete key attribute has meaning outside the database context, it\u2019s a natural key, regardless of whe\u00aether it\u2019s automatically generated. Think about the application users: if they refer to a key attribute when talking about and working with the application, it\u2019s a natural key: \u201cCan you s\u00aeend me the pictures of item #123-abc?\u201d  Experience has sh\u00aeown that natural primary keys usually cause problems in the end. A good primary key must be unique, immutable, and never null. Few entity at\u00aetr\u00aeibutes satisfy these requirements, and some that do can\u2019t be efficiently indexed by SQL databases (although this is an implementation detail and shouldn\u2019t be the deciding factor for or against a particular key). In addition, you should make certain that a candidate key definition never changes throughout the lifetime of the database. Changing the value (or even definition) of a primary key,\u00ae and \u00aeall foreign keys that refer to it, is a frustrating task. Expect your database schema to survive decades, even if your application won\u2019t.",
    "page161": "Furthermore, you can often only find natural candidate keys by combining several columns in a composite natural key. These composite keys, although certainly appropriate for some schema artifacts (like a link table in a many-to-many relationshi\u00aep), potentially make maintenance, ad hoc queries, and schema evolution much more difficult. We talk about composite keys later in the book, For these reasons, we strongly recommend that you add synthetic identifiers, also called surrogate keys. Surrogate keys have no business meaning they have unique values generated by the database or application. Application users ideally don\u2019t see or refer to these key values; they\u2019re part of the system internals. Introducing a surrogate key column is also appropriate in the common situation when there are no candidate keys. In other words, (almost) every table in your schema should have a dedicated surrogate primary key column with only this purpose.  There are a number of well-known approaches to generating surrogate key values. The aforementioned @GeneratedValue annotation is how you configure this.  Conf\u00aeiguring key generators- The @Id annotation is required to mark the iden\u00aetifier property of an entity class. Without the @GeneratedValue next to it, the JPA provider assumes that you\u2019ll take care of creating and assigning an identifier value before you save an instance. We call this an application-assigned identifier. Assigning an entity identifier manually is necessary when you\u2019re dealing with a legacy database and/or natural primary keys. We have more to say about this kind of mapping in a dedicated section,",
    "page162": "Usually you want the system to gene\u00aerate a primary key value when you save an entity instance, so you write the\u00ae GeneratedValue annotation next to @Id. JPA standardizes several value-generation strategies with the javax.persistence.GenerationType enum, which you select with @GeneratedValue(strategy):  GenerationType.AUTO Hibernate pi\u00aecks an appropriate strategy, asking t\u00aehe SQL dialect of your configured database what is best. This is equivalent to @GeneratedValue() without any settings\u00ae. GenerationType.SEQUENCE Hibernate expects (and creates, if you use the tools) a sequence named HIBERNATE_SEQUENCE in your database. The sequence will be\u00ae called separately before every INSERT, producing sequential numeric values. GenerationType.IDENTITY Hibernate expects (and creates in table DDL) a special auto-incremented primary key column that automatically generates a numeric value on INSERT, in the database. GenerationType.TABLE Hibernate will use an extra table in your database schema that holds the next numeric primary key value, one row for each entity class. This table will be read and updated accordingly, before INSERTs. The default table name is\u00ae HIBERNATE_SEQUENCES with columns SEQUENCE_NAME and SEQUENCE_NEXT_HI_VALUE. (The internal implementation uses a more complex but efficient hi/lo generation algorithm; more on this later.)",
    "page163": "JPA has two built-in annotations you can use to configure named generators: @javax persistence.SequenceGenerator and @javax.persistence.TableGenerator. With these annotations, you can create a named generator with your own \u00aesequence and table names. As usual with JPA annotations, you can unfortunately only use them at the top of a (maybe otherwise empty) class, and not in a package-info.java file.  For this reason, and because the JPA annotations don\u2019t give us access to the full Hiber\u0002nate feature set, we prefer an alternative: the native @org.hibernate.annotations .GenericGenerator annotation. It supports all Hibernate identifier generator strate\u0002gies and their configuration details. Unlike the rather limited JPA annotations, you can use the Hibernate annotation in a package-info.java file, typically in the same package as your domain model classes. The next listing shows a recommended configuration. This Hibernate-specific generator configuration has\u00ae the following advantages:  The enhanced-sequence B s\u00aetrategy produces sequential numeric values. If your SQL dialect supports sequences, Hibernate will use an actual database sequence. If your DBMS doesn\u2019t support native sequences, Hibernate will man\u0002age and use an extra \u201csequence table,\u201d simulating the behavior of a sequence. This gives you re\u00aeal portability: the generator can always be called before per\u0002forming an SQL INSERT, unlike, for example, auto-increment identity columns, which produce a value on INSERT that has to be returned to the application afterward.",
    "page164": "You can configure the sequence_name C. Hibernate will either use an existing sequence or create it when you generate the SQ\u00aeL schema automatically. If your DBMS doesn\u2019t support sequences, this will be the special \u201csequence table\u201d name. You can start with an initial_value D that gives you room for test data. For example, when your integration test runs, Hibernate will make any new data insertions from test code with identifier values greater than 1000. Any test data you want to \u00aeimport before the test can use numbers 1 to 999, a\u00aend you can refer to the stable identifier values in your tests: \u201cLoad item with id 123 and run some tests on it.\u201d This is applied when Hibernate generates the SQL schema and sequence; it\u2019s\u00ae a DDL option. You \u00aecan share the same database sequence among all your domain model classes. There is no harm in specifying @GeneratedValue(generator \"ID_GENERATOR\") in all your entity classes. It doesn\u2019t matter if primary key values aren\u2019t contiguous for a particular entity, as long as they\u2019re unique with\u00aein one table. If you\u2019re worried about contention, because the sequence has to be called prior to every INSERT, we discuss a variation of this generator configuration later,",
    "page165": "Finally, you use java.lang.Long as the type of the identifier property in the entity class, which maps perfectly to a n\u00aeumeric database sequence generator.\u00ae You could also use a long primitive. The main difference is what someItem.getId() returns on a new item that hasn\u2019t been stored in the database: either null or 0. If you want to test whether an item is new, a null check is probably easier to understand for someone else reading your code. You shouldn\u2019t use another integral type such as int or short for identifiers. Although they will work for a while (perhaps even years), as your data\u0002base size grows, you may be limited by their range. An Integer would work for almost two months if you generated a new identifier each millisecond with no gaps, and a Long would last for about 300 million years.  Although recommended for most applications, the enhanced-sequence strategy a\u00aes shown in listing is just one of the strategies built into Hibernate. Identifier generator strategies Following is a list of all available Hibernate identifier generator strategies, their options, and our usage recommendations. If you don\u2019t want to read the whole list now, enable GenerationType.AUTO and check what Hibernate defaults to for you\u00aer database dialect. It\u2019s most likely sequence or identity a good but maybe not t\u00aehe most efficient or portable choice. If you require consistent portable behavior, and identifier values to be available before INSERTs, use enhanced-sequence, as shown in the previous section. This is a portable, flexible, and modern strategy, also offering various optimizers for large datasets.",
    "page166": "We also show the relationship between each standard JPA strategy and its native Hibernate equivalent. Hibernate has been growing organically, so there are now two sets of mappings between standard and native strategies; we call them Old and New in the list. You can switch this mapping with the hibernate.id.new_generator_mappings setting in your persistenc\u00aee.xml file. The default is true; hence the New mapping. Soft\u0002ware doesn\u2019t age quite as well as wine: native Automatically selects other strategies, such as sequence or identity, \u00aedepending on the configured SQL dialect. You have to look at the Javadoc (or even the source) of the SQL dialect you configured in persistence.xml. Equiva\u0002lent to JPA GenerationType.AUTO with the Old mapping. sequence Uses a native database sequence named HIBERNATE_SEQUENCE. The sequence is called before each INSERT of a new row. You can customize the sequence name and provide additional DDL settings; see the Javadoc for the class org.hibernate.id.SequenceGenerator and its parent. enhanced-sequence Uses a native database sequence when supported; other\u0002wise falls back to an extra database table with a single column and row, emulating a sequence. Defaults to name HIBERNATE_SEQUENCE. Always calls the database \u201csequence\u201d before an INSERT, providing the same behavior independently of whether the DBMS supports real sequences. Supports an org.hibernate .id.enhanced.Optimizer to avoid hitting the database before each INSERT; defaults to no optimization and fetching a new value for each INSERT. You can find more examples in chapter\u00ae 20. For all parameters, see the Javad\u00aeoc for the class org.hibernate.id.enhanced.SequenceStyleGenerator. Equivalent to JPA GenerationType.SEQUENCE and \u00aeGenerationType.AUTO with the New mapping enabled, most likely your best option of the built-in strategies.",
    "page167": "seqhilo Uses a native database sequence named HIBERNATE_SEQUENCE, opti\u00ae\u0002mizing calls before INSERT by combining hi/lo values. If the hi value retrieved from the sequence is 1, the next 9 insertions will be made wit\u00aeh key values 11, 12, 13, \u2026, 19. Then the sequence is called again to obtain the next hi value (2 or higher), and the procedure repeats with 21, 22, 23, and so on. You can config\u0002ure the maximum lo value (9 is t\u00aehe default) with the max_lo parameter. Unfor\u0002tunately, due to a quirk in\u00ae Hibernate\u2019s code, you can not confi\u00aegure this strategy in @GenericGenerator. The only way to \u00aeuse it is with JPA Generation\u0002Type.SEQUENCE and the Old mapping. You can configure it with the standar\u00aed JPA @SequenceGenerator annotation on a (maybe otherwise empty) class. See the Javadoc for the class org.hibernate.id.SequenceHiLoGenerator and its parent for mo\u00aere information. Consider using enhanced-sequence instead, with an optimizer hilo Uses an extra table named HIBERNATE_UNIQUE_KEY with the same algo\u0002rithm as the seqhilo strategy. The table has a single column and row, holding the next value of the sequence. The default maximum lo value is 32767, so you most likely want to configure it with the max_lo parameter. See the Javadoc for the class org.hibernate.id.TableHiLoGenerator for more information. We don\u2019t recom\u00aemend this legacy strategy; use e\u00aenhanced-sequence instead with an optimizer.",
    "page168": "enhanced-table Uses an extra table named HIBERNATE_SEQUENCES, with one row by default representing the sequence, storing the next value. This value is selected and updated when an identifier value has to be generated. You can configure this generator to use multiple rows instead: one for each generator; see the Javadoc for org.hibernate.id.enhanced.TableGenerator. Equivalent to JPA GenerationType.TABLE with the New mapping enabled. Replaces the outdated but similar org.hibernate.id.MultipleHiLoPerTableGenerator, which is the Old mapping for JPA GenerationType.TABLE. \uf0a1 identity Supports IDENTITY and auto-increment columns in DB2, MySQL, MS SQL Server, and Sybase. The identifier value for the primary key co\u00aelumn will be ge\u00aenerated on INSERT of a row. Has no options. Unfortunately, due to a quirk in Hibernate\u2019s code, you can not configure this strategy in @Generic\u00aeGenerator. The only way to use it is with JPA GenerationTy\u00aepe.IDENTITY and the Old or New mapping, making it the default for GenerationType.IDENTITY. \uf0a1 i\u00aencrement At Hibernate startup, reads the maximum (numeric) primary key column value of each entity\u2019s table and increments the value by one each time a new row is inserted. Es\u00aepecially efficient if a non-clustered Hibernate application has exclusive access to the database; but don\u2019t use it in any other scenario.",
    "page169": "select Hibernate won\u2019t generate a key value or include the primary key col\u0002umn in an INSERT statement. Hibernate expects the DBMS to assign a (default in schema\u00ae or by trigger) value to the column on insertion. Hibernate then retrieves the primary key column with a \u00aeSELECT query after insertion. Required parameter is key, naming the database identifier property (such as id) for the SELECT. This strategy isn\u2019t very efficient and should only be used with ol\u00aed JDBC drivers that can\u2019t return generated keys directly. uuid2 Produces a unique 128-bit UUID in the application layer. Useful when you need globally unique identifiers across databases (say, you merge data from several distinct production databases in batch runs every night into an archive). The UUID can be enco\u00aeded either as a java.lang.String, a byte[16], or a java .util.UUID property in your entity class. Replaces the legacy uuid and uuid .hex strategies. You configure it with an org.hibernate.id.UUIDGeneration\u0002Strategy; see the Javadoc for the class org.hibernate.id.UUIDGenerator for more details. guid Uses a globally unique identifier produced by the database, with an SQL function available on Oracle, Ingres, MS SQL Server, and MySQL. Hibernate calls the database function before an INSERT. Maps to a java.lang.String identifier property. If you need full control over identifier generation, config\u0002ure the strategy of @GenericGenerator with the fully qualified name of a class that implements the org.hibernate.id.IdentityGenerator interface.",
    "page170": "QUOTING SQL IDENTIFIERS- From time to time, especially in legacy databases, you\u2019ll encounter identifiers with strange characters or whitespace, or wish to force case sensitivity. Or, as in the previous example, the automatic mapping of a class or property would require a table or col\u0002umn name that is a reserved keyword.   Hibernate 5 knows the r\u00aeeserved keywords of your DBMS through the configured\u00ae database dialect. Hibernate 5 can automatically put quotes around such strings when generating SQL. You can enable this automatic quoting with hibernate.auto_quote _keyword true in your persistence unit configuration. If you\u2019re using an older version of Hibernate, or you find that the dialect\u2019s information is incomplete, you must still apply quotes on names manually in your mappings if there is a conflict w\u00aeith a keyword.   If you quote a table or column name in your mapping \u00aewith backticks, Hibernate always quotes this identifier in the generated SQL. This still works in latest versions of Hibernate, but JPA 2.0 standardized this functionality as delimited identifiers with dou\u0002ble quotes.  If you have to quote all SQL identifiers, create an orm.xml file and add the setting <delimited-identifiers/> to its <persistence-unit-defaults> section, as shown in listing 3.8. Hibernate then enforces quoted identifiers everywhere.   You should consider renaming tables or columns with reserved keyword names whenever possible. Ad hoc SQL queries are difficult to write in an SQL console if you have to quote and escape everything properly by hand.   Next, you\u2019ll see how Hibernate can help when you encounter organizations with strict conven\u00aetions for database table and column names.To summarize, our recommendations on identifier generator strategies are as follows:  In general, we prefer pre-insert generation strategies that produce identifier values independently before INSERT. Use enhanced-sequence, which uses a native database sequence when supported and otherwise falls back to an extra database table with a single column and row, emulating a sequence. We assume from now on that you\u2019ve added identifier properties to the entity classes of your domain model and that after you complete the basic mapping of each entity and its identifier property, you continue to \u00aemap the value-typed properties of the entities. We talk about value-type mappings in the next chapter. Read on for some special options that can simplify and enhance your class mappings. Entity-mapping options- You\u2019ve now mapped a persistent class with @Entity, using defaults for all other settings, such as the mapped SQL table name. The following section explores some classlevel options an\u00aed how you control them:  Naming defaults and strategies Dynamic SQL generation Entity mutability These are options; you can skip \u00aethis section and come back later when you have to deal with a specific problem. Controlling names- Let\u2019s first talk about the naming of entity classes and tables. If you only specify @Entity on the persistence-capable class, the default mapped tabl\u00aee name is the same as the class name. Note that we write SQL artifact names in UPPERCASE to make them easier to distinguish SQL is actually case insensitive. So the Java entity class Item maps to the ITEM table. You can override the table name with the JPA @Table annotation, as shown next.",
    "page171": "Entities are the coarser-grained classes of your system. Their instances have an independent life cycle and their own identit\u00aey, and many other instances can ref\u0002erence them.  Value types, on the other hand, are dependent on a particular entity class. A value type instance is bound to its owning entity instance, and only one entity instance can reference it it has no individual identity.  We looked at Java identity, object equality, and database identity, and at what makes good primary keys. You learned which generators for primary key values Hibernate provides out of the box, and how to use and extend this identifier system.  We discussed some useful class mapping options, such as naming strategies and dynamic SQL generation. After spending the previous chapter almost exclusively on entities and the respec\u0002tive class- and identity-mapping options, we now focus on value types in their various forms. We split value types into two categories: basic value-typed classes that come with the JDK, such as String, Date, primitives, and their wrappers; and developer\u0002defined value-typed class\u00aees, such as Address and MonetaryAmount in CaveatEmptor.  In this chapter, we first map persistent properties with JDK types and learn the basic mapping annotations. You see how to work with various aspects of properties: overriding defaults, customizing access, and generated values. You also see how SQL is used with derived properties and transformed column values. We wrap up basic properties with temporal properties and mapping enumerations. We then dis\u0002cuss custom value-typed classes and map them as embeddable components. You learn how classes relate to the database schema and make your classes embeddable, while allowing for overriding embedded attributes. We complete embeddable components by mapping nested components. Finally, we discuss how to customize loading and storing of property values at a lower level with flexible JPA converters, a standardized extension point of every JPA provider. Dynamic SQL generation- By default, Hibernate creates SQL statements for each persistent class when the persis\u0002tence unit is created, on startup. These statements are simple create, read, update, and delete (CRUD) operations for reading a single row, deleting a row, and so on. It\u2019s cheaper to store these in memory up front, instead of generating SQL strings every time such a simple query has to be executed at runtime. In addition, prepared state\u0002ment caching at the JDBC level is much more efficient if there are fewer statements. How can Hibernate create an UPDATE statement on startup? After all, the columns to be updated aren\u2019t known at this time. The answer is that the generated SQL state\u0002ment updates all columns, and if the value of a particular column isn\u2019t modified, the statement sets \u00aeit to its old value.   In some situations, such as a legacy table with hundreds of columns where the SQL statements will be large for even the simplest operations (say, only one column needs updating), you should disable this startup SQL generation\u00ae and switch to dynamic state\u0002ments generated at runtime. An extremely large number of entities can also impact startup time, because Hibernate has to generate all SQL statements for CRUD up front. Memory consumption for this query statement cache will also be high if a doz\u00aeen state\u0002ments must be cached for thousands of entities. This can be an issue in virtual envi\u0002ronments with memory limitations, or on low-po\u00aewer devices.   To disable generation of INSERT and UPDATE SQL statements on startup, you need native Hibernate annotations:",
    "page172": "This configuration by exception app\u00aeroach means you don\u2019t have to annotate a property to make it persistent; y\u00aeou only have to configure the mapping in an exceptional case. Sev\u0002eral annotations are available in JPA to customize and control basic property map. Overriding basic property defaults- You might not want all properties of an entity class to be persistent. For example, although it makes sense to have a persistent Item#initialPrice property, an Item#totalPriceIncludingTax property shouldn\u2019t be persistent if you only com\u0002pute and use its value at runtime, and hence shouldn\u2019t be stored in the database. To exclude a property, mark the field or the getter method of the property with the @javax.pers\u00ae\u00aeistence.Transient annotation or use th\u00aee Java transient keyword. The transient keyword usually only excludes fields for Java serialization, but it\u2019s also rec\u0002ognized by JPA providers.   We\u2019ll come back to the placement of the annotation on fields or getter methods in a moment. Let\u2019s assume as we have before that Hibernate will access fields directly because @Id has been placed on a field. Therefore, all other JPA and Hibernate map\u0002ping an\u00aenotations are also on fields.  We have to admit that this annotation isn\u2019t very useful. It only has two parameters: the one shown here, optional, marks the property as not optional at the Java object level. By default, all persistent properties are nullable and optional; an Item may have an unknown initialPrice. Mapping the initialPrice property as non-optional makes sense if you have a NOT NULL constraint on the INITIALPRICE column in your SQL schema. If Hibernate is generating the SQL schema, it will include a NOT NULL con\u0002straint automatically for non-optional properties. Mapping basic pr\u00aeoperties- When you map a persistent class, whether it\u2019s an entity or an embeddable type (more about these later, in section 5.2), all of its properties are considered\u00ae persistent by default. The default JPA rules for properties of persistent classes are these:  \uf0a1 If the property is a primitive or a primitive wrapper, or of type String, BigInteger, BigDecimal, java.util.Date, java.util.Calendar, java.sql.Date, java.sql .Time, java.sql.Timestamp, byte[], Byte[], char[], or Character[], it\u2019s auto\u0002matically persistent. Hibernate loads and stores the value of the property in a col\u0002umn with an appropriate SQL type and the same name as the property. Otherwise, if you annotate the class of the property as @Embeddable, or you map the property itself as @Embedded, Hibernate maps the property as an embedded component of the owning class. We discuss embedding of components later in this chapter, with the Address and MonetaryAmount embeddable classes o\u00aef CaveatEmptor. Otherwise, if the type of the property is java.io.Serializable, its value is stored in its serialized form. This typically isn\u2019t what you want, and you should always map J\u00aeava classes instead of storing a heap of bytes in the database. Imag\u0002ine maintaining a database with this binary information when the application is gone in a few years. Otherwise, Hibernate will throw an exception on startup, complaining that it doesn\u2019t understand the type of the property",
    "page173": "The Column annotation has a few other parameters, most of which control SQL-level details such as ca\u00aetalog and schema names. They\u2019re rarely needed, and we only show them throughout this book when necessary.   Property annotations aren\u2019t always on fields, and you may not want Hibernate to access fields directly.  Customizing property access- The persistence engine accesses the properties of a class either directly through fields or indirectly through getter and setter methods. An annotated entity inherits the default from the position of the mandatory @Id annotation. For example, if you\u2019ve declared @Id on a field, not a getter method, all other mapping annotations for that entity are expected on fields. Annotations are never on the setter methods.   The default access strategy isn\u2019t only applicable to a single entity class. Any @Embedded class inherits the default or explicitly declared access strategy of its own\u0002ing root entity clas\u00aes. We cover embedded components later in this chapter. Further\u0002more, Hibernate accesses any @MappedSuperclass properties with the default or explicitly declared access strategy of the mapped entity class. Inheritance is the topic of chapter 6.   The JPA specification offers the @Access annotation for overriding the default behavior, with the parameters AccessType.FIELD and AccessType.PROPERTY. If you set @Access on the class/entity level, Hibernate accesses all properties of the class according to the selected strategy. You then set any other mapping annotations, including the @Id, on either fields or getter methods, respectively. Now, when you store an Item and forget to set a value on the initialPrice field, Hibernate will complain with an exception before hitting the database with an SQL statement. Hibernate knows that a value is required to perform an INSERT or UPDATE.\u00ae If you don\u2019t mark the property as optional and try to save a NULL, the database will reject the SQL statement, and Hibernate will throw a constraint-violation exception. There isn\u2019t much difference in the end result, but it\u2019s cleaner to avoid hitting the data\u0002base with a statement that fails. We\u2019ll talk about the other parameter of @Basic, the fetch option, when we explore optimization strategies later, in section  Instead of @Basic, most engineers use the more versatile @Column annotation to declare nullability: We\u2019ve now shown you three ways to declare whether a property value is required: with the @Basic annotation, the @Column annotation, and earlier with the Bean Validation @NotNull annotation in section 3.3.2. All have the same effect on the JPA provider: Hibernate does a null check when saving and generates a NOT NULL constraint in the database schema. We recommend the Bean Validation @NotNull annot\u00aeation so you can manually validate an Item instance and/or have your user interface code in the presentation layer execute validation checks automatically.   The @Column annotation can also override the mapping of the property name to the database column:",
    "page174": "Hibernate reads your Java domain model classes and mapping metadata and generates schema DDL statements. You can export them into a text file or execute them directly on your database whenever you run integration tests. Because schema languages are mostly vendor-specific, every option you put in your mapping metadata has the potential to bind the metadata to a particular database product keep this in mind when using schema features.  Hibernate creates the basic schema for your tables and constraints automatically; it even creates sequences, d\u00aeepending on the identifier generator you select. But there are some schema artifacts Hibernate can\u2019t and won\u2019t create automatically. These include all kinds of highly vendor-specific performance options and any other artifacts that are relevant only for the physical storage of data (tablespaces, for example). Besides these physical concerns, your DBA will often supply custom additional schema statements to improve the generated schema. DBAs should get involved early and verify the automatically generated schema from Hibernate. Never go into production with an unchecked automatically generated schema. If your development process allows, changes made by the DBA can flow back into your Java systems, for you to add to mapping metadata. In many projects, the mapping metadata can contain all the necessary schema changes from a DBA. Then Hibernate can generate the final production schema during the regular build by including all comments, constraints, indexes, and so on.  In the following sections, we show you how to customize the generated schema and how to add auxiliary database schema artifacts (we call them objects sometimes; we don\u2019t mean Java ob\u00aejects here). We discuss custom data types, additional integrity rules, indexes, and how you can replace some of the (sometimes ugly) auto-generated artifact names produced by Hibernate.If your application can auction an item only once in the real world, your database schema should guarantee that. If an auction always has a starting price, your\u00ae database model should include an appropriate constraint. If data satisfies all integrity rules, the data is consistent, a term you\u2019ll meet again in section 11.1.  We also assume that consistent data is correct: everything the database states, either explicitly\u00ae or implicitly, is true; everything else is false. If you want to know more about the theory behind this approach, look up the closed-world assumption (CWA) Sometimes you can start a project top-down. There is\u00ae no existing database schema and ma\u00aeybe not even any data your application is completely new. Many developers like to let Hibernate automatically generate the scripts for a database schema. You\u2019ll probably also let Hibernate deploy the schema on the test database on your development machine or your continuous build systems for integra\u00aetion testing. Later, a DBA will take the generated scripts and write an improved and final schema for production deployment. The first part of this chapter shows you how to improve the schema from within JPA and Hibernate, to make your DBA happy.  At the other end of the spectrum are systems with existing, possibly complex schema\u00aes, with years\u2019 worth of data. Your new application is just a small gear in a big m\u00aeachine, and your DBA won\u2019t allow any (sometimes even non-disruptive) changes to the database. You need a flexible object/relational mapping so you don\u2019t have to twist and bend the Java classes too much when things don\u2019t fit right away. This will be the subject of the second half of this chapter,\u00ae including a discussion of composite primary and foreign keys.  Let\u2019s start with a clean-room implementation and Hibernate-generated schemas.",
    "page175": "This property defines when the create and drop scripts should be executed. Your custom SQL scripts will contain CREATE DOMAIN statements, which must be executed before the tables using the\u00aese domains are created. With these settings, the schema generator runs the create script first before reading your ORM metadata (annotations, XML files) and creating the tables. The drop script executes after Hiberna\u00aete drops the tables, giving you a chance to clean up anything you created. Other options are metadata (ignore cu\u00aestom script sources) and script (only use a custom script source; ignore ORM metadata in annotations and XML files). D This is the location of the custom SQL script for creation of the schema. The path is (a) the location of the script resource on the classpath; (b) the location of the script as a file:// URL; or, if neither (a) nor (b) matches, (c) the absolute or relative file path on the local file system. This example uses (a). E This is the custom SQL script for dropping the schema. F This load script runs after the tables have been created. We\u2019ve mentioned that DDL is usually highly vendor-specific. If your application has to support several database dialects, you may need several sets of create/drop/load scripts to customize the schema for each database dialect. You can solve this with several persistence unit configurations in the persistence.xml file.  Alternatively, Hibernate has\u00ae its own proprietary configuration for schema customization in an hbm.xml mapping file, You can hook the following three types of custom SQL scripts\u00ae into Hibernates schema-generation process:  The create script executes when the schema is generated. A custom create script can run before, after, or instead of Hibernates automatically generated scripts. In other words, you can write an SQL script that runs before or after Hibernate generates tables, constraints, and so on from your mapping metadata.  The drop script executes when Hibernate removes schema artifacts. Just like the create script, a drop script can run before, after, or instead of Hibernates automatically generated statements. The load script always executes after Hibernate generates the schem\u00aea, as the last step after creation. Its main purpose is importing test or master data, before your application or unit test runs. It can contain any kind of SQL statement, including DDL statements such as ALTER, i\u00aef you want to further customize the schema. This customization of the schema-generation process is actually standardized; you configure it with JPA properties in persistence.xml for a persistence unit. By default, Hibernate expects one SQL statement per line in scripts. This switches to the more convenient multiline extractor. SQL statements in scripts are terminated with semicolon. You can write your own org.hibernate.tool.hbm2ddl.ImportSqlCommandExtractor implementation if you want to handle the SQL script in a different way.",
    "page176": "Table constraints An integrity rule that applies to several columns or several rows is a table co\u00aenstraint. A typical declarative table constraint is UNIQUE: all rows are checked for duplicate values (for example, each user must have a distinct email address). A rule affecting only a single row but multiple columns is \u201cthe auction end time has to be after the auction start time.\u201d Database constraints If a rule applies to more than one table, it has database scope. You should already be familiar with th\u00aee most common database constraint, the foreign key. This rule guarantees the integrity of references betw\u00aeeen rows, usually in separate tables, but not always (self-referencing foreign key constraints aren\u2019t uncommon). Other database constraints involving several tables aren\u2019t uncommon: for example, a bid can only be stored if the auction end time of the referenced item hasn\u2019t been reached. Most (if not all) SQL database management systems support these kinds of constraints and the most important options of each. In addition to simple keywords su\u00aech as NOT NULL and UNIQUE, you can usually also declare more complex rules with the CHECK constraint that applies an arbitrary SQL expression. Still, integrity constraints are one of the weak areas in the SQL standard, and solutions from vendors can differ significantly.  Furthermore, non-declarative and procedural constraints are possible with database triggers that intercept data-modification operations\u00ae. A trig\u00aeger can then implement the constraint procedure directly or call an existing stored procedure.  Integrity constraints can be checked immediately when a data-modification statement is executed, or the check can be deferred until the end of a transaction. The violation response in SQL databases is usually rejection without any possibility of customization. Foreign keys are special because you can  typically decide what should happen with ON DELETE or ON UPDATE for referenced rows. Systems that ensure data integrity only in application code are prone to data corruption and often degrade the quality of the database over time. If the data store doesn\u2019t enforce rules, a trivial undetected application bug can cause unrecoverable problems such as incorrect or lost data.  In contrast to ensuring data \u00aeconsistency in procedural (or object-oriented) application code, database management systems allow you to implement integrity rules with declar\u00aeations, as a database schema. The advantages of declarative rules are fewer possible errors in code and a chance for the DBMS to optimize data access.  In SQL databases, we identify four kinds of \u00aerules: Domain constraints A domain is (loosely speaking, and in the database world) a data type in a database. Hence, a domain constraint defines the range of possible values a particular data type can handle. For example, an INTEGER data type is usable for integer values. A CHAR data type can hold character strings: for example, all characters defined in ASCII or some other encoding. Because we mostly use data types built-in the DBMS, we rely on the domain constra\u00aeints as defined by the vendor. If supported by your SQL database, you can use the (often limited) support for custom domains to add additional constraints for particular existing data types, or create user-defined data types (UDT). Column constraints Restricting a column to hold values of a particular domain and type creates a column constraint. For example, you declare in the schema that the EMAIL column holds values of VARCHAR type. Alternatively, you could create a new domain called EMAIL_ADDRESS with further constraints and apply it to a column instead of VARCHAR. A special column constraint in an SQL database is NOT NULL.",
    "page177": "This constraint restricts valid username values to a maximum length of 15 characters, and the string can\u2019t begin with admin to avoid confusion. You can call any SQL functions supported by your DBMS; the column Definition is always passed through into the exported schema.  Note that you have a choice: creating and using a domain or adding a single column constraint has the same effect. Domains are usually easier to maintain and avoid duplicating.  At the time of writing, Hibernate doesn\u2019t support its proprietary annotation @org.hibernate.\u00aeannotations.Check on individual properties; you use it for table level constraints. Hibernate appends table constraints to the generated CREATE TABLE statement, which can contain arbitrary SQL expressions.  You can implement multirow table constraints with expressions that are more complex. You may need a sub select in the expression to do this, which may not be supported by your DBMS. But there are some common multiro\u00aew table constraints, like UNIQUE, that you can add directly in the mappings. You\u2019ve already seen the @Column(unique true | false) option in the previous section. Now all pairs of USERNAME and EMAIL must be unique, for all rows in the USERS table. If you don\u2019t provide a name for the constraint here, UNQ_USERNAME_EMAIL an automatically generated and probably ugly name is used.  The last kinds of constraints we discuss are database-wide rules that span several tables. Hibernate passes on database constraint violations in error exceptions; check whether the exception in your transaction has a cause, somewhere in the exception chain, of type org.hibernate.exception.ConstraintViolationException. This exception can provide more information about the error, such as the name of the failed database constraint. The SQL standard includes domains, which unfortunately are rather limited and often not supported by the DBMS. If your system supports SQL domains, you can use them to add constraints to data types.  In your custom SQL create script, define an EMAIL_ADDRESS domain based on the VARCHAR data type: The additional constraint is a check of the presence of an @ symbol in the string. The (relatively minor) advantage of such domains in SQL is the abstraction of common constraints into a single location. Domain constraints are always checked immediately when data is inserted and modified. Several constraints are present in this mapping. The NOT NULL constraint is common; you\u2019ve seen it many times before. The second is a UNIQUE column constraint; users can\u2019t have duplicate email addresses. At the\u00ae time of writing, there was unfortunately no way to customize the name of this single-column unique constraint in Hibernate; it will get an ugly auto-generated name in your schema. Last, the column Definiti\u00aeon refers to the domain you\u2019ve added with your custom create script. This definition is an SQL fragment, exported into your schema directly, so be careful with database-specific SQL",
    "page178": "This statement declares the foreign key constraint for the ITEM_ID column in the BID table, referencing the primary key column of the ITEM table. You can customize the name of the constraint with the foreign Key option in an @JoinColumn mapping A foreign Key attribute is also supported in @PrimaryKeyJoinColumn, @MapKeyJoinColumn, @JoinTable, @CollectionTable, and @AssociationOverride mappings.  The @ForeignKey annotation has some rarely needed options we haven\u2019t shown: You can write your own foreign Key Definition, an SQL fragment such as FOREIGN KEY ([column]) REFERENCES [table]([column]) ON UPDATE [action]. Hibernate will use this SQL fragment instead of the provider-generated fragment, it can be in the SQL dialect supported by your DBMS.  The Constraint Mode s\u00aeetting is useful if you want to disable foreign key generation completely, with the value NO_CONSTRAINT. You can then write the foreign key constraint yourself with an ALTER TABLE statement, probably in a load script as we\u2019ve shown. Naming constraints properly is not only good practice, but also helps significantly when you have to read exception messages.  This completes our discussion of database integrity rules. Next, we look at some optimization you might want to include in your schema for performance reasons.  Indexes are a key feature when optimi\u00aezing the performance of a database application. The query optimiz\u00aeer in a DBMS can use indexes to avoid excessive scans of the data tables. Because they\u2019re relevant only in the physical implementation of a database, indexes aren\u2019t part of the SQL standard, and the DDL and available indexing options are product specific. You can, however, embed the most common schema artifacts for typical indexes in mapping metadata A user can only make bids until an auction ends. Your database should guarantee that invalid bids can\u2019t be stored so that whenever a row is inserted into the BID table, the CREATEDON timestamp of the bid is checked against the auction ending time. This kind of constraint involves two tables: BID and ITEM.  You can create a rule that spans several tables with a join in a sub select in any SQL CHECK expression. Instead of referring only to the table on which the constraint is declared, you may query (usually for the existence or nonexistence of a particular piece of information) a different table. The problem is t\u00aehat you can\u2019t use the @org.hibernate.annotations.Check annotation on either the Bid or Item class. You don\u2019t know which table Hibernate will create first.  Therefore, put your CHECK constraint into an ALTER TABLE statement that executes after all the tables have been created. A good place is the load script, because it always executes at that time A row in the BID table is now valid if its CREATEDON value is less than or equal to the auction end time of the referenced ITEM row.  By far the most common rules that span several tables are referential integrity rules. They\u2019re widely known as foreign keys, which are a combination of two things: a key valu\u00aee copy from a related row and a constraint that guarantees that the referenced value exists. Hibernate creates foreign key constraints automatically for all foreign key columns in association mappings. If you check the schema produced by Hibernate, you\u2019ll notice that these constraints also have automatically generated database identifiers names that aren\u2019t easy to read and that make debugging more difficult. You see this kind of statement in the generated schema",
    "page179": "If you encounter a USERS table in a legacy schema, it\u2019s likely that USERNAME is the primary key. In this case, you have no surrogate identifier that Hibernate generates automatically. Instead, your application has to assign the identifier value when saving an instance of the User class Hibernate calls the protected no-argument constructor when it loads a User from the database and then assigns the username field value directly. When you instantiate a User, call the public constructor with a username. If you don\u2019t declare an identifier generator on the @Id property, Hibernate expects the application to take care of the primary key value assignment.  Composite (natural) primary keys require a bit more work Suppose the primary key of the USERS table is a composite of the two columns USERNAME and DEPARTMENTNR. You write a separate composite identifier class that declares just the key properties a\u00aend call this class User Id This class has to be @Embeddable and Serializable any type used as an identifier type in JPA must be Serializable. C You don\u2019t have to mark the properties of the composite key as @NotNull; their database columns are automatically NOT NULL when embedded as the primary key of an entity. D The JPA specification requires a public no-argument constructor for an embeddable identifier class. Hibernate accepts protected visibility. E The only public constructor should have\u00ae the key values as arguments. F You have to override the equals() and hashCode() methods with the same semantics the composite key has in your database. In this case, this is a straightforward comparison of the username and department values. Many queries in Caveat Emptor will probably involve the username of a User entity. You can speed up these queries by creating an index for the column of this property. Another candidate for an index is the combination of USERNAME and EMAIL columns, which you also use frequently in queries. You can declare single or multicolumn indexes on the entity class with the @Table annotation and its indexes attribute If you don\u2019t provide a name for the index, a generated name is used.\u00ae  We don\u2019t recommend adding indexes to your schema ad hoc because it feels like an index could help with a performance problem. Get the excellent book SQL Tuning by Dan Tow (Tow, 2003) if you want to learn efficient database-optimization techniques and especially how indexes can get you closer to the best-performing execution plan for your queries.  Customizing the database schema is often possible only if you\u2019re working on a new system with no existing data. If you have to deal with an existing legacy schema, one of the most common issues is working with natural and composite keys. We mentioned in section 4.2.3 that we think natural primary keys can be a bad idea. Natural keys often make it difficult to change the data model when business requirements change. They may even, in extreme cases, impact performance. Unfortunately, many legacy schemas use (natural) composite keys heavily; and for the reason we discourage the use of composite keys, it may be difficult to change the legacy schema to use non-composite natural or surrogate keys. Therefore, JPA supports natural and composite primary and foreign keys.",
    "page180": "We\u2019ve already shown the @SecondaryTable annotation in an inheritance mapping in section 6.5. It helped to break out properties of a particular subclass into a separate table. This generic functionality has more uses but be aware that a properly designed system should have, simplified, more classes than tables.  Suppose that in a legacy schema, you aren\u2019t keeping a user\u2019s billing address information with the other user details in the USERS main entity table, but in a separate table. Figure 9.5 shows this schema. The user\u2019s home address is stored in the columns STREET, ZIPCODE, and CITY of the USERS table. The user\u2019s billing address is stored in the BILLING_ADDRESS table, which has the primary key column USER_ID, which is also a foreign key constraint referencing the ID primary key of the USERS table  The User class has two properties of embedded type: home Address and billing Address. The first is a regular embedded mapping, and the Address class is @Embeddable.  Just as in section 5.2.3, you can use the @AttributeOverrides annotation to override the mapping of embedded pr\u00aeoperties. Then, @Column maps the individual properties to the BILLING_ADDRESS table, with its table option. Remember that an @AttributeOverride replaces all mapping information for a property: any annotations on the Address fields are ignored if you override. Therefore, you have to specify nullability and length again in the @Column override.  We\u2019ve shown you a secondary table mapping example with an embeddable property. Of course, you could also break out simple basic properties like the username string in a secondary table. Keep in mind that reading and maintaining these mappings can be a problem, though; you should only map legacy unchangeable schemas with secondary tables. A foreign key constraint on the SELLER column in the ITEM table ensures that the seller of the item exists by requiring the same \u00aeseller value to be present on\u00ae some column in some row on some table. There are no other rules; the target column doesn\u2019t need a primary key constraint or even a unique const\u00aeraint. The target table can be any table. The value can be a numeric identifier of the seller or a customer number string; only the type has to be the same for the foreign key reference source and target.  Of course, a foreign key constraint usually references p\u00aerimary key column(s). Nevertheless, legacy databases sometimes have foreign key constraints that don\u2019t follow this simple rule. Sometimes a foreign key constraint references a simple unique column a natural non-primary key. Let\u2019s assume that in Caveat Emptor, So far, this is nothing special; you\u2019ve seen such a simple unique property mapping before. The legacy aspect is the SELLER_CUSTOMERN\u00aeR column in the ITEM table, with a foreign key constraint referencing the user\u2019s CUSTOMERNR instead of the user\u2019s ID You specify the referencedColumnName attribute of @JoinColumn to declare this relationship. Hibernate now knows that the referenced target column is a natural key, and not the primary key, and manages the foreign key relationship accordingly.  If the target natural key is a composite key, use @JoinColumns instead as in the previous section. Fortunately, it\u2019s often straightforward to clean up such a schema by refactoring foreign keys to reference primary keys if you can make changes to the database that don\u2019t disturb other applications sharing the data.  This completes our discussion of natural, composite, and foreign key\u2013related problems you may have to deal with when you try to map a legacy schema. Let\u2019s move on to another interesting special strategy",
    "page181": "strategies fo\u00aer runtime data management. These strategies are crucial to the performance and correct behavior of your applications.  In this chapter, we discuss the life cycle of entity instances how an instance becomes persistent, and how it stops being considered persistent and the method calls and management operations that trigger these transitions. The JPA Entity Manager is your primary interface for accessing data.  Before we look at the API, let\u2019s start with entity instances, their life cycle, and the events that trigger a change of sta\u00aete. Although some of the material may be formal, a solid understanding of the persistence life cycle is essential. Because JPA is a transparent persistence mechanism classes are unaware of their own persistence capability it\u2019s possible to write application logic that\u2019s unaware whether the data it operates on represents persistent state or temporary state that exists only in memory. The application shouldn\u2019t necessarily need to care that an instance is\u00ae persistent when invoking its methods. You can, for example, invoke the Item #calculateTotalPrice() business method without having to consider persistence at all (for example, in a unit test).  Any application with persistent state must interact with the persistence service whenever it needs to propagate state held in memory to the database (or vice versa). In other words, you have to call the Java Persistence\u00ae interfaces to store and load data.  When interacting with the persistence mechanism that way, the application must concern itself with the state and life cycle of an entity instance with respect to persistence. We refer to this as the persistence life cycle: the states an entity instance goes through during its life. We also use the term unit of work: a set of (possibly) state changing operations considered one (usually atomic) group. Another piece of the puzzle is the persistence context provided by the persistence service. In part 3, you\u2019ll load and store data with Hibernate and Java Persistence. You\u2019ll introduce the progra\u00aemming interfaces, how to write transactional applications, and how Hibernate can load data from the database most efficiently.  Starting with chapter 10, you\u2019ll learn the most important strategies for interacting with entity instances in a JPA application. You\u2019ll see the life cycle of entity instances: how they become persistent, detached, and removed. This chapter is where you\u2019ll get to know the most i\u00aemportant interface in JPA: the Entity Manager. Next, chapte\u00aer 11 defines database and system transaction essentials and how to control concurrent access with Hibernate and JPA. You\u2019ll also see non transactional data access. In chapter 12, we\u2019ll go through lazy and eager loading, fetch plans, strategies, and profiles, and wrap up with optimizing SQL execution. Finally, chapter 13 covers cascading state transitions, listening to and intercepting events, auditing and versioning with Hibernate Envers, and filtering data\u00ae dynamically.  After reading this part, you\u2019ll know how to work with Hibernate and Java Persistence programming interfaces and how to load, modify, and store objects efficiently. You\u2019ll under\u00aestand how transactions work and why conversational processing can open up new approaches for application design. You\u2019ll be ready to optimize any object-modification scenario and apply the best fetching and caching strategy to increase performance and scalability You now understand how Hibernate and ORM solve the static aspects of the object/relational mismatch. With what you know so far, you can create a mapping between Java classes and an SQL schema, solving the structural mismatch problem. For a reminder of the pr\u00aeoblems you\u2019re solving, see section 1.2.  An efficient application solution requires something more: you must investigate",
    "page182": "A persistent entity instance has a representation in the database. It\u2019s stored in the database or it will be stored when the unit of work completes. It\u2019s an instance with a database identity, as defined in section 4.2; its database identifier is set to the primary key value of the database representation.  The application may have created instances and then made them persistent by calling Entity Manager #persist(). There may be instances that became persistent when  the application created a reference to the object from another persistent instance that the JPA provider already manages. A persistent entity instance may be an instance retrieved from the database by execution of a query, by an identifier lookup, or by navigating the object graph starting from another persistent instance.  Persistent instances are always associated with a persistence context. You\u00ae see more about this in a moment You can delete a persistent entity instance from the database in several ways: For example, you can remove it with Entity Manager #remove(). It may also become available for deletion if you remove a reference to it from a mapped collection with orphan removal enabled.  An entity instance is then in the removed state: the provider will delete it at the end of a unit of work. You should discard any references you may hold to it in the application \u00aeafter you finish working with it for example, after you\u2019ve rendered the removal confirmation screen your users see Think of the persistence context as a service that remembers all the modifications and state changes you made to data in a particular unit of work (this is somewhat simplified, but it\u2019s a good starting point).  We now dissect all these terms: entity states, persistence contexts, and managed scope. You\u2019re probably more accustomed to thinking about what SQL statements you ha\u00aeve to manage to get stuff in and out of the database; but one of the key factors of your success with Java Persistence is your understanding of state management, so stick with us through this section. Different ORM solutions use different terminology and define different states and state transitions for the persistence life cycle. Moreover, the states used internally may be different from those exposed to the client application. JPA defines four states, hiding the complexity of Hibernates internal implementation from the client code. Figure 10.1 shows these states and their transitions. The state chart also includes the method calls to the Entity Manager (and Query) API that trigger transitions. We discuss this chart in this chapter; refer to it whenever you need an overview. Let\u2019s explore the states and transitions in more detail. Instances created with the new Java operator are transient, which means their state is lost and garbage-coll\u00aeected as soon as they\u2019re no longer referenced. For example, new Item() creates a transient instance of the Item class, just like new Long() and new Big Decimal(). Hibernate doesn\u2019t provide any rollback functionality for transient instances; if you modify the price of a transient Item, you can\u2019t automatically undo the change.  For an entity instance to transition from transient to persistent state, to become managed, requires either a call to the Entity Manager #persist() method or the creation of a reference from an already-persistent instance and enabled cascading of ",
    "page183": "The persistence context acts as a first-level cache; it remembers all entity instances y\u00aeou\u2019ve handled in a particular unit of work. For example, if you ask Hibernate to load an entity instance using a primary key value (a lookup by identifier), Hibernate can first check the current unit of work in the persistence context. If Hibernate finds the entity instance in the persistence context, no database hit occurs this is a repeatable read for an application. Consecutive em.find(Item. Class, ITEM_ID) calls with the same persistence context will yield the same result.  This cache also affects results of arbitrary queries, executed for example with the javax.persistence.Query API. Hibernate reads the SQL result set of a query and transforms it into entity instances. This process first tries to resolve every entity instance in the persistence context by identifier lookup. Only if an instance with the same identifier value can\u2019t be found in the current persistence context does Hibernate read the rest of the data from the result-set row. Hibernate ignores any potentially newer data in the result set, due to read-committed transaction isolation at the database level, if the entity instance is already present in the persistence context.  The persistence context cache is always on it can\u2019t be turned off. It ensures the following The persistence layer isn\u2019t vulnerable to stack overflows in the case of circular references in an object graph. There can never be conflicting representations of the same database row at the end of a unit of work. The provider can safely write all changes made to an entity instance to the database.  Likewise, changes made in a particular persistence context are always immediately v\u00aeisible to all other code executed inside that unit of work and its persistence context. JPA guarantees repeatable entity-instance reads To understand detached entity instances, consider loading an instance. You call Entity Manager #find() to retrieve an entity instance by its (known) identifier. Then you end your unit of work and close the persistence context. The application still has a handle a reference to the instance you loaded. It\u2019s now in the detached state, and the data is becoming stale. You could discard the reference and let the garbage collector reclaim the memory. Or, you could continue working with the data in the detached state and later call the merge() method to save your modifications in a new unit of work. We\u2019ll discuss detachment and merging again later in this chapter, in a dedicated section.  You should now have a basic understanding of entity instance states and their transitions. Our next topic is the persistence context: an essential service of any Java Persistence provider The persistence context allows the persistence engine to perform automatic dirty checking, detecting which entity instances the application modified. The provider then synchronizes with the database the state of instances monitored by a persistence context, either automatically or on demand. Typically, when a unit of work completes, the provider propagates state held in memory to the database through the execution of SQL INSERT, UPDATE, and DELETE statements (all part of the Data Modification Language [DML]) This flushing procedure may also occur at other times. For example, Hibernate may synchronize with the database before executio\u00aen of a query. This ensures that queries are aware of changes made earlier during the unit of work.",
    "page184": "Hibernate joins the in-progress system transaction when an Entity Manager is created and waits for \u00aethe transaction to commit. When Hibernate is notified (by the JTA engine) of the commit, it performs dirty checking of the persistence context and synchronizes with the database. You can also force dirty checking synchronization manually by calling EntityManager#flush() at any time during a transaction.  You decide the scope of the persistence context by choosing when to close() the Entity Manager. You have to close the persistence context at some point, so always place the close() call in a finally block.  How long should the persistence context be open? Let\u2019s assume for the following examples that you\u2019re writing a server, and each client request will be processed with one persistence context a\u00aend system transaction in a multithreaded environment. If you\u2019re familiar with servlets, imagine the code in listing 10.1 embedded in a servlet\u2019s service() method. Within this unit of work, you access the Entity Mana\u00aeger to load and store data. A new transient Item is instantiated as usual. O\u00aef course, you may also instantiate it before creating the EntityManager. A call to persist() makes the transient instance of Item persistent. It\u2019s now managed by and associated with the current persistence context.  To store the Item instance in the database, Hibernate has to execute an SQL INSERT statement. When the transaction of this unit of work commits, Hibernate flushes the persistence context, and the INSERT occurs at that time. Hibernate may even batch the INSERT at the JDBC level with other statements. When you call persist(), only the identifier value of the Item is assigned. Alternatively, if your identifier generator isn\u2019t pre-insert, the INSERT statement will be executed immediately when persist() is called. You may want to review section In Java SE and some EE architectures (if you only have plain s\u00aeervlets, for example), you get an Entity Manager by calling Entity Manager Factory #createEntityManager(). Your application code shares the Entity Manager Factory, representing one persistence unit, or one logical database. Most applications have only one shared  (The TM class is a convenience clas\u00aes bundled with the example code of this book. Here it simplifies the lookup of the standard User Transaction API in JNDI. The JPA class provides convenient access to the shared Entity Manager Factory.)  Everything between tx. Begin() and tx.commit() occurs in one transaction. For now, keep in mind that all database operat\u00aeions in transaction scope, such as the SQL statements executed by Hibernate, completely either succeed or fail. Don\u2019t worry too much about the transaction code for now; you\u2019ll read more about concurrency control in the next chapter. We\u2019ll look at the same examp\u00aele again with a focus on the transaction and exception-handling code. Don\u2019t write empty catch clauses in your code, though you\u2019ll have to roll back the transaction and handle exceptions.  Creating an Entity Manager starts its persistence context. Hibernate won\u2019t access the database until necessary; the Entity Manager doesn\u2019t obtain a JDBC Connection from the pool until SQL statements have to be executed. You can create and close an Entity Manager without hitting the database. Hibernate executes\u00ae SQL statements when you look up or query data and when it flushes changes detected by the persistence context to the database. Hibernate joins the in-progress system transaction when an Entity Manager is created and waits for the transaction to commit. When Hibernate is notified (by the JTA engine) of the commit, it performs dirty checking of the persistence context and synchronizes with the database.",
    "page185": "You can mod\u00aeify the Item instance, and the persistence context will detect th\u00aeese changes and record them in the database automatically. When Hibernate flushes the persistence context during commit, it executes the necessary SQL DML statements to synchronize the changes with the database. Hibernate propagates state changes to the database as late as possible, toward the end of the transaction. DML statements usually create locks in the database that are held until the transaction completes, so Hibernate keeps the lock duration in the database as short as possible.  Hibernate writes the new Item#name to the database with an SQL UPDATE. By default, Hibernate includes all columns of the mapped ITEM table in the SQL UPDATE statement, up\u00aedating unchanged columns to their old values. Hence, Hibernate can generate these basic SQL statements at startup, not at runtime. If you want to include only modified (or non-nullable for INSERT) columns in SQL statements, you can enable dynamic SQL generation as discussed in section 4.3.2.  Hibernate detects the changed name by comparing the Item with a snapshot copy it took before, when the Item was loaded from the database. If your Item is different from the snapshot, an UPDATE is necessary. This snapshot in the p\u00aeersistence context consumes memory. Dirty checking with snapshots can also be time consuming, because Hibernate has to compare all instances in the persistence context with their snapshot during flushing. It\u2019s better (but not required) to fully initialize the Item instance before managing it with a persistence context. The SQL INSERT statement contains the values that were held by the instance at the point when persist() was called. If you don\u2019t set the name of the Item before making it persistent, a NOT NULL constraint may be violated. You can modify the Item after calling persist(), and your changes will be propagated to the database with an additional SQL UPDATE statement.  If one of the INSERT or UPDATE statements made when flushing fails, Hibernate causes a rollback of changes made to persistent instances in this transaction at the database level. But Hibernate doesn\u2019t roll back in-memory changes to persistent instances. If you change the Item name after persist(), a commit failure won\u2019t roll back to the old name. This is reasonable because a failure of a transaction is normally non-recoverable, and you have to discard the failed persistence context and Entity Manager immediately. We\u2019ll discuss exception handling in the next chapter.  Next, you load and modify the stored data. You can retrieve persistent instances from the database with the EntityManager. For the next example, we assume you\u2019ve kept the identifier value of the Item stored in the previous section somewhere and are now looking up the same instance in a new unit of work by identifier  You don\u2019t need to cast the returned value of the find() operation; it\u2019s a generic method, and its return type i\u00ae\u00aes set as a side effect of the first parameter. The retrieved entity instance is in persistent state, and you can now modify it inside the unit of work.  If no persistent instance with the given identifier value can be found, fi\u00aend() returns null. The find() operation always hits the database if there was no hit for the",
    "page186": "As soon as you call any method such as Item#getName() on the proxy, a SELECT is executed to fully initialize the placeholder. The exception to this rule is a mapped database identifier getter method, such as getId(). A proxy may look like the real thing, but it\u2019s only a placeholder carrying the identifier value of the entity instance it represents. If the database record no longer exists when the proxy is initialized, an EntityNotFoundException is thrown. Note that the exception can be thrown when Item\u00ae#getName() is called. E Hibernate has a convenient static initialize() method that loads the proxy\u2019s data. F After the persistence context is closed, item is in detached state. If you don\u2019t initialize the proxy while the persistence context is still open, you get a LazyInitializationException if you access the proxy. You can\u2019t load data on demand once the persistence context is closed. The solution is simple: load the data before you close the persistence context.  We\u2019ll have much more to say about proxies, lazy loading, and on-demand fetching in ch\u00aeapter 12.  If you want to remove the state of an entity instance from the database, you have to make it transient. To make an entity instance transient and delete its database representation, call the remove() method on the EntityManager If you call find(), Hibernate executes a SELECT to load the Item. If you call getReference(), Hibernate attempts to avoid the SELECT and returns a proxy. C Calling remove() queues the entity instance for deletion when the unit of work completes; it\u2019s now in removed state. If remove() is called on a proxy, Hibernate executes a SELECT to load the data. You may want to customize how Hibernate detects dirty state, using an extension point. Set the property hibernate.entity_dirtiness_strategy in your persistence.xml configuration file to a class name that implements org.hibernate.CustomEntityDirtinessStrategy. See the Javadoc of this interface for more information. org.hibernate.Interceptor is another extension point used to customize dirty checking, by implementing its findDirty() method. You can find an example interceptor in section 13.2.2.  We mentioned earlier that the persistence context enables repeatable reads of entity instances and provides an object-identity guarantee: The first find() operation hits the database and retrieves the Item instance with a SELECT statement. The second find() is resolved in the persistence context, and the same cached Item instance is returned.  Sometimes you need an entity instance but you don\u2019t want to hit the database.  If you don\u2019t want to hit the database when loading an entity instance, because you aren\u2019t sure you need a fully initialized instance, you can tell the EntityManager to attempt the retrieval of a hollow placeholder a proxy: If the persistence context already contains an Item with the given identifier, that Item instance is returned by getReference() without hitting the databa\u00aese. Furthermore, if no persistent instance with that identifier is currently managed, H\u00aeibernate produces a hollow placeholder: a proxy. This means getReference() won\u2019t access the database, and it doesn\u2019t return null, unlike find(). C JPA offers PersistenceUnitUtil helper methods such as isLoaded() to detect whether you\u2019re working with an uninitialized proxy.",
    "page187": "Java Persistence also offers bulk operations that translate into direct SQL \u00aeDELETE statements without life cycle interceptors in the application. We\u2019ll discuss these operations in section 20.1.  Let\u2019s say you load an entity instance from the database and work with the data. For some reason, you know t\u00aehat another application or maybe another thread of your application has updated the underlying row in the database. Next, we\u2019ll see how to refresh the data held in memory. After you load the entity instance, you realize (how isn\u2019t important) that someone else changed the data in the database. Calling refresh() causes Hibernate to execute a SELECT to read and marshal a whole result set, overwriting changes you already made to the persistent instance in application memory. If the database row no longer exists (someone deleted it), Hibernate throws an EntityNotFoundException on refresh().  Most applications don\u2019t have to manually refresh in-memory state; concurrent modificati\u00aeons are typically resolved at transaction commit time.  The best use case for refreshing is with an extended persistence context, which might span sever\u00aeal request/response cycles and/or system transactions. While you wait for user input with an open persistence context, data gets stale, and selective refreshing may be required depending on the duration of the co\u00aenversation and the dialogue between the user and the system. Refreshing can be useful to undo changes made in memory during a conversation, if the user cancels the dialogue. We\u2019ll have more to say about refreshing in a conversation in section. An entity instance must be fully initialized during life cycle transitions. You may have life cycle callback methods or an entity listener enabled (see section 13.2), and the instance must pass through these interceptors to complete its full life cycle. D An entity in removed state is no longer in persistent state. You can check this with the contains() operation. E You can make the removed instance persistent again, cancelling the deletion. F When the transaction commits, Hibernate synchronizes the state transitions with the database and executes the SQL DELETE. The JVM garbage collector detects that the item is no longer referenced by anyone and finally deletes the last trace of the dat\u00aea. By default, Hibernate won\u2019t alter the identifier value of a removed entity instance. This means the item.getId() method still returns the now outdated identifier value. Sometimes it\u2019s useful to work with the \u201cdeleted\u201d data further: for example, you might want to save the removed Item again if your user decides to undo. As shown in the example, you can call persist() on a removed instance to cancel the deletion before the persistence context is flushed. Alternatively, if you set the property hibernate.use_ identifier_ rollback to true in persistence.xml, Hibernate will reset the identifier value after removal of an entity instance. In the previous code example, the identifier value is reset to the default value of null (it\u2019s a Long). The Item is now the same as in\u00ae transient state, and you can save it again in a new persistence context.",
    "page188": "The persistence context is a cache of persistent instances. Every entity instance in persistent state is associated with the persistence context.  Many Hibernate users who ignore this simple fact run into an OutOfMemoryException. This is typically the case when you load thousands of entity instances in a unit of work but never intend to modify them. Hibernate still has to create a snapshot of each instance in the persistence context cache, which can lead to memory exhaustion. (Obviously, you should execute a bulk data operation if you modify thousands of rows we\u2019ll get back to this kind of unit of work in section 20.1.)  The persistence context cache never shrinks automatically. Keep the size of your persistence context to the necessary minimum. Often, many persistent instances in your context are there by accident for example, because you needed only a few items but queried for many. Extremely large graphs can have a serious performance impact and require significant memory for state snapshots. Check that your queries return only data you need, and consider the following ways to control Hibernate\u2019s caching behavior.  You can call EntityManager#detach(i) to evict a persistent instance manually from the persistence context. You can call EntityManager#clear() to detach all persistent entity instances, leaving you with an empty persistence context.  The native Session API has some extra operations you might find useful. You can set the entire persistence context to read-only mode. This disables state snapshots and dirty checking, and Hibernate won\u2019t write modifications to the database. Replication is useful, for example, when you ne\u00aeed to retrieve data from one database and store it in another. Replication takes detached instances loaded in one persistence context and makes them persistent in another persistence context. You usually open these contexts from two different EntityManagerFactory configurations, enabling two logical databases. You have to map the entity in both configurations.  The replicate() operation is only available on the Hibern\u00aeate Session API. Here is an example that loads an Item instance from one database and copies it into another Connections to both databases can participate in the same system transaction. ReplicationMode controls the details of the replication procedure: IGNORE Ignores the instance when there is an existing database row with the same identifier in the database. OVERWRITE Overwrites any existing database row with the same identifier in the database. EXCEPTION Throws an exception if there is an existing database row with the same identifier in the target database. LATEST_VERSION Overwrites the row in the database if its version is older than the version of the given entity instance, or ignores the instance otherwise. Requires enabled optimistic concurrency contro\u00ael with entity versioning You may need replication when you reconcile data entered into different databases. An example case is a product upgrade: if the new version of your application requires a new database (schema), you may want to mig\u00aerate and replicate the existing data once.  The persistence context does many things for you: automatic dirty checking, guaranteed scope of object identity, and so on. It\u2019s equally impo\u00aertant that you know some of the details of its management, and that you sometimes \u00aeinfluence what goes on behind the scenes. ",
    "page189": "Next, we look at the detached entity state. We already mentioned some issues you\u2019ll see when entity instances aren\u2019t associated with a persistence context anymore, such as disabled lazy initialization. Let\u2019s explore the detached state with some examples, so you know what to expect when you work with data outside of a persistence context.  If a reference leaves the scope of guaranteed identity, we call it a reference to a detached entity instance. When the persistence context is closed, it no longer provides an iden\u00aetity-mapping service. You\u2019ll run into aliasing problems when you work with detached entity instances, so make sure you understand how to handle the identity of detached instances. If you look up data using the same database identifier value in the same persistence context, the result is two references to the same in-memory instance on the JVM heap. Consider the two units of work shown next. In the first unit of work at begin() B, you start by creating a persistence context C and loading some entity instances. Because references a and b are obtained f\u00aerom the same persistence context, they have the same Java identity D. They're equal from the same persistence context, they have the same Java identity D. They're equa\u00ael E because by default equals() relies \u00aeon Java identity comparison. They obviously have the same database identity F. They reference the same Item instance, in persistent state, managed by the persistence context fo\u00aer that unit of work. The first part of this example finishes by committing the transaction. By default, Hibernate flushes the persistence context of an EntityManager and synchronizes changes with the database whenever the joined transaction is committed. All the previous code examples, except some in the \u00aelast section, have used that strategy. JPA allows implementations to synchronize the persistence context at other times, if the\u00aey wish.  Hibernate, as a JPA implementation, synchronizes at the following times:  When a joined JTA system transaction is committed  Before a query is executed we don\u2019t mean lookup with find() but a query with javax.persistence.Query or the similar Hibernate API  When the application calls flush() explicitly Here, you load an Item instance B and change its name C. Then you query the database, retrieving the item\u2019s name D. Usually Hibernate recognizes that data has changed in \u00aememory and synchronizes these modificatio\u00aens with the database before the query. This is the behavior of FlushModeType.AUTO, the default if you join the EntityManager with a transaction. With FlushModeType.COMMIT, you\u2019re disabling flushing before queries, so you may see different data returned by the query than what you have in memory. The synchronization then occurs only when the transaction commits.  You can at any time, while a transaction is in progress, force dirty checking and synchronization with the database by calling EntityManager#flush(). This concludes our discussion of the transient, persistent, and removed entity states, and the ba\u00aesic usage of the EntityManager API. Mastering these state transitions and API methods is essential; every JPA application is built with these operations.",
    "page190": "When you begin a journey, it\u2019s a good idea to have a mental map of\u00ae the terrain you\u2019ll be passing through. The same is true for an intellectual journey, such as learning to write computer programs. In this case, you\u2019ll need to know the basics of what computers are and how they work. You\u2019ll want to have some idea of what a computer program is and how one is created. Since you will be writing programs in the Java programming language, you\u2019ll want to know something about that language in particular and about the modern, networked computing environment for which Java is designed. As you read this chapter, don\u2019t worry if you can\u2019t understand everything in detail. (In fact, it would be impossible for you to learn all the details from the brief expositions in this chapter.) Concentrate on learning enough about the big ideas to orient yourself, in preparation for the rest of the book. Most of what is covered in this chapter will be covered in much greater detail later in the book. A computer is a complex system consisting of many different components. But at the heart or the brain, if you want of the computer is a single component that does the actual computing. This is the Central Processing Unit, or CPU. In a modern desktop computer, the CPU is a single \u201cchip\u201d on the order of one square inch in size. The job of the CPU is to execute programs. A program is simply a list of unambiguous in\u00aestructions meant to be followed mechanically by a computer. A computer is built to carry out instructions that are written in a very simple type of language called machine language. Each type of computer has its own machine language, and the computer can directly execute a program only if the program is expressed in that l\u00aeanguage. (It can execute programs written in other languages if they are first translated into machine language.) When the CPU executes a program, that program is stored in the computer\u2019s main memory (also called the RAM or random access memory). In addition to the program, memory can also hold data that is being used or processed by the program. Main memory consists of a sequence of locations. These locations are numbered, and the sequence number of a location is called its address. An address provides a way of picking out one particular piece of information from among the millions stored in memory. When the CPU needs to access the program instruction or data in a particular location, it sends the address of that information as a signal to the memory; the memory responds by sending back the data contained in the specifie location. The CPU can also store information in memory by specifying the information to be stored and the address of the location where it is to be stored. On the level of machine language, the operation of the CPU is fairly straightforward (although it is very complicated in detail). The CPU executes a program that is stored as a sequence of machine lang\u00aeuage instructions in main memory. It does this by repeatedly reading, or fetching, an instruction from memory and then carrying out, or executing, that instruction. This process fetch an instruction, execute it, fetch another instruction, execute it, and so on forever is called th\u00aee fetch-and-execute cycle. With one exception, which will be covered in the next section, this is all that the CPU ever does. The details of the fetch-and-exec\u00aeute cycle are not terribly important, but there are a few basic things you should know. The CPU contains a few internal registers, which are small memory units capable of holding a single number or machine language instruction. The CPU uses one of these registers the program counter, or PC to keep track of\u00ae where it is in the program it is executing. The PC stores the address of the next instruction that the CPU should execute. At the beginning of each fetch-and-execute cycle, the CPU checks the PC to see which instruction it should fetch. During the course of the fetch-and-execute cycle, the number in the PC is updated to indicate the instruction \u00aethat is to be executed in the next cycle. A computer executes machine language programs mechanically that is without understanding them or thinking about them simply because of the way it is physically put together. This is not an easy concept. A computer is a machine built of millions of tiny \u00aeswitches called transistors, which have the property that they can be wired together in such a way that an output from one switch can turn another switch on or off. As a computer computes, these switches turn each other on or off in a pattern determined both by the way they are wired together and by the program that the computer is executing.",
    "page191": "Machine language instructions are expressed as binary numbers. A binary number is made up of just two possible digits, zero and one. So, a machine language instruction is just a sequence of zeros and ones. Each particular sequence encodes some particular instruction. The data that the computer manipulates is also encoded as binary numbers. A computer can work directly with binary numbers because switches can readily represent such numbers: Turn the switch on to represent a one; turn it off to represent a zero. Machine language instructions are stored in memory as patterns of switches turned on or off, When a machine language instruction is loaded into CPU, all that happens is that certain switches are turned on or off in the pattern that encodes that particular instruction. The CPU is built to respond to this pattern by executing the instruction it encodes; it does this simply because of the way all the other switches in the CPU are wired together. So, you should understand this much about how computers work: Main memory holds machine language programs and data. These are encoded as binary numbers. The CPU fetches machine language instructions from memory \u00aeone after another and executes them. It does this mechanically, without thinking about or understanding what it does-and therefore the program it executes must be perfect, complete in all details, and unambiguous because the CPU can do nothing but execute it exactly as written. Here is a schematic view of this first-stage understanding of the computer Asynchronous Events: Polling Loops and Interrupts The CPU spends almost all of its time fetching instructions from memory and executing them. However, the CPU and main memory are only two out of many components in areal computer system, A complete system contains other devices such as: A hard disk for storing programs and data files.(Note that main memory holds only a comparatively small amount of information, and holds it only as long as the power is turned on. A hard disk is necessary for permanent storage of larger amounts of information, but programs have to be loaded from disk into main memory before they can actually be executed.) A keyboard and mouse for user input. A monitor and printer which can be used to display the computer's output. A modem that allows the computer to communicate with other computers over telephone lines. A network interface that allows the computer to communicate with other computers that are connected to it on a network. A scanner that converts images into coded binary numbers that can be stored and manipulated on the computer. The list of devices is entirely open ended, and computer systems are built so that they can easily be expanded by adding new devices. Somehow the CPU has to communicate with and control all these devices. The CPU can only do this by executing machine language instructions (which is all it can do, period). The way this works is that for each device in a system, there is a device driver, which consists of software that the CPU executes when it has to deal with the device. Installing a new device on a system generally has two steps: plugging the device physically into the computer, and installing the device driver software. Without the device driver, the actual physical device would be useless, since the CPU would not be able to communicate with it. A computer system consisting of many devices is typically organized by connecting those devices to one or more busses. A bus is a set of wires that carry various sorts of information between the devices connected to \u00aethose wires. The wires carry data, addresses, and controls signals. An address directs the data to a particular device and perhaps to a particular register or location within that device. Control signals can be used, for example, by one device to alert another that data is available for it on the data bus. A fairly simple computer system might be organized like this Now, devices such as keyboard, mouse, and network interface can produce input that ne\u00aeeds to be processed by the CPU. How does the CPU know that the data is there? One simple idea, which turns out to be nit very satisfactory, is for the CPU to keep checking for incoming data over and over. Whenever it finds data, it processes it. This method is called polling, since the CPU polls the input devices continually to see whether they have any input data to report. Un\u00aefortunately, although polling is very simple, it is also very inefficient. The CPU can waste an awful lot of time just waiting for input. To avoid this inefficiency, interrupts are often us\u00aeed instead of polling. An interrupt is a signal sent by another device to the CPU. The CPU responds to an interrupt signal by putting aside whatever it is doing in order to respond to the interrupt. Once it has handled the interrupt, it returns to what it was doing before th\u00aee interrupts occurred. For example, when you press a key on your computer keyboard, a keyboard interrupt is sent to the CPU. The CPU responds to this signal by inter\u00aerupting what it is doing, reading the key \u00aethat you pressed, processing it, and returning to the task it was performing before you pressed the key.",
    "page192": "Again, you should understand that this is a purely mechanical process: A device signals an interrupt simply by turning on a wire. The CPU is built so that when that wire is turned on, the CPU saves enough information about what it is currently doing so that it can return to the same state later. This information consists of the contents of important internal registers such as the program counter. Then the CPU jumps to some predetermined memory location and begins executing the instructions stored there. Those instructions make up an interrupt handler that does the processing necessary to respond to the interrupt. (This interrupt handler is part of the device driver software for the device that signalled the interrupt.) At the end of the interrupt handler is an instruction that tells the CPU to jump back to what\u00ae it was doing; it does that by restoring its previously saved state. Interrupts allow the CPU to deal with asynchronous events. In the regular fetch-and execute cycle, things happen in a predetermined order; everything that happens is \u201csynchronized\u201d with everything else. Interrupts make it possible for the CPU to deal efficiently with events that happen \u201casynchronously,\u201d that is, at unpredictable times. As another example of how interrupts are used, consider what happens when the CPU need to access data that is stored on the hard disk. The CPU can access data directly only if it is in main memory. Data on the disk has to be copied into memory before it can be accessed. Unfortunately, on the scale of speed at which the CPU operates, the disk drive is extremely slow. W\u00aehen the CPU needs data from the disk, it sends a signal to the disk drive telling it to locate the data and get it ready. (This signal is sent synchronously, under the control of a regular program.) Then, instead of just waiting the long and unpredictalble amount of time that the disk drive will take to do this, the CPU goes on with some other task. When the disk drive has the data ready, it sends an interrupt signal to the CPU. The interrupt handler can then read the requested data. Now, you might have noticed that all this only makes sense if the CPU actually has several tasks to perform. If it has nothing better to do, it might as well spend its time polling for input or waiting for disk drive operations to complete. All modern computers use multitasking to perform several tasks at once. Some computers can be used by several people at once. Since the CPU is so fast, it can quickly switch its attention from one us\u00aeer to another, devoting a fraction of a second to each user in turn. This application of multitasking is called timesharing. But a modern personal computer with just a single user also uses multitasking. For example, the user might be typing a paper while a clock is continuously displaying the time and a file is being downloaded over the network. Each of the individual tasks that the CPU is working on is called a thread. (Or a process; there are technical differences between threads and processes, but they are not important here.) At any given time, only one thread can actually be executed by a CPU. The CPU will continue running the same thread until one of several things happens The thread might voluntarily yield control, to give other threads a chance to run. The thread might have to wait for some asynchronous event to occur. For example, the thread might request some data from the disk drive, or it might wait for the user to press a key. While it is waiting, the thread is said to be blocked, and other threads have a chance to run. When the event occurs, an interrupt will \u201cwake up\u201d the thread so that it can continue running. The thread mi\u00aeght use up its allotted slice of time and be suspended to allow other threads to run. Not all computers can \u201cforcibly\u201d suspend a thread in this way; those that can are said to use preemptive multitasking. To do preemptive multitasking, a computer needs a special timer device that generates an interrupt at regular intervals, such as 100 times per second. When a timer interrupt occurs, the CPU has a chance to switch from one thread to another, whether the thread that is currently running likes it or not. Ordinary users, and indeed ordinary programmers, have no need to deal with interrupts and interrupt handlers. They can concentrate on the different tasks or threads that they want the computer to perform; the details of how the computer manages to get all those tasks done are not important to them. In fact, most users, and many programmers, can ignore threads and multitasking altogether. However, threads have become increasingly important as computers have become more powerful and as they have begun to make more use of multitasking. Indeed, threads are built into the Java programming language as a fundamental programming concept. Just as important in Java and in modern programming in general is the basic concept of asynchronous events. While programmers don\u2019t actually deal with interrupts directly, they do often find themselves writing event handlers, which, like interrupt handlers, are called asynchronously when specified events occur. Such \u201cevent-driven programming\u201d has a very different feel from the more traditional straight-through, synchronous programming. We will begin with the more traditional type of programming, which is still used for programming individual tasks, but we will return to threads and events later in the text. By the way, the software that does all the interrupt handling and the communication with the user and with hardware devices is called the operating system. The operating system is the basic, essential software without which a computer would not be able to function. Other programs, such as word processors and World Wide Web browsers, are dependent upon the operating system. Common operating systems include Linux, DOS, Windows 2000, Windows XP, and the Macintosh OS",
    "page193": "The Java Virtual Machine Machine language consists of very simple instructions that can be executed directly by the CPU of a computer. Almost all programs, though, are written in high-level programming languages such as Java, Pascal, or C++. A program written in a high-level language cannot be run directly on any computer. First, it has to be translated into machine language. This translation can be done by a program called a compiler. A compiler takes a high-level-language program and translates it into an executable machi\u00aene-language program. Once the translation is done, the machine-language program can be run any number of times, but of course it can only be run on one type of computer (since each type of computer has its own individual machine language). If the program is to run on another type of computer it has to be re-translated, using a different compiler, into the appropriate machine language. There is an alternative to compiling a high-level language program. Instead of using a compiler, which translates the program all at once, you can use an interpreter, which translates it instruction-by-instruction, as necessary. An interpreter is a program that acts much like a CPU, with a kind of fetch-and-execute cycle. In order to execute a program, the interpreter runs in a loop in which it repeatedly reads one instruction from the program, decides what is necessary to carry out that instruction, and then performs the appropriate machine-language commands to do so. one use of interpreters is to execute high-level language programs. For example, the programming language Lisp is usually executed by an interpreter rather than a compiler. However, interpreters have another purpose: they can let you use a machine-language program meant for one type of computer on a completely different type of computer. For example, there is a program called \u201cVirtual PC\u201d that runs on Macintosh computers. Virtual PC is an interpreter that executes machine-language programs written for IBM-PC-clone computers. If you run Virtual PC on your Macintosh, you can run any PC program, including programs written for Windows. (Unfortunately, a PC program will run much more slowly than it would on an actual IBM clone. The problem is that Virtual PC executes several Macintosh machine-language instructions for each PC machine-language instruction in the program it is interpreting. Compiled programs are inherently faster than interpreted programs. The designers of Java chose to use a combination of compilation and interpretation. Programs written in Java are compiled into machine language, but it is a machine language for a computer that doesn\u2019t really exist. This so-called \u201cvirtual\u201d computer is known as the Java virtual machine. The machine language for the Java virtual machine is called Java bytecode. There is no reason why Java bytecode could not be used as the machine language of a real computer, rather than a virtual computer. However, one of the main selling points of Java is that it can actually be used on any computer. All that the computer needs is an interpreter for Java bytecode. Such an interpreter simulates the Java virtual machine in the same way that Virtual PC simulates a PC computer. Of course, a different Java bytecode interpreter is needed for each type of computer, but once a computer has a Java bytecode interpreter, it can run any Java bytecode program. And the same Java bytecode program can be run on any computer that has such an interpreter. This is one of the essential features of Java: the same compiled program can be run on many different types of computers. Why, you might wonder, use the intermediate Java bytecode at all? Why not just distribute the original Java program and let each person compile it into the machine language of whatever computer they want to run it on? There are many reasons. First of all, a compiler has to understand Java, a complex high-level language. The compiler is itself\u00ae a complex program. A Java bytecode interpreter, on the other hand, is a fairly small, simple program. This makes it easy to write a bytecode interpreter for a new type of computer; once that is done, that computer can run any compiled Java program. It would be much harder to write a Java compiler for the same computer. Furthermore, many Java programs are meant to be downloaded over a network. This leads to obvious security concerns: you don\u2019t want to download and run a program that will damage your computer or your files. The bytecode interpreter acts as a buffer between you and the program you download. You are really running the interpreter, which runs the downloaded program indirectly. The interpreter can protect you from potentially dangerous actions on the part of that program. i should note that there is no necessary connection between Java and Java bytecode. A program written in Java could certainly be compiled into the machine language of a real computer. And programs written in other languages could be compiled into Java bytecode. However, it is the combination of Java and Java bytecode that is platform-independent, secure, and network compatible while allowing you to program in a modern high-level object-oriented language. I should also note that the really hard part of platform-independence is providing a \u201cGraphical User Interface\u201d with windows, butt\u00aeons, etc. that will work on all the platforms that support Java.",
    "page194": "Programs must be designed. No one can just sit down at the computer and compose a program of any complexity. The discipline called software engineering is concerned with the construction of correct, working, well-written programs. The software engineer tends to use accepted and proven methods for analyzing the problem to be solved and for designing a program to solve that problem. During the 1970s and into the 80s, the primary software engineering methodology was structured programming. The structured programming approach to program design was based on the following advice: To solve a large problem, break the problem into several pieces and work on each piece separately; to solve each piece, treat it as a new problem which can itself be broken down into smaller problems; eventually, you will work your way down to problems that can be solved directly, without further decomposition. This approach is called top-down programming. There is nothing wrong with top-down programming. It is a valuable and often-used approach to problem-solving. However, it is incomplete. For one thing, it deals almost entirely with producing the instructions necessary to solve a problem. But as time went on, people realized that the design of the data structures for a program was as least as important as the design of subroutines and control structures. Top-down programming doesn\u2019t give adequate consideration to the data that the program manipulates. Another problem with strict top-down programming is that it makes it difficult to reuse work done for other projects. By starting with a particular problem and subdividing it into convenient pieces, top-down programming tends to produce a design that is unique to that problem. It is unlikely that you will be able to take a large chunk of programming from another program and fit it into your project, at least not without extensive modification. Producing hig\u00aeh-quality programs is difficult and expensive, so programmers and the people who employ them are always eager to reuse past work. So, in practice, top-down design is often combined with bottom-up design. In bottom-up design, the approach is to start \u201cat the bottom,\u201d with problems that you already know how to solve (and for which you might already have a reusable software component at hand). From there, you can work upwards towards a solution to the overall probl\u00aeem. The reusable components should be as \u201cmodular\u201d as possible. A module is a component of a larger system that interacts with the rest of the system in a simple, well-defined, straightforward manner. The idea is that a module can be \u201cplugged into\u201d a system. The details of what goes on inside the module are not important to the system as a whole, as long as the module fulfills its assigned role corre\u00aectly. This is called information hiding, and it is one of \u00aethe most important principles of software engineering. One common format for software modules is to contain some data, along with some subroutines for manipulating that data. For example, a mailing-list module might contain a list of names and addresses along with a subroutine for adding a new name, a subroutine for printing mailing labels, and so forth. In such modules, the data itself is often hidden inside the module; a program that uses the module can then manipulate the data only indirectly, by calling the subroutines provided by the module. This protects the data, since it can only be manipulated in known, well-defined ways. And it makes it easier for programs to use the module, since they don\u2019t have to worry about the details of how the data is represented. Information about the representation of the data is hidden. Modules that could support this kin\u00aed of information-hiding became common in programmin\u00aeg languages in the early 1980s. Since then, a more advanced form of the same idea has more or less taken over software engineering. This latest approach is called object-oriented programming, often abbreviated as OOP. The central concept of object-oriented programming is the object, which is a kind of module containing data and subroutines. The point-of-view in OOP is that an object is a kind of self sufficient entity that has an internal state (the data it contains) and that can respond to messages (calls to its subroutines). A mailing list object, for example, has a state consisting of a list of names and addresses. If you send it a message telling it to add a name, it will respond by modifying its state to reflect the change. If you send it a message telling it to print itself, it will respond by printing out its list of names and addresses. The OOP approach to software engineering is to start by identifying the objects involved in a problem and the messages that those objects should respond to. The program that results is a collection of objects, each with its own data and its own set of responsibilities. The objects interact by sending messages to each other. There is not much \u201ctop-down\u201d in such a program, and people used to more traditional programs can have a hard time getting used to OOP. However, people who use OOP would claim that object-oriented programs tend to be better models of the way the world itself works, and that they are therefore easier to write, easier to understand, and more likely to be correct. You should think of objects as \u201cknowing\u201d how to respond to certain messages. Different objects might respond to the same message in different ways. For example, a \u201cprint\u201d message would produce very different results, depending on the object it is sent to. This property of objects that different objects can respond to the same message in different ways is called polymorphism. It is common for objects to bear a kind of \u201cfamily resemblance\u201d to one another. Objects that contain the same type of data and that respond to the same messages in the same way belong to the same class. (In actual programming, the class is primary; that is, a class is created and then one or more objects are created using \u00aethat class as a template.) But objects can be similar without being in exactly the same class.",
    "page195": "For example, consider a drawing program that lets the user draw lines, rectangles, ovals, polygons, and curves on the screen. In the program, each visible object on the screen could be represented by a software object in the program. There would be five classes of objects in the program, one for each type of visible object that can be drawn. All the lines would belong to one class, all the rectangles to another class, and so on. These classes are obviously related; all of them represent \u201cdrawable objects.\u201d They would, for example, all presumably be able to respond to a \u201cdraw yourself\u201d message. Another level of grouping, based on the data  to represent each type of object, is less obvious, but would be very useful in a program: We can group polygons and curves together as \u201cmultipoint objects,\u201d while lines, rectangles, and ovals are \u201ctwo-point objects.\u201d (A line is determined by its endpoints, a rectangle by two of its corners, and an oval by two corners of the rectangle that contains it.) We could diagram these relationships as follows DrawableObject, MultipointObject, and TwoPointObject would be classes in the program. MultipointObject and TwoPointObject would be subclasses of DrawableObject. The class Line would be a subclass of TwoPointObject and (indirectly) of DrawableObject. A subclass of a class is said to inherit the properties of that class. The subclass can add to its inheritance and it can even \u201coverride\u201d part of that inheritance (by defining a different response to some method). Nevertheless, lines, rectangles, and so on are drawable objects, and the class DrawableObject expresses this relationship. Inheritance is a powerful means for organizing a program. It is also related to the problem o\u00aef reusing software components. A class is the ultimate reusable component. Not only can it be reused directly if it fits exactly into a program you are trying to write, but if it just almost fits, you can still reuse it by defining a subclass and making only the small changes necessary to adapt it exactly to your needs. So, OOP is meant to be both a superior program-development tool and a partial solution to the software reuse problem. Objects, classes, and object-oriented programming will be important themes throughout the rest of this text. The Modern User Interface - When computers were first introduced, ordinary people including most programmers couldn\u2019t get near them. They were locked up in rooms with white-coated attendants who would take your programs and data, feed them to the computer, and return the computer\u2019s response some time later. When timesharing where the computer switches its attention rapidly from one person to another was invented in the 1960s, it became possible for several people to interact directly with the computer at the same time. On a timesharing system, users sit at \u201cterminals\u201d where they type commands to the computer, and the computer types back its\u00ae response. Early personal computers also used typed commands and responses, except that there was only one person involved at a time. This type of interaction between a user and a computer is called a command-line interface. Today, of course, most people interact with computers in a completely different way. They use a Graphical User Interface, or GUI. The computer draws interface components on the screen. The components include things like windows, scroll bars, menus, buttons, and icons. Usually, a mouse is used to manipulate such components. Assuming that you have not just been teleported in from the 1970s, you are no doubt already familiar with the basics of graphical user interfaces! A lot of GUI interface components have become fairly standard. That is, they have similar appearance and behavior on many different computer platforms including Macintosh, Windows, and Linux. Java programs, which are supposed to run on many different platforms without modification to the program, can use all the standard GUI components. They might vary a little in appearance from platform to platform, but their functionality should be identical on any computer on which the program runs. Shown below is an image of a very simple Java program actually an \u201capplet\u201d, since it is meant to appear on a Web page that shows a few standard GUI interface components. There are four components that the user can interact with: a button, a checkbox, a text field, and a pop-up menu. These components are labeled. There are a few other components in the applet. The labels themselves are components (even though you can\u2019t interact with them). The right half of the applet is a text area component, which can display multiple lines of text, and a scrollbar component appears along\u00aeside the text area when the number of lines of text becomes larger than will fit in the text area. And in fact, in Java terminology, the whole applet is itself considered to be a \u201ccomponent. \u201dNow, Java actually has two complete sets of GUI components. One of these, the AWT or Abstract Windowing Toolkit, was available in the original version of Java. The other, which is known as Swing, is included in Java version 1.2 or later, and is used in preference to the AWT in most modern Java programs. The apple\u00aet that is shown above uses components that are part of Swing. If your Web browser uses an old version of Java, you might get an error when the browser tries to load the applet. Remember that most of the applets in this textbook require Java 5.0 (or higher). When a user interacts with the GUI components in this applet, an \u201cevent\u201d is generated. For example, clicking a push button generates an event, and pressing return while typing in a text field generates an event. Each time an event is generated, a message is sent to the applet telling it that the event has occurred, and the applet responds according to its program. In fact, the program consists mainly of \u201cevent handlers\u201d that tell the applet how to respond to various types of events. In this example, the applet has been programmed to respond to each event by displaying a message in the text area.",
    "page196": "The use of the term \u201cmessag\u00aee\u201d here is deliberate. Messages, as you saw in the previous section, are sent to objects. In fact, Java GUI components are implemented as objects. Java includes many predefined classes that represent various types of GUI components. Some of these classes are subclasses of others. Here is a diagram showing some of Swing\u2019s GUI classes and their relationships. Don\u2019t worry about the details for now, but try to get some feel about how object-oriented programming and inheritance are used here. Note that all the GUI classes are subclasses, directly or indirectly, of a class called JComponent, which represents general properties that are shared by all Swing components. Two of the direct subclasses of JComponent themselves have subclasses. The classes JTextArea and JTextField, which have certain behaviors in common, are grouped together as subclasses of JTextComponent. Similarly JButton and JToggleButton are subclasses of JAbstractButton, which represents properties common to both buttons and checkboxes. (JComboBox, by the way, is the Swing class that represents pop-up menus.) Just from this brief discussion, perhaps you can see how GUI programming can make effective use of object-oriented design. In fact, GUI\u2019s, with their \u201cvisible objects,\u201d are probably a major factor contributing to the popularity of OOP. The Internet and the World-Wide Web - Computers can be connected together on networks. A computer on a network can communicate with other computers on the same network by exchanging data and files or by sending and receiving messages.\u00ae Computers on a network can even work together on a large computation. Today, millions of computers throughout the world are connected to a single huge network called the Internet. New computers are being connected to the Internet every day. Computers can join the Internet by using a modem to establish a connection through telephone lines. Broadband connections to the Internet, such as DSL and cable modems, are increasingly common. They allow faster data transmission than is possible through telephone modems. There are elaborate protocols for communication over the Internet. A protocol is simply a detailed specification of how communication is to proceed. For two computers to communicate at all, they must both be using the same protocols. The most basic protocols on the Internet are the Internet Protocol (IP), which specifies how data is to be physically transmitted from one computer to another, and the Transmission Control Protocol (TCP), which ensures that data sent using IP is received in its entirety and without error. These two protocols, which are referred to collectively as TCP/IP, provide a foundation for communication. Other protocols use TCP/IP to send specific types of information such as web pages, electronic mail, and data files. All communication over the Internet is in the form of packets. A packet consists of some data being sent from one computer to another, along with addressing information that indicates where on the Internet that data is supposed to go. Think of a packet as an envelope with an address on the outside and a message on the inside. (The message is the data.) The\u00ae packet also includes a \u201creturn address,\u201d that is, the address of the sender. A packet can hold only a limited amount of data; longer messages must be divided among several packets, which are then sent individually over the net and reassembled at their destination. Every computer on the Internet has an IP address, a number that identifies it uniquely among all the computers on the net. The IP address is used for addressing packets. A computer can only send data to another computer on the Internet if it knows that computer\u2019s IP address. Since people prefer to use names rather than numbers, most computers are also identified by names, called domain names. For example, the main computer of the Mathematics Department at Hobart and William Smith Colleges has the domain name math.hws.edu. (Domain names are just for convenience; your computer still needs to know IP addresses before it can communicate. There are computers on the Internet whose job it is to translate domain names to IP addresses. When you use a domain name, your computer sends a message to a domain name server to find out the corresponding IP address. Then, your computer uses the IP address, rather than the domain name, to communicate with the other computer.) The Internet provides a number of services to the computers connected to it (and, of course, to the users of those computers). These services use TCP/IP to send various types of \u00aedata over the net. Among the most popular services are instant messaging, file sharing, electronic mail, and the World-Wide Web. Each service has its own protocols, which are used to control transmission of data over the network. Each service also has some sort of user interface, which allows the user to view, send, and receive data through the service. For example, the email service uses a protocol known as SMTP (Simple Mail Transfer Protocol) to transfer email messages from one computer to another. Other protocols, such as POP and IMAP, are used to fetch messages from an email account so that the recipient can read them. A person who uses email, however, doesn\u2019t need to understand or even know about these protocols. Instead, they are used behind the scenes by the programs that the person uses to send and receive email messages. These programs provide an easy-to-use user interface to the underlying network protocols. The World-Wide Web is perhap\u00aes the most exciting of network services. The World-Wide Web allows you to request pages of information that are stored on computers all over the Internet. A Web page can contain links to other pages on the same computer from which it was obtained or to other computers anywhere in the world. A computer that stores such pages of information is called a web server. The user interface to the Web is the type of program known as a web browser. Common web browsers include Internet Explorer and Firefox. You use a Web browser to request a page of information. The browser will send a request for that page to the computer on which the page is stored, and when a response is received from that computer, the web browser displays it to you in a neatly formatted form. A web br\u00aeowser is just a user interface to the Web. Behind the scenes, the web browser uses a protocol called HTTP (HyperText Transfer Protocol) to send each page request and to receive the response from the web server.",
    "page197": "On a basic level (the level of machine language), a computer can perform only very simple operations. A computer performs complex tasks by stringing together large numbers of such operations. Such tasks must be \u201cscripted\u201d in complete and perfect detail by programs. Creating complex programs will never be really easy, but the difficulty can be handled to some extent by giving the program a clear overall structure. The design of the overall structure of a program is what I call \u201cprogramming in the large.\u201d Programming in the small, which is sometimes called coding, would then refer to filling in the details of that design. The details are the explicit, step-by-step instructions for \u00aeperforming fair\u00aely small-scale tasks. When you do coding, you are working fairly \u201cclose to the machine,\u201d with some of the same concepts that you might use in machine language: memory locations, arithmetic operations, loops and branches. In a high-level language such as Java, you get to work with these concepts on a level several steps above machine language. However, you still have to worry about getting all the details exactly right. This chapter and the next examine the facilities for programming in the small in the Java programming language. Don\u2019t be misled by the term \u201cprogramming in the small\u201d into thinking that this material is easy or unimportant. This material is an essential foundation for all types of programming. If you don\u2019t understand it, you can\u2019t write programs, no matter how good you get at designing their large-scale structure. The Basic Java Application - A program is a sequence of instructions that a computer can execute to perform some task. A simple enough idea, but for the computer to make any use of the instructions, they must be written in a form that the computer can use. This means that programs have to be written in programming languages. Programming languages differ from ordinary human languages in being completely unambiguous and very strict about what is and is not allowed in a program. The rules that determine what is allowed are called the syntax of the language. Syntax rules specify the basic vocabulary of the language and how programs can be constructed using things like loops, branches, and subroutines. A syntactically correct program \u00aeis one that can be successfully compiled or interpreted; programs that have syntax errors will be rejected (hopefully with a useful \u00aeerror message that will help you fix the problem). So, to be a successful programmer, you have to develop a detailed knowledge of the syntax of the programming language that you are using. However, syntax is only part of the story. It\u2019s not enough to write a program that will run you want a program that will run and produce the correct result! That is, the meaning of the program has to be right. The meaning of a program is referred to as its semantics. A semantically correct program is one that does what you want it to. Furthermore, a program can be syntactically and semantically correct but still be a pretty bad program. Using the language correctly is not the same as using it well. For example, a good program has \u201cstyle.\u201d It is written in a\u00ae way that will make it easy for people to read and to understand. It follows conventions that will be familiar to other programmers. And it has an overall design that will make sense to human readers. The computer is completely oblivious to such things, but to a human reader, they are paramount. These aspects of programming are sometimes referred to as pragmatics. When I \u00aeintroduce a new language feature, I will explain the syntax, the semantics, and some of the pragmatics of that feature. You should memorize the syntax; that\u2019s the easy part. Then you should get a feeling f\u00aeor the semantics by following the examples given, making sure that you understand how they work, and maybe writing short programs of your own to test your understanding. And you should try to appreciate and absorb the pragmatics this means learning how to use the language feature well, with style that will earn you the admiration of other programmers. Of course, even when you\u2019ve become familiar with all the individual features of the language, that doesn\u2019t make you a programmer. You still have to learn how to construct complex programs to solve particular problems. For that, you\u2019ll need both experience and taste. You\u2019ll find hints about software development throughout this textbook. We begin our exploration of Java with the problem that has become traditional for such beginnings: to write a program that displays the me\u00aessage \u201cHello World!\u201d. This might seem like a trivial problem, but getting a computer to do this is really a big first step in learning a new programming language (especially if it\u2019s your first programming language). It means that you understand the basic process of: 1. getting the program text into the computer, 2. compiling the program, and 3. running the compiled program. The first time through, each of these steps will probably take you a few tries to get right. I won\u00ae\u2019t go into the details here of how you do each of these steps; it depends on the particular computer and Java programming environment that you are using. See Section 2.6 for information about creating and running Java programs in specific programming environments. But in general, you will type the program using some sort of text editor and save the program in a file. Then, you will use some command to try to compile the file. You\u2019ll either get a message that the program contains syntax errors, or you\u2019ll get a compiled version of the program. In the case of Java, the program is compiled into Java bytecode, not into machine language. Finally, you can run the compiled program by giving some appropriate command. For Java, you will actually use an interpreter to execute the Java bytecode. Your pr\u00aeogramming environment might automate some of the steps for you, but you can be sure that the same three steps are being done in the background.",
    "page198": "Here is a Java program to display th\u00aee message \u201cHello World!\u201d. Don\u2019t expect to understand what\u2019s going on here just yet some of it you won\u2019t really understand until a few chapters from now: // A program to display the message // \\\"Hello World!\\\" on standard output public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello World!\\\"); } } // end of class HelloWorld The command that actually displays the message is: System.out.println(\\\"Hello World!\\\"); This command is an example of a subroutine call st\u00aeatement. It uses a \u201cbuilt-in subroutine\u201d named System.out.println to do the actual work. Recall that a subroutine consists of the instructions for performing some task, chunked together and given a name. That name can be used to \u201ccall\u201d the subroutine whenever that task needs to be performed. A built-in subroutine is one that is already defined as part of the language and therefore automatically available for use in any program. When you run this program, the message \u201cHello World!\u201d (without the quotes) will be displayed on standard output. Unfortunately, I can\u2019t say exactly what that means! Java is meant to run on many different platforms, and standard output will mean different things on different platforms. However, you can expect the message to show up in some convenient place. (If you use a command-line interface, lik\u00aee that in Sun Microsystem\u2019s Java Development Kit, you type in a command to tell the computer to run the program. The computer will type the output from the program, Hello World!, on the next line.) You must be curious about all the other stuff in the above program. Part of it consists of comments. Comments in a program are entirely ignored by the computer; they are there for human readers only. This doesn\u2019t mean that they are unimportant. Programs are meant to be read by people as well as by computers, and without comments, a program can be very difficult to understand. Java has two types of comments. The first type, used in the above program, begins with // and extends to the end of a line. The computer ignores the // and everything that follows it on the same line. Java has another style of comment that can extend over many lines. That type of comment begins with /* and ends with */. Everything else in the program is required by the rules of Java syntax. All programming in Java is done inside \u201cclasses.\u201d The first line in the above program (not counting the comments) says that this is a class named HelloWorld. \u201cHelloWorld,\u201d the name of the class, also serves as the name of the program. Not every class is a program. In order to define a program, a class must include a subroutine named main, with a definition that takes the form public static void main(String[] args) { (statements) } When you tell the Java interpreter to run the program, the interpreter calls the main() subroutine, and the statements that it contains are executed. These statements make up the script that tells the computer exactly what to do when the program is executed. The main() routine can call subroutines that are defined in the same class or even in other classes, but it i\u00aes the main() routine that determines how and in what order the other subroutines are used. The word \u201cpublic\u201d in the first line of main() means that this routine can be called from outside the program. This is essential because the main() routine is called by the Java interpreter, which is something external to the program itself. The remainder of the first line of the routine is harder to explain at the moment; for now, just think of it as part of the required syntax. The definition of the subroutine that is, the instructions that say what it does consists of the sequence of \u201cstatements\u201d enclosed between braces, { and }. Here, I\u2019ve used statements as a placeholder for the actual statements that make up the program. Throughout this textbook, I will always use a similar format: anything that you see in this style of text (italic in angle brackets) is a placeholder that describes something you need to type when you write an actual program. As noted above, a subroutine can\u2019t exist by itself. It has to be part of a \u201cclass\u201d. A program is defined by a public class that takes the form. public class hprogram-namei { hoptional-variable-declarations-and-subroutinesi public static void main(String[] args) { statements } optional-variable-declarations-and-subroutines } The name on the first line is the name of the program, as well as the name of the class. If the name of the class is HelloWorld, then the class must be saved in a file called HelloWorld.java. When this file is compiled, another file named HelloWorld.class will be produced. This class file, HelloWorld.class, contains the Java bytecode that is e\u00aexecuted by a Java interpreter. HelloWorld.java is called the source code for the program. To execute the program, you only need the compiled class file, not the source code. The layout of the program on the page, such as the use of blank lines and indentation, is not part of the syntax or semantics of the language. The computer doesn\u2019t care about layout you could run the entire program together on one line as far as it is concerned. However, layout is important to human readers, and there are certain style guidelines for layout that are followed by most programmers. These style guidelines are part of the pragmatics of the Java programming language.",
    "page199": "Variables and the Primitive Types - Names are fundamental to programming. In programs, names are used to refer to many different sorts of things. In order to use those things, a programmer must understand the rules for giving names to things and the rules for using the names to work with those things. That is, the programmer must understand the syntax and the semantics of names. According to the syntax rules of Java, a name is a sequence of one or more characters. It must begin with a letter or underscore and must consist entirely of letters, digits, and underscores. (\u201cUnderscore\u201d refers to the character \u2019 \u2019.) For example, here are some legal names: No spaces are allowed in identifiers; HelloWorld is a legal identifier, but \u201cHello World\u201d is not. Upper case and lower case letters are considered to be different, so that HelloWorld, helloworld, HELLOWORLD, and hElloWorLD are all distinct names. Certain names are reserved for special uses in Java, and cannot be used by the programmer for other purposes. These reserved words include: class, public, static, if, else, while, and several dozen other words. Java is actually pretty liberal about what counts as a letter or a digit. Java uses the Unicode character set, which includes thousands of characters from many different languages and different alphabets, and many of these characters count as letters or digits. However, I will be sticking to what can be typed on a regular English keyboard. The pragmatics of naming includes style guidelines about how to choose names for things. For example, it is customary for names of classes to begin with upper case letters, while names o\u00aef variables and of subroutines begin with lower case letters; you can avoid a lot of confusion by following the same convention in your own programs. Most Java programmers do not use underscores in names, although some do use them at the beginning of the names of certain kinds of variables. When a name is made up of several words, such as HelloWorld or interestRate, it is customary to capitalize each word, except possibly the first; this is sometimes referred to as camel case, since the upper case letters in the middle of a name are supposed to look something like the humps on a camel\u2019s back. Finally, I\u2019ll no\u00aete that things are often referred to by compound names which consist of several ordinary names separated by periods. (Compound names are also called qualified names.) You\u2019ve already seen an example: System.out.println. The idea here is that things in Java can contain other things. A compound name is a kind of path to an item through one or more levels of containment. The name System.out.println indicates that something called \u201cSystem\u201d contains something called \u201cout\u201d which in turn contains something called \u201cprintln\u201d. Non-compound names are called simple identifiers. I\u2019ll use the term identifier to refer to any name simple or compound that can be used to refer to something in Java. (Note that the reserved words are not identifiers, since they can\u2019t be used as names for things.) Variables - Programs manipulate data that are stored in memory. In machine language, data can only be referred to by giving the numerical address of the location in memory where it is stored. In a high-level language such as Java, names are used instead of numbers to refer to data. It is the job of the computer to keep track of where in memory the data is actually stored; the programmer only has to remember the name. A name used in this\u00ae way to refer to data stored in memory is called a variable. Variables are actually rather subtle. Properly speaking, a variable is not a name for the data itself but for a location in memory that can hold data. You should think of a variable as a container or box\u00ae where you can store data that you will need to use later. The variable refers directly to the box and only indirectly to the data in the box. Since the data in the box can change, a variable can refer to different data values at different times during the execution of the program, but it always refers to the same box. Confusion can arise, especially for beginning programmers, because when a variable is used in a program in certain ways, it refers to the container, but when it is used in other ways, it refers to the data in the container. You\u2019ll see examples of both cases below. (In this way, a variable is something like the title, \u201cThe President of the United States.\u201d This title can refer to different people at different times, but it always refers to the same office. If I say \u201cthe President went fishing,\u201d I mean that George W. Bush went fishing. But if I say \u201cHillary Clinton wants to be President\u201d I mean that she wants to fill the office, not that she wants to be George Bush.) In Java, the only way to get data into a variable that is, into the box that the variable names is with an assignment statement. An assignment statement takes the form: (variable) = (expression); where expression represents anything that refers to or computes a data value. When the computer comes to an assignment statement in the course of executing a program, it evaluates the expression and puts the resulting data value into the variable. For example, consider the simple assignment statement rate = 0.07; The variable in this assignment statement is rate, and the expression is the number 0.07. The computer executes this assignment statement by putting the number 0.07 in the variable rate, replacing whatever was there before. Now, consider the following more complicated assignment statement, which might come later in the same program.",
    "page200": "Types and Literals- A variable in Java is designed to hold only one particular type of data; it can legally hold that type of data and no other. The compiler will consider it to be a syntax error if you try to violate this rule. We say that Java is a strongly typed language because it enforces this rule. There are eight so-called primitive types built into Java. The primitive types are named byte, short, int, long, float, double, char, and boolean. The first four types hold integers (whole numbers such as 17, -38477, and 0). The four integer types are distinguished by the ranges of integers they can hold. The float and double types hold real numbers (such as 3.6 and -145.99). Again, the two real types are distinguished by their range and accuracy. A variable of type char holds a single character from the Unicode character set. And a variable of type boolean holds one of the two logical values true or false. Any data value stored in the computer\u2019s memory must be represented as a binary number, that is as a string of zeros and ones. A single zero or one is called a bit. A string of eight bits is called a byte. Memory is usually measured in terms of bytes. Not surprisingly, the byte data type refers to a single byte of memory. A variable of type byte holds a string of eight bits, which can represent any of the integers between -128 and 127, inclusive. (There are 256 integers in that range; eight bits can represent 256 two raised to the power eight different values.) As for the other integer types, short corresponds to two bytes (16 bits). Variables of type short have values in the range -32768 to 32767. int corresponds to four bytes (32 bits). Variables of type int have values in the range -2147483648 to 2147483647. long corresponds to eight bytes (64 bits). Variables of type long have values in the range -9223372036854775808 to 9223372036854775807. You don\u2019t have to remember these numbers, but they do give you some idea of the size of integers that you can work with. Usually, you should just stick to the int data type, which is good enough for most purposes. The float data type is represented in four bytes of memory, using a standard method for encoding real numbers. The maximum value for a float is about 10 raised to the power 38. A float can have about 7 significant digits. (So that 32.3989231134 and 32.3989234399 would both have to be rounded off to about 32.398923 in order to be stored in a variable of type float.) A double takes up 8 bytes, can range up to about 10 to the power 308, and has about 15 significant digits. Ordinarily, you should stick to the double type for real values. A variable of type char occupies two bytes in memory. The value of a char variable is a single character such as A, *, x, or a space character. The value can also be a special character such a tab or a carriage return or one of the many Unicode characters that come from differen\u00aet languages. When a character is typed into a program, it must be surrounded by single quotes; for example: \u2019A\u2019, \u2019*\u2019, or \u2019x\u2019. Without the quotes, A would be an identifier and * would be a multiplication operator. The quotes are not part of the value and are not stored in the vari\u00aeable; they are just a convention for naming a particular character constant in a program. A name for a constant value is called a literal. A literal is what you have to type in a program to represent a value. \u2019A\u2019 and \u2019*\u2019 are literals of type char, representing the character values A and *. Certain special characters have special literals that use a backslash, \\, as an \u201cescape character\u201d. In particular, a tab is represented as \u2019t\u2019, a carriage return as \u2019r\u2019, a linefeed as \u2019 \u2019, the single quote character as , and the backslash itself as \u2019\\\u2019. Note that even though you type two characters between the quotes in \u2019t\u2019, the value represented by this literal is a single tab character. Numeric literals are a little more complicated than you might expect. Of course, there are the obvious literals such as 317 and 17.42. But there are other possibilities for expressing numbers in a Java program. First of all, real numbers can be represented in an exponential form such as 1.3e12 or 12.3737e-108. The \u201ce12\u201d and \u201ce-108\u201d represent powers of 10, so that 1.3e12 means 1.3 t\u00aeimes 1012 and 12.3737e-108 means 12.3737 times 10\u2212108. This format can be used to express very large and very small numbers. Any numerical literal that contains a decimal point or exponential is a literal of type double. To make a literal of type float, you have to append an \u201cF\u201d or \u201cf\u201d to the end of the number. For example, \u201c1.2F\u201d stands for 1.2 considered as a value of type float. (Occas\u00aeionally, you need to know this because the rules of Java say that you can\u2019t assign a value of type double to a variable of type float, so you might be confronted with a ridiculous-seeming error message if you try to do something like \u201cx = 1.2;\u201d when x is a variable of type float. You have to say \u201cx = 1.2F;. This is one reason why I advise sticking to type double for real numbers.) Even for integer literals, there are some complications. Ordinary integers such as 177777 and -32 are literals of type byte, short, or int, depending on their size. You can make a literal of type long by adding \u201cL\u201d as a suffix. For example: 17L or 728476874368L. As another complication, Java allows octal (base-8) and hexadecimal (base-16) literals. I don\u2019t want to cover base-8 and base-16 in detail, but in case you run into them in other people\u2019s programs, it\u2019s worth knowing a few things: Octal numbers use only the digits 0 through 7. In Java, a numeric literal that begins with a 0 is interpreted as an octal number; for example, the literal 045 represents the number 37, not the number 45. Hexadecimal numbers use 16 digits, the usual digits 0 through 9 and the letters A, B, C, D, E, and F. Upper case and lower case letters can be used interchangeably in this context. The letters represent the numbers 10 through 15. In Java, a hexadecimal literal begins\u00ae with 0x or 0X, as in 0x45 or 0xFF7A. Hexadecimal numbers are also used in character literals to represent arbitrary Unicode characters. A Unicode literal consists of u followed by four hexadecimal digits. For example, the character literal \u2019u00E9\u2019 represents the Unicode character that is an \u201ce\u201d with an acute accent.",
    "page201": "Variables in Programs- A variable can be used in a program only if it has first been declared. A variable declaration statement is used to declare one or more variables and to give them names. When the computer executes a variable declaration, it sets aside memory for the variable and associates the variable\u2019s name with that memory. A simple variable declaration takes the form. (type-name) (variable-name-or-names); The hvariable-name-or-namesi can be a single variable name or a list of variable names separated by commas. (We\u2019ll see later that variable declaration statements can actually be somewhat more complicated than this.) Good programming style is to declare only one variable in a declaration statement, unless the variables are closely related in some way. For example: int numberOfStudents; String name; double x, y; boolean isFinished; char firstInitial, middleInitial, lastInitial; It is also good style to include a comment with each variable declaration to explain its purpose in the program, or to give other information that might be useful to a human reader. in this chapter, we will only use variables declared inside the main() subroutine of a program. Variables declared inside a subroutine are called local variables for that subroutine. They exist only inside the subroutine, while it is \u00aerunning, and are completely inaccessible from outside. Variable declarations can occur anywhere inside the subroutine, as long as each variable is declared before it is used in any expression. Some people like to declare all the variables at the beginning of the subroutine. Others like to wait to declare a variable until it is needed. My preference: Declare importa\u00aent variables at the beginning of the subroutine, and use a comment to explain the purpose of each variable. Declare \u201cutility variables\u201d which are not important to the overall logic of the subroutine at the point in the subroutine where they are first used. Strings, Objects, Enums, and Subroutines- The previous section introduced the eight primitive data types and the type String. There is a fundamental difference between the primitive types and the String type: Values of type String are objects. While we will not study objects in detail until Chapter 5, it will be useful for you to know a little about them and about a closely related topic: classes. This is not just because strings are useful but because objects and classes are essential to understanding another important programming concept, subroutines. Another reason for considering classes and objects at this point is so that we can introduce enums. An enum is a data type that can be created by a Java programmer to represent a small collection of possible values. Technically, an enum is a class and its possible values are objects. Enums will be our first example of adding a new type to the Java language. We will look at them later in this section. Built-in Subroutines and Functions- Recall that a subroutine is a set of program instructions that have been chunked together and given a name. In Chapter 4, you\u2019ll learn how to write your own subroutines, but you can get a lot done in a program just by calling subroutines that have already been written for you. In Java, every subroutine is contained in a class or in an object. Some classes that are standard parts of the Java language contain predefined subroutines that you can use. A value of type String, which is an object, contains subroutines that can be used to manipulate that string. These subroutines are \u201cbuilt into\u201d the Java language. You can call all these subroutines without understanding how they were written or how they work. Indeed, that\u2019s the whole point of subroutines: A subroutine is a \u201cblack box\u201d which can be used without knowing what goes on inside. Classes in Java have two very different functions. First of all, a class can group together variables and subroutines that are contained in that class. These variables and subroutines are called static members of the class. You\u2019ve seen one example: In a class that defines a program, the main() routine is a static member of the class. The parts of a class definition that define static members are marked with the reserved word \u201cstatic\u201d, just like the main() routine of a program. However, classes have a second function. They are used to describe objects. In this role, the class of an object specifies what subroutines and variables are contained in that object. The class is a type in the technical sense of a specification of a certain type of data value and the object is a value of that type. For example, String is actually the \u00aename of a class that is included as a standard part of the Java language. String is also a type, and literal strings such as \\\"Hello World\\\" represent values of type String. So, every subroutine is contained either in a class or in an object. Classes contain subroutines called static member subroutines. Classes also describe objects an\u00aed the subroutines that are contained in those objects. This dual use can be confusing, and in practice most classes are designed to perform primarily or exclusively in only one of the two possible roles. For example, although the String class does contain a few rarely-used static member subroutines, it exists mainly to specify a large number of subroutines that are contained in objects of type String. Another standard class, named Math, exists entirely to group together a number of static member subroutines that compute various common mathematical functions.",
    "page202": "To begin to get a handle on all of this complexity, let\u2019s look at the subroutine System.out.print as an example. As you have seen earlier in this chapter, this subroutine is used to display information to the user. For example, System.out.print(\\\"Hello World\\\") displays the message, Hello World. System is one of Java\u2019s standard classes. One of the static member variables in this class is named out. Since this variable is contained in the class System, its full name which you have to use to refer to it in your programs is System.out. The variable System.out ref\u00aeers to an object, and that object in turn contains a subroutine named print. The compound identifie\u00aer System.out.print refers to the subroutine print in the object out in the class System. (As an aside, I will note that the object referred to by System.out is an object of the class PrintStream. PrintStream is another class that is a standard part of Java. Any object of type PrintStream is a destination to which information can be printed; any object of type PrintStream has a print subroutine that can be used to send information to that destination. The object System.out is just one possible destination, and System.out.print is the subroutine that sends information to that particular destination. Other objects of type PrintStream might send information to other destinations such as files or across a network to other compute\u00aers. This is object-oriented programming: Many different things which have something in common they can all be used as destinations for information can all be used in the same way through a print subroutine. The PrintStream class expresses the commonalities among all these objects.) Since class names and variable names are used in similar ways, it might be hard to \u00aetell which is which. Remember that all the built-in, predefined names in Java follow the rule that class names begin with an upper case letter while variable names begin with a lower case letter. While this is not a formal syntax rule, I recommend that you follow it in your own programming. Subroutine names should also begin with lower case letters. There is no possibility of confusing a variable with a subroutine, since a subroutine name in a program is always followed by a left parenthesis. (As one final gene\u00aeral note, you should be aware that subroutines in Java are often referred to as methods. Generally, the term \u201cmethod\u201d means a subroutine that is contained in a class or in an object. Since this is true of every subroutine in Java, every subroutine in Java is a method. The same is not true for other programming languages. Nevertheless, the term \u201cmethod\u201d is mostly used in the context of object-oriented programming, and until we start doing real object-oriented programming in Chapter 5, I will prefer to use the more general term, \u201csubroutine.\u201d) Classes can contain static member subroutines, as well as static member variables. For example, the System class contains a subroutine named exit. In a program, of course, this subroutine must be referred to as System.exit. Calling this subroutine will terminate the program. You could use it if you had some reason to terminate the program before the end of the main routine. For historical reasons, this subroutine takes an integer as a parameter, so the subroutine call statement might look like \u201cSystem.exit(0);\u201d or \u201cSystem.exit(1);\u201d. (The parameter tells the computer why the program was terminated. A parameter value of 0 indicates that the \u00aeprogram ended normally. Any other value indicates that the program was terminated because an error was detected.\u00ae But in practice, the value of the parameter is usually ignored.) Every subroutine performs some specific task. For some subroutines, that task is to compute or retrieve some data value. Subroutines of this type are called functions. We say that a function returns a value. The returned value must then be used somehow in the program. You are familiar with the mathematical function that computes the square root of a number. Java has a corresponding function called Math.sqrt. This function is a static member subroutine of the class named Math. If x is any numerical value, then Math.sqrt(x) computes and returns the square root of that value. Since Math.sqrt(x) represents a value, it doesn\u2019t make sense to put it on a line by itself in a subroutine call statement such as Math.sqrt(x); // This doesn\u2019t make sense! What, after all, would the computer do with the value computed by the function in this case? You have to tell the computer to do something with the value. You might tell the computer to display it: System.out.print( Math.sqrt(x) ); // Display the square root of x. or you might use an assignment statement to tell the computer to store that value in a variable:",
    "page203": "The function call Math.sqrt(x) represents a value of type double, and it can be used anyplace where a numeric literal of type double could be used. The Math class contains many static member functions. Here is a list of some of the more important of them: Math.abs(x), which computes the absolute value of x. The usual trigonometric functions, Math.sin(x), Math.cos(x), and Math.tan(x). (For all the trigonometric functions, angles are measured in radians, not degrees.) The inverse trigonometric functions arcsin, arccos, and arctan, which are written as: Math.asin(x), Math.acos(x), and Math.atan(x). The return value is expressed in radians, not degrees. The exponential function Math.exp(x) for computing the number e raised to the power x, and the natural logarithm function Math.log(x) for computing the logarithm of x in the base e. Math.pow(x,y) for computing x raised to the power y. Math.floor(x), which rounds x down to the nearest integer value that is less than or equal to x. Even though the return value is mathematically an integer, it is returned as a value of type double, rather than of type int as you might expect. For example, Math.floor(3.76) is 3.0. The function Math.round(x) returns the integer that is closest to x. Math.random(), which returns a randomly chosen double in the range 0.0 <= Math.random() < 1.0. (The computer act\u00aeually calculates so-called \u201cpseudorandom\u201d numbers, which are not truly random but are random enough for most purposes.) For these functions, the type of the parameter the x or y inside the parentheses can be any value of any numeric type. For most of the functions, the value returned by the function is of type double no matter what the type of the parameter. However, for Math.abs(x), the value returned will be the same type as x; if x\u00ae is of type int, then so is Math.abs(x). So, for example, while Math.sqrt(9) is the double value 3.0, Math.abs(9) is the int value 9. Note that Math.random() does not have any parameter. You still need the parentheses, even though there\u2019s nothing between them. The parentheses let the computer know that this is a subroutine rather than a\u00ae variable. Another example of a subroutine that has no parameters is the function System.currentTimeMillis(), from the System class. When this function is executed, it retrieves the current time, expressed as the number of milliseconds that have passed since a standardized base time (the start of the year 1970 in Greenwich Mean Time, if you care). One millisecond is one-thousandth of a second. The return value of System.currentTimeMillis() is of type long. This function can be used to measure the time that it takes the computer to perform a task. Just record the time at which the task is begun and the time at which it is finished and take the difference. Here is a sample program that performs a few mathematical tasks and reports the time that it takes for the program to run. On some computers, the time reported might be zero, because it is too small to measure in milliseconds. Even if it\u2019s not zero, you can be sure that most of the time reported by the computer was spent doing output or working on tasks other than the program, since the calculations performed in this program occupy only a tiny fraction of a second of a computer\u2019s time. Operations on Strings- A value of type String is an object. That object contains data, namely the sequence of characters that make up the string. It also contains subroutines. All of these subroutines are in fact functions. For example, every string object contains a function named length that computes the number of characters in that string. Suppose that advice is a variable that refers to a String. For example, advice might have been declared and assigned a value as follows: String advice; advice = \\\"Seize the day!\\\"; Then advice.length() is a function call that returns the number of characters in the string \u201cSeize the day!\u201d. In this case, the return value would be 14. In general, for any string variable str, the value of str.length() is an int equal to the number of characters in the string that is the value of str. Note that this function has no parameter; the particular string whose length is being computed is the value of str. The length subroutine is defined by the class String, and it can be used with any value of type String. It can even be used with String literals, which are, after all, just constant values of type String. For example, you could have a program count the characters in \u201cHello World\u201d for you by saying System.out.print(\\\"The number of characters in \\\"); System.out.println(\\\"the string \"Hello World\" is \\\"); System.out.println( \\\"Hello World\\\".length() ); The String class defines a lot of functions. Here are some that you might find useful. Assume that s1 and s2 refer to values of type String:s1.equals(s2) is a function that returns a boolean value. It returns true if s1 consists of exactly the same sequence of characters as s2, and returns false otherwise. s1.equalsIgnoreCase(s2) is another boolean-valued function that checks whether s1 is the same string as s2, but this function considers upper and lower case letters to be equivalent. Thus, if s1 is \u201ccat\u201d, then s1.equals(\\\"Cat\\\") is false, while s1.equalsIgnoreCase(\\\"Cat\\\") is true. s1.length(), as mentioned above, is an integer-valued function that gives the number of characters in s1. s1.length(), as mentioned above, i\u00aes an integer-valued function that gives the number of characters in s1. s1.charAt(N), where N is an integer, returns a value of type char. It returns the Nth character in the string. Positions are numbered starting with 0, so s1.charAt(0) is actually the first character, s1.charAt(1) is the second, and so on. The final position is s1.length() - 1. For example, the value of \\\"cat\\\".charAt(1) is \u2019a\u2019. An error occurs if the value of the parameter is less than zero or greater than s1.length() - 1.",
    "page204": "s1.substring(N,M), where N and M are integers, returns a value of type String. The returned value consists of the characters in s1 in positions N, N+1,. . . , M-1. Note that the character in position M is not included. The returned value is called a substrin\u00aeg of s1. \u2022 s1.indexOf(s2) returns an integer. If s2 occurs as a substring of s1, then the returned value is the starting position of that substring. Otherwise, the returned value is -1. You can also use s1.indexOf(ch) to search for a particular character, ch, in s1. To find the first occurrence of x at or after position N, you can use s1.indexOf(x,N). \u2022 s1.compareTo(s2) is an integer-valued function that compares the two strings. If the strings are equal, the value returned is zero. If s1 is less than s2, the value returned is a number less than zero, and if s1 is greater than s2, the value returned is some number greater than zero. (If both of the strings consist entirely of lower case letters, then \u201cless than\u201d and \u201cgreater than\u201d refer to alphabetical order. Otherwise, the ordering is more complicated.) \u2022 s1.toUpperCase() is a String-valued function that returns a new string that is equal to s1, except that any lower case letters in s1 have been converted to upper case. For example, \\\"Cat\\\".toUpperCase() is the string \\\"CAT\\\". There is also a function s1.toLowerCase(). \u2022 s1.trim() is a String-valued function that returns a new string that is equal to s1 except that any non-printing characters such as spaces and tabs have been trimmed from the beginning and from the end of the string. Thus, if s1 has the value \\\"fred\u00ae \\\", then s1.trim() is the string \\\"fred\\\". For the functions s1.toUpperCase(), s1.toLowerCase(), and s1.trim(), note that the value of s1 is not modified. Instead a new string is created and returned as the value of the function. The returned value could be used, for example, in an assignment statement such as \u201csmallLetters = s1.toLowerCase();\u201d. To change the value of s1, you could use an assignment \u201cs1 = s1.toLowerCase();\u201d. Here is another extremely useful fact about strings: You can use the pl\u00aeus operator, +, to concatenate two strings. The concatenation of two strings is a new string consisting of all the characters of the firs\u00aet string followed by all the characters of the second string. For example, \\\"Hello\\\" + \\\"World\\\" evaluates to \\\"HelloWorld\\\". (Gotta watch those spaces, of course if you want a spac\u00aee in the concatenated string, it has to be somewhere in the input data, as in \\\"Hello \\\" + \\\"World\\\".) Let\u2019s suppose that name is a variable of type String and that it already refers to the name of the person using the program. Then, the program could greet the user by executing the statement: System.out.println(\\\"Hello, \\\" + name + \\\". Pleased to meet you!\\\"); Even more surprising is that you can actually concatenate values of any type onto a String using the + operator. The value is converted to a string, just as it would be if you printed it to the standard output, and then it is concatenated onto the string. For example, the expression \\\"Number\\\" + 42 evaluates to the string \\\"Number42\\\". And the statements System.out.print(\\\"After \\\"); System.out.print(years); System.out.print(\\\" years, the value is \\\"); System.out.print(principal); can be replaced by the single statement: System.out.print(\\\"After \\\" + years + \\\" years, the value is \\\" + principal); Obviously, this is very convenient. It would have shortened some of the examples presented earlier in this chapter. Introduction to Enums- Java comes with eight built-in primitive types and a large set of types that are defined by classes, such as String. But even this large collection of types is not sufficient to cover all the possible situations that a programmer might have to deal with. So, an e\u00aessential part of Java, just like almost any other programming language, is the ability to create new types. For the most part, this is done by defining new classes; you will learn how to do that in Chapter 5. But we will look here at one particular case: the ability to define enums (short for enumerated types). Enums are a recent addition to Java. They were only added in Version 5.0. Many programming languages have something similar, and many people believe that enums should have been part of Java from the beginning. Technically, an enum is considered to be a special kind of class, but that is not important for now. In this section, we will look at enums in a simplified form. In practice, most uses of enums will only need the simplified form that is presented here An enum is a type that has a fixed list of possible values, which is specified when the enum is created. In some ways, an enum is similar to the boolean data type, which has true and false as its only possible values. However, boolean is a primitive type, while an enum is not. The definition of an enum types has the (simplified) form: enum (enum-type-name) { (list-of-enum-values) } This definition cannot be inside a subroutine. You can place it outside the main() routine of the program. The henum-type-namei can be any simple identifier. This identifier becomes the name of the enum type, in the same way that \u201cboolean\u201d is the name of the boolean type and \u201cString\u201d is the name of the String type. Ea\u00aech value in the hlist-of-enum-valuesi must be a simple identifier, and the identifiers in the list are separated by commas. For example, here is the definition of an enum type named Season whose values are the names of the four seasons of the year:",
    "page205": "enum Season { SPRING, SUMMER, FALL, WINTER } By convention, enum values are given names that are made up of upper case letters, but that is a style guideline and not a syntax rule. Enum values are not variables. Each value\u00ae is a constant that always has the same value. In fact, the possible values of an enum type are usually referred to as enum constants. Note that the enum constants of type Season are considered to be \u201ccontained in\u201d Season, which means following the convention that compound identifiers are used for things that are contained in other things the names that you actually use in your program to refer to them are Season.SPRING, Season.SUMMER, Season.FALL, and Season.WINTER. Once a\u00aen enum type has been created, it can be used to declare variables in exactly the same ways that other types are used. For example, you can declare a variable named vacation of type Season with the statement: Season vacation; After declaring the variable, you can assign a value to it using an assignment statement. The value on the right-hand side of the assignment can be one of the enum constants of type Season. Remember to use the full name of the constant, including \u201cSeason\u201d! For example: vacation = Season.SUMMER; You can print out an enum value with an output statement such as System.out.print(vacation). The output value will be the name of the enum constant (without the \u201cSeason.\u201d). In this case, For some unfathomable reason, Java has never made it easy to read data typed in by the user of a program. You\u2019ve already seen that output can be displayed to the user using the subroutine System.out.print. This subroutine is part of a pre-defined object called System.out. The purpose of this object is precisely to dis\u00aeplay output to the user. There is a corresponding object called System.in that exists to read data input by the user, but it provides only very primitive input facilities, and it requires some advanced Java programming skills to use it effectively. Java 5.0 finally makes input a little easier with a new Scanner class. However, it requires some knowledge of object-oriented programming to use this class, so it\u2019s not appropriate for use here at the beginning of this course. (Furthermore, in my opinion, Scanner still does not get things quite right.) There is some excuse for this lack of concern with input, since Java is meant mainly to write programs for Graphical User Interfaces, and those programs have their own style of input/output, which is implemented in Java. However, basic support is needed for input/output in old-fashioned non-GUI programs. Fortunately, it is possible to extend Java by creating new classes that provide subroutines that are not available in the standard part of the language. As soon as a new class is available, the subroutines that it contains can be used in exactly the same way as built-in routines. Along these lines, I\u2019ve written a class called TextIO that defines subroutines for reading values typed by the user of a non-GUI program. The subroutines in this class make it possible to get input from the standard input object, System.in, without knowing about the advanced aspects of Java that are needed to use Scanner or to use System.in directly. TextIO also contains a set of output subroutines. The output subroutines are similar to th\u00aeose provided in System.out, but they provide a few additional features. You can use whichever set of output subroutines you prefer, and you can even mix them in the same program. To use the TextIO class, you must make sure that the class is available to your program. What this means depends on the Java progr\u00aeamming environment that you are using. In general, you just have to add the source code file, TextIO.java, to the same directory that contains your main program. See Section 2.6 for\u00ae more information about how to use TextIO. A First Text Input Example The input routines in the TextIO class are static member functions. (Static member functions were introduced in the previous section.) Let\u2019s suppose that you want your program to read an integer typed in by the user. The TextIO class contains a static member function named getlnInt that you can use for this purpose. Since this function is contained in the TextIO class, you have to refer to it in your program as TextIO.getlnInt. The function has no parameters, so a complete call to the function takes the form \u201cTextIO.getlnInt()\u201d. This function call represents the int value typed by the user, and you have to do something with the returned value, such as assign it to a variable. For example, if userInput is a variable of type int (created with a declaration statement \u201cint userInput;\u201d), then you could use the assignment statement",
    "page206": "enum Season { SPRING, SUMMER, FALL, WINTER } By convention, enum values are given names that are made up of upper case letters, but that is a style guideline and not a syntax rule. Enum values are not variables. Each value is a constant that always has the same value. In fact, the possible values of an enum type are usually referred to as enum constants. Note that the enum constants of type Season are considered to be \u201ccontained in\u201d Season, which means following the convention that compound identifiers are used for things that are contained in other things the names that you actually use in your program to refer to them are Season.SPRING, Season.SUMMER, Season.FALL, and Season.WINTER. Once an enum type has been created, it can be used to declare variables in exactly the same ways that other types are used. For example, you can declare a variable n\u00aeamed vacation of type Season with the statement: Season vacation; After declaring the variable, you can assign a value to it using an assignment statement. The value on the right-hand side of the assignment can be one of the enum constants of type Season. Remember to use the full name of the constant, including \u201cSeason\u201d! For example: vacation = Season.SUMMER; You can print out an enum value with an output statement such as System.out.print(vacation). The output value will be the name of the enum constant (without the \u201cSeason.\u201d). In this case, the output would be \u201cSUMMER\u201d. For some unfathomable reason, Java has never made it easy to read data typed in by the user of a program. You\u2019ve already seen that output can be displayed to the user using the subroutine System.out.print. This subroutine is part of a pre-defined object called System.out. The purpose of this object is precisely to display output to the user. There is a corresponding object called System.in that exists to read data input by the user, but it provides only very primitive input facilities, and it requires some advanced Java programming skills to use it effectively. Java 5.0 finally makes input a little easier with a new S\u00aecanner class. However, it requires some knowledge of object-oriented programming to use this class, so it\u2019s not appropriate for use here at the beginning of this course. (Furthermore, in my opinion, Scanner still does not get things quite right.) There is some excuse for this lack of concern with input, since Java is meant mainly to write programs for Graphical User Interfaces, and those programs have their own style of input/output, which is implemented in Java. However, basic support is needed for input/output in old-fashioned non-GUI programs. Fortunately, it is possible to extend Java by creating new classes that provide subroutines that are not available in the standard part of the language. As soon as a new class is available, the subroutines that it contains can be used in exactly the same way as built-in routines. Along these lines, I\u2019ve written a class called TextIO that defines subroutines for reading values typed by the user of a non-GUI program. The subroutines in this class make it possible to get input from the standard input object, System.in, without knowing about the advanced aspects of Java that are needed to use Scanner or to use System.in directly. TextIO also contains a set of output subroutines. The output subroutines are similar to those provided in System.out, but they provide a few additional features. You can use whichever set of output subroutines you prefer, and you can even mix them in the same program. To use the TextIO class, you must make sure that the class is available to your program. What this means depends on the Java programming environment that you are using. In general, you just have to add the source code file, TextIO.java, to the same directory that contains your main program. See Section 2.6 for more information about how to use TextIO. A First Text Input Example The input routines in the TextIO class are static member functions. (Static member functions were introduced in the previous section.) Let\u2019s suppose that you want your program to read an integer typed in by the user. The TextIO class contains a static member function named getlnInt that you can use for this purpose. Since this function is contained in the TextIO class, you have to refer to it in your program as TextIO.getlnInt. The function has no parameters, so a complete call to the function takes the form \u201cTextIO.getlnInt()\u201d. This function call represents the int value typed by the user, and you have to do something with the returned value, such as assign it to a variable. For example, if userInput is a variable of type int (created with a declaration statement \u201cint userInput;\u201d), then you could use the assignment statement Formatted Output- If you ran the preceding Interest2 example, you might have noticed that the answer is not always written in the format that is usually used for dollar amounts. In general, dollar amounts \u00aeare written with two digits after the decimal point. But the program\u2019s output can be a number like 1050.0 or 43.575. It would be better if these numbers were printed as 1050.00 and 43.58. Java 5.0 introduced a formatted output capability that makes it much easier than it used to be to control the format of output numbers. A lot of formatting options are available. I will cover just a few of the simplest and most commonly used possibilities here. You can use the function System.out.printf to produce formatted output. (The name \u201cprintf,\u201d which stands for \u201cprint formatted,\u201d is copied from the C and C++ programming languages, which have always have a similar formatting capability). System.out.printf takes two or more parameters. The first parameter is a String that specifies the format of the output. This parameter is called the format string. The remaining parameter\u00aes specify the values that",
    "page207": "are to be output. Here is a statement that will print a number in the proper format for a dollar amount, where amount is a variable of type double: System.out.printf( \\\"%1.2f\\\", amount ); TextIO can also do formatted output. The function TextIO.putf has the same functionality as System.out.printf. Using TextIO, the above example would be: TextIO.printf(\\\"%1.2\\\",amount); and you could say TextIO.putln(\\\"%1.2f\\\",principal); instead of TextIO.putln(principal); in the Interest2 program to get the output in the right format. The output format of a value is specified by a format specifier. The format string (in the simple cases that I cover here) contains one format specifier for each of the values that is to be output. Some typical format specifiers are %d, %12d, %10s, %1.2f, %15.8e and %1.8g. Every format specifier begins with a percent sign (%) and ends with a letter, possibly with some extra formatting information in between. The letter specifies the type of output that is to be produced. For example, in %d and %12d, the \u201cd\u201d specifies that an integer is to be written. The \u201c12\u201d in %12d specifies the minimum number of spaces that should be used for the output. If the integer that is being output takes up f\u00aeewer than 12 spaces, extra blank spaces are added in front of the integer to bring the total up to 12. We say that the output is \u201cright-justified in a field of length 12.\u201d The value is not forced into 12 spaces; if the value has more than 12 digits, all the digits will be printed, with no extra spaces. The specifier %d means the same as %1d; that is an integer will be printed using just as many spaces as necessary. (The \u201cd,\u201d by the way, stands for \u201cdecimal\u201d (base-10) numbers. You can use an \u201cx\u201d to output an integer value in hexadecimal form.) The letter \u201cs\u201d at the end of a format specifier can be used with any type of value. It means that the value should be output in its default format, ju\u00aest as it would be in unformatted output. A number, such as the \u201c10\u201d in %10s can be added to specify the (minimum) number of characters. The \u201cs\u201d stands for \u201cstring,\u201d meaning that the value is converted into a String value in the usual way. The format specifiers for values of type double are even more complicated. An \u201cf\u201d, as in %1.2f, is used to output a number in \u201cfloating-point\u201d form, that is with digits after the decimal point. In %1.2f, the \u201c2\u201d specifies the number of digits to use after the decimal point. The \u201c1\u201d specifies the (minimum) number of characters to output, which effectively means that just as many characters as are necessary should be used. Similarly, %12.3f would specify a floating-point format with 3 digits after the decimal point, right-justified in a field of length 12. Very large and very small numbers should be written in exponential format, such as 6.00221415e23, representing \u201c6.00\u00ae221415 times 10 raised to the power 23.\u201d A format specifier such as %15.8e specifies an output in exponential form, with the \u201c8\u201d telling how many digits to use after the decimal point. If you use \u201cg\u201d instead of \u201ce\u201d, the output will be in floating-point form for small values and in exponential form for large values. In %1.8g, the 8 gives the total number of digits in the answer, including both the digits before the decimal point and the digits after the decimal point.\u00ae In addition to format specifie\u00aers, the format string in a printf statement can include other characters. These extra characters are just copied to the output. This can be a convenient way to insert values into the middle of an output string. For example, if x and y are variables of type int, you could say System.out.printf(\\\"The product of %d and %d is %d\\\", x, y, x*y); When this statement is executed, the value of x is substitut\u00aeed for the first %d in the string, the value of y for the second %d, and the value of the expression x*y for the third, so the output would be something like \u201cThe product of 17 and 42 is 714\u201d (quotation marks not included in output!).",
    "page208": "Introduction to File I/O System.out sends its output to the output destination known as \u201cstandard output.\u201d But standard output is just one possible output destination. For example, data can be written to a file that is stored on the user\u2019s hard drive. The advantage to this, of course, is that the data is saved in the file even after the program ends, and the user can print the file, email it to someone else, edit it with another program, and so on. TextIO has the ability to write data to files and to read dat\u00aea from files. When you write output using the put, putln, or putf method in TextIO, the output is sent to the current output destination. By default, the current output destination is standard output. However, TextIO has some subroutines that can be used to change the current output destination. To write to a file named \u201cresult.txt\u201d, for example, you would use the statement: TextIO.writeFile(\\\"result.txt\\\"); After this statement is executed, any output from TextIO output statements will be sent to the file named \u201cresult.txt\u201d instead of to standard output. The file should be created in the same directory that contains the program. Note that if a file with the same name already exists, its previous contents will be erased! In many cases, you want to let the user select the file that will be used for output. The statement TextIO.writeUserSelectedFile(); will open a t\u00aeypical graphical-user-interface file selection dialog where the user can specify the output file. If you want to go back to sending output to standard output, you can say TextIO.writeStandardOutput(); You can also specify the input source for TextIO\u2019s various \u201cget\u201d functions. The default input source is standard input. You can use the statement TextIO.readFile(\\\"data.txt\\\") to read from a file named \u201cdata.txt\u201d instead, or you can let the user select the input file by saying TextIO.readUserSelectedFile(), and you can go back to reading from standard input with TextIO.readStandardInput(). When your program is reading from standard input, the user gets a chance to correct any errors in the input. This is not possible when the program is reading f\u00aerom a file. If illegal data is found when a program tries to read from a file, an error occurs that will crash the program. (Later, we will see that is is possible to \u201ccatch\u201d such errors and recover from them.) Errors can also occur, though more rarely, when writing to files. A complete understanding of file input/output in Java requires a knowledge of object oriented programming. We will return to the topic later, in Chapter 11. The file I/O capabilities in TextIO are rather primitive by comparison. Nevertheless, they are sufficient for many applications, and they will allow you to get some experience with files sooner rather than later. As a simple example, here is a program that asks the user some questions and outputs the user\u2019s responses to a file named \u201cprofile.txt\u201d: Details of Expressions- This section takes a closer look at expressions. Recall that an expression is a piece of program code that represents or computes a value. An expression can be a liter\u00aeal, a variable, a function call, or several of these things combined with operators such as + and >. The value of an expression can be assigne\u00aed to a variable, used as a parameter in a subroutine call, or combined with other values into a more complicated expression. (The value can even, in some cases, be ignored, if that\u2019s what you want to do; this is more common than you might think.) Expressions are an essential part of programming. So far, these notes have dealt only informally with expressions. This section tells you the more-or-less complete story (leaving out some of the less commonly used operators). The basic building blocks of expressions are literals (such as 674, 3.14, true, and \u2019X\u2019), variables, and function calls. Recall that a function is a subroutine that returns a value. You\u2019ve already seen some examples of functions, such as the input routines from the TextIO class and the mathematical functions from the Math class. The Math class also contains a couple of mathematical constants that are useful in mathematical expressions: Math.PI represents \u03c0 (the ratio of the circumference of a circle to its diameter), and Math.E represents e (the base of the natural logarithms). These \u201cconstants\u201d are actually member variables in Math of type double. They are only approximations for the mathematical constants, which would require an infinite number of digits to specify exactly. Literals, variables, and function calls are simple expressions. More complex expressions can be built up by using operators to combine simpler\u00ae expressions. Operators include + for adding two numbers, > for comparing two values, and so on. When several operators appear in an expression, there is a qu\u00aeestion of precedenc\u00aee, which determ\u00aeines how the operators are grouped for evaluation. For example, in the expression \u201cA + B * C\u201d, B*C is computed first and then the result is added to A. We say that multiplication (*) has higher precedence than addition (+). If the default precedence is not what you want, you can use parentheses to explicitly specify the grouping you want. For example, you could use \u201c(A + B) * C\u201d if you want to add A to B first and then multiply the result by C. The rest of this section gives details of operators in Java. The number of operators in Java \u00aeis quite large, and I will not cover them all here. Most of the important ones are here; a few will be covered in later chapters as they become relevant.",
    "page209": "Arithmetic Operators Arithmetic operators include addition, subtraction, multiplication, and division. They are indicated by +, -, *, and /. These operations can be used on values of any numeric type: byte, short, int, long, float, or double. When the computer actually calculates one of these operations, the two values that it combines must be of the same type. If your program tells the computer to combine two values of different types, the computer will convert one of the values from one ty\u00aepe to another. For example, to compute 37.4 + 10, the computer will convert the integer 10 to a real number 10.0 and will then compute 37.4 + 10.0. This is called a type conversion. Ordinarily, you don\u2019t have to worry about type conversion in expressions, because the computer does it automatically. When two numerical values are combined (after doing type conversion on one of them, if necessary), the answer will be of the same type. If you multiply two ints, you get an int; if you multiply two doubles, you get a double. This is what you would expect, but you have to be very careful when you use the division operator /. When you divide two integers, the answer will always be an integer; if the quotient has a fractional part, it is discarded. For example, the value of 7/2 is 3, not 3.5. If N is an integer variable, then N/100 is an integer, and 1/N is equal to zero for any N greater than one! This fact is a common source of programming errors. You can force the computer to compute a real number as the answer by making one of the operands real: For example, when the computer evaluates 1.0/N, it first converts N to a real number in order to match the type of 1.\u00ae0, so you get a real number as the answer. Java also has an operator for computing the remainder when one integer is divided by another. This operator is indicated by %. If A and B are integers, then A % B represents the remainder when A is divided by B. (However, for negative operands, % is not quite the same as the usual mathematical \u201cmodulus\u201d operator, since if one of A or B is negative, then the value of A % B will be negative.) For example, 7 % 2 is 1, while 34577 % 100 is 77, and 50 % 8 is 2. A common use of % is to test whether a given integer is even or odd. N is even if N % 2 is zero, and it is odd if N % 2 is 1. More generally, you can check whether an integer N is evenly divisible by an integer M by checking whether N % M is zero. Finally, you might need the unary minus operator, which takes the negative of a number. For example, -X has the same value as (-1)*X. For completeness, Java also has a unary plus operator, as in +X, even though it doesn\u2019t really do anything. By the way, recall that the + operator can also be used to concatenate a value of any type onto a String. This is another example of type conversion. In Java, any type can be automatically converted into type String. Increment and Decrement- You\u2019ll find that adding 1 to a variable is an extremely common operation in programming. Subtracting 1 from a variable is also pretty common. You might perform the operation of adding 1 to a variable with assignment statements such as: counter = counter + 1; goalsScored = goalsScored + 1; The effect of the assignment statement x = x + 1 is to take the old value of the variable x, compute the result of adding 1 to that value, and store the answer as the new value of x. The same operation can be accomplished by writing x++ (or, if you prefer, ++x). This actually changes the value of x, so that it has the same effect as writing \u201cx = x + 1\u201d. The two statements above could be written counter++; goalsScored++; Similarly, you could write x-- (or --x) to subtract 1 from x. That is, x-- performs the same computation as x = x - 1. Adding 1 to\u00ae a variable is c\u00aealled incrementing that variable, and subtracting 1 is called decrementing. The operators ++ and -- are called the increment operator and the decrement operator, respectively. These operators can be used on variables belonging to any of the numerical types and also on variables of type char. Usually, the operators ++ or -- are used in statements like \u201cx++;\u201d or \u201cx--;\u201d. These statements are commands to change the value of x. However, it is also legal to use x++, ++x, x--, or --x as expressions, or as parts of larger expressions. That is, you can write things like: y = x++; y = ++x; TextIO.putln(--x); z = (++x) * (y--); The statement \u201cy = x++;\u201d has the effects of adding 1 to the value of x and, in addition, assigning some value to y. The value assigned to y is the value of the expression x++, which is defined to be the old value of x, before the 1 is added. Thus, if the value of x is 6, the statement \u201cy = x++;\u201d will change the value of x to 7, but it will change the value of y to 6 since the value assigned to y is the old value of x. On the other hand, the value of ++x is defined\u00ae to be the new value of x, after the 1 is added. So if x is 6, then the statement \u201cy = ++x;\u201d changes the values of both x and y to 7. The decrement operator, --, works in a similar way. This can be confusing. My advice is: Don\u2019t be confused. Use ++ and -- only in stand-alone statements, not in exp\u00aeressions. I will follow this advice in all the examples in these notes.",
    "page210": " and closing the persistence context H. Refer\u00aeences a and b are in detached state when the fi\u00aerst pe\u00aersistence context is closed. You're dealing with instances that live outside of a guaranteed scope of object identity.  You can see that a and c, loaded in a different persistence context, aren't identical The test for equality with a.equals(c) is also false J. A test for database identity still returns true 1).This behavior can lead to pro\u00aeblems if you treat entity instances as equal in detached state. For example, consider the followin\u00aeg extensi\u00aeo\u00aen of the code, after the second unit of work has ended  This example adds all three references to a Set. All are references to detached instan\u00aeces. Now, if you check the size of the collection the number of elements  wha\u00aet result do you expect?  A Set doesn\u2019t allow duplicate elements. Duplicates are detected by the Set; whenever you add a reference, the Item#equals() method is called automatically against all other elements already in the collection. If equals() returns true for any element already in the collection, the addition doesn\u2019t occur By default, all Java classes inherit the equals() method of java.lang.Object. This implementation uses a double-equals = comparison to check whether two references refer to the same in-memory instance on the Java heap.",
    "page211": " You may guess that the number of elements in the collection is two. After all, a and b are references to the same in-memory instance; they have been loaded in the same persistence context. You obtained reference c from another persistence context; it refers to a different instance on the heap. You have three references to two instance\u00aes, but you know this only because you\u2019ve seen the code that loaded the data. In a real application, you \u00aemay not know that a and b are loaded in a different context than c. Furthermore, you obviously expect \u00aethat the collection has exactly one element, because a, b, and c represent the same database row, the same Item. Whenever you work with instances in detached state and you test them for equality (usually in\u00ae hash-based collections), you need to supply your own implementation of the equals() and hashCode() method\u00aes for your mapped entity class. This is an im\u00aeportant issue: if you don\u2019t work with entity instances in detached state, no action is needed, and the default equals() implementation of java.lang.Object is fine. You rely on H\u00aeibernate\u2019s guaranteed scope of object identity\u00ae within a persistence context. Even if you work with detached instances: if you never check if they\u2019re equal, you never put them in a Set or use them as keys in a Map, you don\u2019t \u00aehave to worry. If all you do is render a detached Item on the screen, you aren\u2019t comparing it to anything.",
    "page212": " Many developers new to JPA think they always have to provide a custom equality routine for all entity classe\u00aes, but this isn\u2019t the case. In section 18.3, we\u2019ll show you an application design with an extended persistence context strategy. This strategy will also extend the scope of guaranteed object identity to span an entire conversation and several system transactions. Note that you still need the discipline not to compare detached instances obtained in two conversations! \u00ae Let\u2019s assume that you want to use detached instances and that you have to test them for equality with your own method. You can implement equals() and hashCode() methods several ways. Keep in mind that when you override equals(), you always need to also override \u00aehashCode() so the two methods are consistent. If two instances are equal, they must have the same hash value.  A seemingly clever approach is to imple\u00aement equals() to compare just the database identifier property, which is often a surrogate primary\u00ae key value. Basically, if two Item instances have the same identifier returned by getId(), they must be the same. If getId() returns null, it must be a transient Item that hasn\u2019t been saved.  Unfortunately, this solution has one huge problem: identifier values aren\u2019t assigned by Hibernate until an instance becomes persistent. If a t\u00aeransient instance were added to a Set before being saved, then when you save it, its hash value would change while it\u2019s contained by\u00ae the Set. This is contrary to the contract of java.util.Set\u00ae, breaking the collection. In particular, this problem makes cascading persistent state useless for mapped associations based on sets. \u00aeWe strongly discourage database identifier equality.",
    "page213": "To get to the solution that we recommend, you need to understand th\u00aee notion of a business key. A business key is a property, or some combination of prop\u00aeerties, that is unique for each instance with the same database identity. Essentially, it\u2019s the natural\u00ae key that you would us\u00aee if you weren\u2019t using a surrogate primary key instead. Unlike a natural primary key, it isn\u2019t an ab\u00aes\u00aeolute requirement that the business key never cha\u00aenges as long as it changes rarely, that\u2019s enough.  We argue that essentially every entity class should have a business key, even if it includes all properties of the class (which would be appropriate for some immutable classes). If your user is looking at a list of items on screen, how do they differentiate between items A, B, and C? The same property, or combination of properties, is your business key. The business key is what the user thinks of as uniquely identifying a particular record, whereas the surrogate key\u00ae is what the application and da\u00aetabase systems rely on. The business key property or \u00aeproperties are most likely constrained UNIQUE in your database schema.  Let\u2019s write custom equality methods for the User entity class; this is easier than comparing Item instances. For the User class, username is a great candidate business key. It\u2019s always required, it\u2019s unique with a database constraint, and it changes rarely, if ever.",
    "page214": "You may have noticed that th\u00aee equals() method code always accesses the properties of the \u201cother\u201d reference via getter methods. This is extremely important, because the reference passed as other may be a Hibernate proxy, not the actual instance that holds the persistent state. You can\u2019t access the username field of a User proxy directly. To initialize the proxy to get \u00aethe property value, you need to access it with a getter method. This is one point where Hibernate isn\u2019t completely transparent, but it\u2019s good practice anyway to use getter methods instead of direct instance variable access.  Check the type of the other reference with \u00aeinstanceof, not by comparing the values of getClass(). Again, t\u00aehe other reference may be a proxy, which is a runtimegenerated subclass of User, so this and other may not be exactly the same type but a valid super/subtype. You can find more about \u00aeproxies in section 12.1.1. For some other entities, the business key may be more complex, consisting of a combination of properties. Here are some hints that\u00ae should help you identify a business key in your domain model classes: Consider what attributes users of your application will refer\u00ae to when they have to identify an object (in the real world). How do users tell the difference between one element and another if they\u2019re displayed on the screen? This is probably the business key you\u2019re looking for\u00ae.",
    "page215": "Every immutable attribute is probably a good candidate for the business key. Mutable attributes may be good candidates, too, if they\u2019re updated rarely or if you can control the case when they\u2019re updated for example, by ensuring the instances aren\u2019t in a Set at the time. Every attribute that has a UNIQUE database constraint is a good candidate for the business key. Remember that the precision of the business key has to be good enough to avoid overlaps. Any date or time-based attribute, such a\u00aes the creation timestamp of the record, is usually a good component\u00ae of a business key, but the accuracy of System currentTimeMillis() depends on the virtual machine a\u00aend operating system. Our recommended safety buffer is 50 milliseconds, which may not be \u00aeaccurate enough if the time-based property is the single attribute of a business key. You can use database identifiers as part of the business key. This seems to contradict our previous statements, but we aren\u2019t talking about the database identifier value of the given entity. You may be able to use the database identifier of an associated entity instance.   For example, a candidate business key for the Bid class is the identifier o\u00aef the It\u00aeem it matches together with the bid amount. You may even have a unique constraint that represents this composite business key in the database schema. You can use the identifier value of the associated Item because it never changes during the life cycle of a Bid the Bid constructor can require an alread\u00aey-persiste\u00aent Item.",
    "page216": "If you follow our advice, you shouldn\u2019t have much difficulty finding a good business key for all your business classes. If you encounter a difficult case, try to solve it without considering Hibernate. After all, it\u2019s purely an object-oriented problem. Notice that it\u2019s almost never correct to override equals() on a subclass and include another property in the comparison. It\u2019s a little tricky to satisfy the Object identity and equality requirements that equality be both symmetric and transitive in this case; and, more important, the business key may not correspond to any well-defined candidate natural key in the database (subclass properties may be mapped to a different table). For more information on customizing equality comparisons, see Effective Java, 2nd edition, by Joshua Bloch (Bloch, 2008), a mandatory book for all Java programmers.  The User class is now prepared for detached state; you can \u00aesafely put instances loaded in different persistence contexts into a Set. Next, we\u2019ll look at some examples that involve detached state, and you see some of the benefits of this concept.  Sometimes you might want to detach an entity instance manually from the persistence context\u00ae. You don\u2019t have to wait for the persistence context to close. You can evict entity instances manually: This example also demonstrates the EntityManager#contains() operation, which retu\u00aerns true if the given instance is in managed persistent state in this persistence context.  You can now work with the user reference in detached state. Many applications only read and render the data after the persistence context is closed.  Modifying the loaded user after the persistence context is closed\u00ae has no effect on it\u00aes persistent representation in the database. JPA allows you to merge any changes back int\u00aeo the database in a new persistence context, though.",
    "page217": "The goal is record the new username of the detached User. First, when you call merge(), Hibernate checks whether a pers\u00aeistent instance in the persistence context has the same database identifier as the detached instance you\u2019re merging.  In this example, the persistence context is empty; nothing has been loaded from the database. Hibernate therefore loads an instance with this identifier from the data base. Then, merge() copies the detached ent\u00aeity instance onto this loaded persistent instance. In other words, the new username you have set on the detached User is also set on the persistent merged User, which merge() returns to you.  Now discard the old reference to the stale and outdated detached state; the detachedUser no longer represents the current state. You can continue modifying the returned mergedUser; Hibernate will \u00aeexecute a single UPDATE when it flushes the persistence context during commit. If there is no persistent instance with the same identifier in the persistence context, and a lookup by identifier in the database is negative, Hibernate instantiates a fresh User. Hibernate then copies your detached instance onto this fresh instance, which it inserts into the database when you synchronize the persistence context with the database.  If the instance you\u00ae\u2019re giving to\u00ae me\u00aerge() is not detached but rather is transient (it doesn\u2019t have an identifier value), Hibernate instantiates a fresh User, copies the values of the transient User onto it, and then makes it persistent and returns it to you. In simpler terms, the merge() operation can handl\u00aee detached and transient entity instances. Hibernate always returns the result to you as a persistent instance.",
    "page218": " An application architecture based on detachment and merging may not call the persist() operation. You can merge new and detached entity instances to store data. The important difference is the re\u00aeturned current stat\u00aee and how you handle this switch of references in your application code. You have to discard the detachedUser and from now on reference the current mergedUser. Every other component in your application still holding on to detachedUser has to switch to mergedUser. If you want to delete a detached instance, you have to merge it first. Then call remove() on the persistent instance returned by merge().  We\u2019ll look at detached state and merging again in chapter 18 and implement a more complex conversation between a \u00aeuser and the system using this strategy.  In this chapter, we finally talk about transactions: how you create and control concurrent units of work in an application. A unit of work is an atomic group of operations. Transactions allow you to set unit of work boundaries and help you isolate one unit of work from another. In a multiuser application, you may also be processing these units of work concurrently.  To handle concurrency, we first focus on units of work at the lowest level: database and system transactions. You\u2019ll learn the APIs for transaction demarcation and how to define units of work in Java code. We\u2019ll talk about how to preserve isolation and control concurrent access with pessimistic and optimistic strategies.  Finally, we look at some special cases and JPA features, based on accessing the database without explicit transactions. Let\u2019s start with some background information.",
    "page219": "Application functionality requires that several things be done in one go. For example, when an auction finishes, the CaveatEmptor application must perform three different tasks: Find the winning bid (highest amount) for the auction item. Charge the seller of the item the cost of the auction. Notify the seller and successful bidder. What happens i\u00aef you can\u2019t bill the auction costs because of a failure in the external credit-card system? The business requirements may state that either all listed actions must succeed or none must succeed. If so, you call these steps collectively a transaction or unit of work. If only a single step fails, the entire unit of work must fail. ACID stands for atomicity, consistency, isolation, durability. Atomicity is the notion tha\u00aet all operations in a transaction execute as an atomic unit\u00ae. Furthermore, transactions allow multiple users\u00ae to work concurrently with the same data without\u00ae compromising the consistency of the data (consistent with database integrity rules). A particular transaction should not be visible to other concurrently running transactions; they should run in isolation. Changes made in a transaction should be durable, even if the system fails after the transaction has completed successfully.  In addition, you want correctness of a transaction. For example, the business rules dictate that the application charges the seller once, not twice. This is a reasonable assumption, but you may not be able to express it with database constraints. Hence, the correctness of a transaction is the responsibility of the application, whereas consistency is the responsibility of the database. Together, these transaction attributes define the ACID criteria.",
    "page220": "We\u2019ve also mentioned system and database transactions. Consider the last example again: during the unit of work ending an auction, we might mark the winning bid in a database system. Then, in the same unit of work, we\u00ae talk to an external system to bill the seller\u2019s credit card. This is a transaction spanning several (sub)systems, with coordinated subordinate transac\u00aetions on possibl\u00aey several r\u00aeesources such as a database connection and an external billing processor.  Database transactions have to be short, because open trans\u00aeactions consume database resources and potentially prevent co\u00aencurrent access due t\u00aeo exclusive locks on data. A single database transaction usually involves only a single batch of database operations.  To execute all of your database \u00aeoperations inside a system transaction, you have to set the boundaries of that unit of work. You must start the transact\u00aeion and, at some point, commit the changes. If an error occurs (either while executing database operations or when committing the transaction), you have to\u00ae roll back the changes to leave the data in a consistent state. This process defines a transaction demarcation and, depending on the technique you use, involves a certain level of manual intervention. In general, transaction boundaries that begin and end a transaction can be set either programmatically in application code or declaratively.",
    "page221": "In a Java SE environment, you call the JDBC API to mark transaction boundaries. You begin a transaction with setAutoCommit(false) on a JDBC Connection and end it by calling commit(). You may, at any ti\u00aeme while the transaction is in progress, force an immediate rollback with rollback().  In an applic\u00aeation that manipulates data i\u00aen several systems, a particular unit of work involves access to more than one transactional resource. In this case, you can\u2019t achieve atomicity with JDBC alone. You need a transaction manager that can handle several resources i\u00aen one system transaction. JTA standardizes system transaction management and distributed transactions so you won\u2019t have to worry much about the lower-level\u00ae details. The main API in JTA is the UserTransaction interface with methods to begin() and commit() a system transaction. The most complicated bit of this code snippet seems to be the exception handling; we\u2019ll discuss this part in a moment. First, you have\u00ae to understand how the transaction management and the EntityManager work togeth\u00aeer.  The EntityManager is lazy; we mentioned in the previous chapter that it doesn\u2019t consume any database connections until SQL statements have to be executed. The same is true for JTA: starting and committing an empty\u00ae transaction is cheap when you haven\u2019t accessed any t\u00aeransactional resources. For example, you could execute this empty unit of work on a server, for each client request, without consuming any\u00ae resources or holding any database locks.",
    "page222": "When you create an EntityManager, it looks for an ongoing JTA system transaction within the current thread of execution. If the EntityManager finds an ongoing transaction, it joins the transaction by listening to transaction events. This means you should always call UserTransaction#begin() and EntityManagerFac\u00aetory#createEntityManager() on the same thread if you want them to be joined. By default, and as explained in chap\u00aeter 10, Hibernate automatically flushes the persistence context when the transaction commits.  If the EntityManager can\u2019t find a started transaction in the same thread when it\u2019s created, it\u2019s in a special unsynchronized mode. In this mode, JPA won\u2019t automatically flush the persistence context. We talk more about this behavior later in this chapter; it\u2019s a convenient feature of JPA when you design more complex conversations. The transaction manager will stop a transaction when it has been running for too long. Remember that you\u00ae want to keep database transactions as short as possible in a busy OLTP system. The default timeout depends on the JTA provider Bitronix, for example, defaults to 6\u00ae0 seconds. You can override this selectively, before you begin the transaction, with UserTransaction#setTransactionTimeout().  We still need to discuss the exception handling of the previous code snippet. If any EntityManager call or flushing th\u00aee persistence context during a commit throws an exception, you must check the current state of the system transaction. When an exception occurs, Hibernate marks the transaction for rollback. This mean\u00aes the only possible outcome for this transaction is undoing all of its changes.",
    "page223": "Because you started the transaction, it\u2019s your job to chec\u00aek for STATUS_MARKED_ROLLBACK. The transaction might also still be STATUS_ACTIVE, if Hibernate wasn\u2019t able to mark it for rollback. In both cases, call UserTransaction#rollback() to abort any SQL statemen\u00aets that have been sent to the database within this unit of work.  All JPA operations, including flushing the persistence context, can throw a RuntimeException. But the methods UserTransaction#begin(), commit(), and even rollback() throw a checked Exception. The exception for rollback requires special treatment: you want to catch this exception and log it; otherwise, the original exception that led to the rollback is lost. \u00aeContinue throwing the original exception after rollback. Typically, you have another layer of interceptors in your system that will finally deal with the exception, for example by rendering an error screen or contacting the operations team. An error during rollback is more difficult to handle properly; we suggest logging and escalation, because a failed rollback indicates a serious system problem. Hibernate throws typed exceptions, all subtypes of RuntimeException that help you identify errors: The most common, HibernateException, is a generic error. You have to either check the exception message or find out more about the cause by calling getCause() on the exception.  A JDBCException is any exception thrown by Hibernate\u2019s internal JDBC layer. This kind of exception is always caused by a particular SQL statement, and you can get the offending statement with getSQL(). The internal exception thrown by the JDBC connection (the JDBC driver) is available with getSQLException() or getCause(), and the database- and vendor-specific error code is available with getErrorCode().",
    "page224": "Hibernate includes subtypes of JDBCException and an internal converter that tries to translate the vendor-specific error code thrown by the database driver into something more meaningful. The built-in converter can produce JDBCConnectionException, SQLGrammarException, LockAcquisitionException, DataException, and ConstraintViolationException for the most important database dialects supported by Hibernate. You can either manipulate or enhance the \u00aedialect for your database or plug in a SQLExceptionConverte\u00aerFactory to customize this conversion. Some developers get excited when they see how many fine-grained exception types Hibernate can throw. This can lead you down the wrong path. For example, you may be tempted to catch a ConstraintViolationException for validation purposes. If you forget to set the Item#name property, and its mapped column is NOT NULL in the database schema, Hibernate will throw this exception when you flush the persistence context. Why not catch it, display a (customized depending on the error code and text) failure message to application users, and let them correct the mistake? This strategy has two significant disadvantages.  First, throwing unchecked values against the database to see what sticks isn\u2019t the right strategy for a scalable application. You want to implement at least some dataintegrity validation in the application l\u00aeayer. Seco\u00aend, exceptions are fatal for your current unit of work. But this isn\u2019t how application users will interpret a validation error: they expect to still be inside a unit of work.",
    "page225": " Doing so helps you during development and also helps any customer-support engineer who has to decide quickly whether it\u2019s an application error (constraint violated, wrong SQL executed) or whether the database system is under load (loc\u00aeks couldn\u2019t be acquired). For validation, you have a unifying framework available with Bean Validation. From a single set of rules in annotations on entities, Hibernate can verify all domain and single-row constraints at the user interface layer and can automatically generate SQL DDL rules.  You now know what exceptions you should catch and when to expect them. One question is probably on your mi\u00aend: what should you do after you\u2019ve caught an exception and rolled back the system transaction? Exceptions t\u00aehrown by Hibernate are fatal. This means you have to close the current persistence context. You aren\u2019t allowed to continue working with the EntityManager that threw an exception. Render an error screen and/or log the error, and then let the user restart \u00aethe conversation with the system using a fresh transaction and persistence context.  As usual, this isn\u2019t the whole picture. Some standardized exceptions aren\u2019t fatal:  javax.persistence.NoResultException Thrown when a Query or TypedQuery is executed with getSingleResult() and no result was returned from the d\u00aeatabase. You can wrap the query call with exception-handling code and continue working with the persistence context. The current transaction won\u2019t be marked \u00aefor rollback.",
    "page226": "javax.persistence.NonUniqueResultException Thrown when a Query or TypedQuery is executed with getSingleResult() and several results were returned from the \u00aedatabase. You can wrap the query call with exception handling code and continue working with the persistence context. Hibernate won\u2019t mark the current transaction for rollback. javax.persistence.QueryTimeoutException Thr\u00aeown when a Query or TypedQ\u00aeuery takes too long to execute. Doesn\u2019t mark the transaction for rollback. You may want to repeat the query, if appropriate.  javax.persistence.LockTimeoutException Thrown when a pessimistic lock couldn\u2019t be acquired. May occur during flushing or explicit locking (more on this topic later in this chapter). The transaction isn\u00ae\u2019t marked for rollback, and you may want to repeat the operation. Keep in mind that endlessly hammering on a database system that is already struggling to keep up won\u2019t improve the situation. Notably absent from this list is javax.persistence.EntityNotFoundException. It can be thrown by the EntityManager#getReference() and refresh() methods, as well as lock(), which you\u2019ll see later in this chapter. Hibernate may throw it when you try to access the reference/proxy of an entity instance and the database record is no longer available. It\u2019s a fatal exception: it marks the current transaction for rollback, an\u00aed you have to close and discard the persistence context.  Programmatic transaction demarcation requires application code written against a transaction demarcation interface such as JTA\u2019s UserTransaction. Declarative transaction demarcation, on the other hand, doesn\u2019t require extra coding.",
    "page227": "In a Java EE application, you can declare when you wish to work inside a transaction. It\u2019s then the responsibility\u00ae of the runti\u00aeme environment to handle this concern. Usually you set transaction boundaries with annotations on your managed components (EJBs, C\u00aeDI beans, and so on).  You can use the older annotation @javax.ejb.TransactionAttribute to demarcate transaction boundaries declaratively on EJB components. You can find examples in section\u00ae 18.2.1.  You can apply the newer and more general @javax.transaction.Transaction\u00aeal on an\u00aey Java EE managed component. You can find an example in section 19.3.1.  All other examples in this chapter work in any Java SE environment, without a special runtime container. Hence, from now on, you\u2019ll only see programmatic transaction demarcation code until we focus on specific Java EE application examples.  Next, we focus on the most complex aspect of ACID properties: how you isolate concur\u00aerently running units of work from each other.  Databases (and other transactional systems) attempt to ensure transaction isolation, meaning that, from the point of view of each concurrent transaction, it appears that no other transactions are in progress. Traditionally, database systems have implemented isolation with locking. A transaction may place a lock on a particular item of data in the database, temporarily preventing read and/or write access to that item by other transactions. Some modern database engines implement transaction isolation with multiversion concurrency control (MVCC), which vendors generally consider mor\u00aee scalable. We\u2019ll discuss isolation assuming a locking model, but most of our observations are also applicable to MVCC.",
    "page228": " How databases implement concurrency control is of the utmost importance in your Java Persistence application. Applications inherit the isolation guarantees provided by the database management system. For example, Hibernate never locks anything in memory. If you consider the many years of experience that database vendors have with implementing concurrency control, you\u2019ll see the advantage of this approach. Additionally, some features in Java Persistence, either because you explicitly use them or by design, can improve the isolation guarantee beyond what the database pro\u00aevides. We discuss concurrency control in several steps. First, we explore t\u00aehe lowest layer: the transaction isolation guarantees provided by the database. After that, you\u2019ll see the Java Persistence features for pessimistic and optimistic concurrency control a\u00aet the application level, and what other isolation guarantees Hibernate can provide. If we\u2019re talki\u00aeng about isolation, you may assume that two things are either isolated or not; there is no grey area in the real world. When we talk about database transactions, complete isolation comes at a high price. You can\u2019t stop the world to access data exclusively in a multiuser OLTP system. Therefore, several isolation levels are available, which, naturally, weaken full isolation but increase performance and scalability of t\u00aehe system First, let\u2019s look at several phenomena that may occur when you weaken full transaction isolation. The ANSI SQL \u00aestandard defines the standard transaction isolation levels in terms of which of these phenomena are permissible.",
    "page229": " A lost update occurs if two transactions both update a data item and then the second transaction aborts, causing both changes to be lost. This occurs in systems that don\u2019t implement concurrency control, where concurrent transactions aren\u2019t isolated. This is shown in figure 11.1.  A dirty read occurs if a transaction reads changes made by another transaction that hasn\u2019t yet been committed. This is dangerous because the changes made by the other transaction may later be rolled back, and invalid data may be written by the first transaction; see figure 11.2.  An unrepeatable read occurs if a transaction \u00aereads a data item twice and reads different state each time. For example, another transaction may have written to the data item and committed between the two reads, as shown in figure 11.3.  A special case of an unrepeatable read is the last commit wins problem. Imagine that two concurrent transactions both read a data item, as shown in figure 11.4. One writes to it and commits, and then the second writes to \u00aeit and comm\u00aeits. The changes made by the first writer are lost. This issue is especially frustrating for users: use\u00aer A\u2019s changes are overwritten without warning, and B has potentially made a decision based on outdated information.  A phantom read is said to occur when a transaction executes a query twice, and\u00ae the second result includes \u00aedat\u00aea that wasn\u2019t visible in the first result or less data because something was deleted. It need not necessarily be exactly the same \u00aequery. Another transaction inserting or deleting data between the executions of the two\u00ae queries causes this situation, as shown in figure 11.5.",
    "page230": "Th\u00aee standard isolation levels are defined by the ANSI SQL standard, but they aren\u2019t specific to SQL databases. JTA defines exactly the same isolation levels, and you\u2019ll use these levels to declare your desired transaction isolation. With increased levels of isolation come higher cost and serious degradation of performance and scalability: Read uncommitted isolation A system that permits dirty reads but not lost updates operates in read uncommitted isolation. One transaction may not write to a row if another uncommitted transaction has already written to it. Any transaction may read any row, however. A DBMS may implement this isolation level with exclusive write locks.  Read committed isolation A system that permits unrepeatable reads but not dirty reads implements read committed isolation. A DBMS may achieve this by using shared read locks and exclusive write locks. Reading transactions don\u2019t block other transactions from accessing a row, but an uncommitted writing transaction blocks all other transactions from accessing the row.  Repeatable read isolation A system operating in repeatable read isolation mode permits neither unrepeatable r\u00aeeads nor dirty reads. Phantom reads may occur. Reading transactions block writing transactions but not other reading transactions, and writing transactions block all other transactions. Serializable isolation The strictest isolation, serializable, emulates serial execution, as if transactions were executed one after another, rather than concurrently. A DBMS may not implement serializable using only\u00ae row-level locks. A DBMS must instead provide some other mechanism that prevents a newly inserted row fro\u00aem becoming visible to a transaction that has already executed a query that would return the row. A crude mechanis\u00aem is exclusively locking the entire database table after a write, so no phantom reads can occur.",
    "page231": "Developers (ourselves included) are often unsure what transaction isolation level to use in a production application. Too high an isolation level harms the scalability of a highly concurrent application. Insufficient isolation may cause subtle, difficult-toreproduce bugs in an application that you won\u2019t\u00ae discover until the system is work\u00aeing under heavy load.  Note that we refer to optimistic locking (with versioning) in the following explanation, a concept explained later in this chapter. You may want to skip this section for now and come back to it later when it\u2019s time to pick an isolation level for your application. Choosing the correct i\u00aesolation level is, after all, highly dependent on your particular scenario. Read the following discussion as recommendations, not dictums carved in stone.  Hibernate tries hard t\u00aeo be as transparent as possible regarding transactional semantics of the database. Nevertheless, persistence context caching and versioning affect these semantics. What is a sensible database isolation level \u00aeto choose in a JPA application?  First, for almost all scenarios, eliminate the read uncommitted isolation level. It\u2019s extremely dangerous to use one transaction\u2019s uncommitted changes in a different transaction. The rollback or failure of one transaction w\u00aeill affect other concurrent transactions. Rollback of the first transaction could bring other transactions down with it, or perhaps even cause them to leave the database in an incorrect state (\u00aethe seller of an auction item might be charged twice consistent with database integrity rules but incorrect). It\u2019s possible that changes made by a transaction that ends up being rolled back could be committed anyway, because they could be read and then propagated by another transaction that is successful!",
    "page232": "Second, most applications don\u2019t need serializable isolation. Phantom reads aren\u2019t usually problematic, and \u00aethis isolation level tends to scale poorly. Few existing applications use serializable isolation in production, but rather rely on selectively applied pessimistic locks that effectively force a serialized execution of operations in certain situations.  Next, let\u2019s consider repeatable read. This level provides reproducibility for query result sets for the duration of \u00aea database transaction. This means you won\u2019t read committed updates from the database if you query it several times. But phantom reads are still possible: \u00aenew rows might appear rows you thought existed might disappear if another transaction \u00aecommitted such changes concurrently. Although you may sometimes want repeatable reads, you typically don\u2019t need them in every transaction\u00ae.  The JPA specification assumes that read committed is the default isolation level. This me\u00aeans you have to deal with unrepeatable reads, phantom reads, and the last commit wins problem. Let\u2019s assume you\u2019re enabling versioning of your domain model entities, something that Hibernate can do for you automatically. The combination of the (mandatory) persistence context cache and versioning already gives you most of the nice features of repeatable read isolation. The persistence context cache ensures that the state of the entity instances loaded by one transaction is isolated from changes made by other transactions. If you retrieve the same entity instance twice in a unit of work, the second lookup will be resolved within the persistence context cache and not hit the database. Hence, your read is repeatable, and you won\u2019t see conflicting committed data. (You still get phantom reads, though, which are typically much easier to deal with.) Additionally, versioning switches to first commit wins. Hence, for almost all multiuser JPA applications, read committed isolation for all database transactions is acceptable with enabled entity versioning.",
    "page233": "nabled entity versioning.  Hibernate retains the isolation level of your database connection; it doesn\u2019t change the level. Most products default to read committed isolation. There are sev\u00aeeral ways you can change either the default transaction isolation level or the settings of the current transaction.  First, you can check whether your DBMS has a global transaction isolation level se\u00aetting in its proprietary configuration. If your DBMS supports the standard SQL statement SET SESSION CHARACTERISTICS, you can execute it to set the transaction settings of all transact\u00aeions started in this particular database session (which means a particular connection to the database, not a Hibe\u00aernate Session). SQL also standardizes the SET TRANSACTION syntax, which sets the isolation level of the current transaction. Finally, the JDBC Connection API offers the setTransactionIsolation() method, which (according to its documentation) \u201cattempts to change the transaction isolation level for this connection.\u201d In a Hibernate/JPA application, you can obtain a JDBC Connection from the native Session API; see section 17.1.  We recommend a different approach \u00aeif you\u2019re using a JTA transaction manager or even a simple JDBC connection pool. JTA tran\u00aesaction management systems, such as Bit\u00aeronix used for the examples of this book, allow you to set a default transaction isolation level for every connection obtained from the pool.  In Bitronix, you can set the default isolation level on startup with\u00ae PoolingDataSource#setIsolationLevel(). Ch\u00aeeck the documentation of your data source provider, application server, or JDBC connection pool for more information.",
    "page234": "We assume from now on that your database connections are by default in read committed isolation level. From t\u00aeime to\u00ae time, a particular unit of work in your application may require a different, usually stricter isolation level. Instead of changing the isolation level of the entire t\u00aeransaction, you should use the Java Persistence API to obtain additional locks on the relevant data. This fine-grained locking is more scalable in a highly concurrent application. JPA offers optimistic version checking and databaselevel pessimistic locking.  Handling concurrency in an optimistic way is appro\u00aepriate when concurrent modifications are rare and it\u2019s feasible to detect conflicts late in a unit of work. JPA offers automatic version checking as an optimistic conflict-detection procedure.  First you\u2019ll enable versioning, because it\u00ae\u2019s turned off by default that\u2019s wh\u00aey you get last commit wins if you don\u2019t do anything. Most multiuser applications, especially web\u00ae applications, should rely on versioning for a\u00aeny concurrently modified @Entity instances, enabling the more user-friendly first commit wins.  The previous sections have been somewhat dry; it\u2019s time for code. After enabling automatic version checking, you\u2019ll see how manual version checking works and when you have to use it. You enable versioning with an @Version annotation on a special additional property of your entity class, as shown next. In this example, each entity instance carries a numeric version. It\u2019s mapped \u00aeto an additional column of the ITEM database table; as usual, the column name defaults to the property name, here VERSION. The actual name of the property and column doesn\u2019t matter you could rename it if VERSION is a reserved keyword in your DBMS.",
    "page235": "You cou\u00aeld add a getVersion() method to the class, but you shouldn\u2019t have a setter method and the application shouldn\u2019t modify the value. Hibernate automatically changes the version \u00aevalue: it increments the version number whenever an Item instance has been found dirty during flushing of the persistence context. The version is a simple counter without any useful semantic value beyond concurrency control. You can use an int, an Integer, a short, a Short, or a Long instead of a long; Hibernate wraps and starts f\u00aerom zero again if the version number reaches the limit of the data type.  After incrementing the version number of a detected dirty Item during flushing, Hibernate compares versions when executing the UPDATE and DELETE SQL statements. For example, assume that in a unit of work, you load an Item and change its name, as follows. Retrieving an entity instance by identifier loads the current version from the database with a SELECT.  The current version of the Item instance is 0.  When the persistence context is flushed, Hibernate detects the dirty Item instance \u00aeand increments i\u00aets version to 1. SQL UPDATE now performs the version check, storing the new version in the database, but only if the database version is still 0.",
    "page236": "Pay attention to the SQL statements, in particular the UPDATE and its WHERE clause. This update will be successful only if there is a row with VERSION 0 in th\u00aee database. JDBC returns the number of updated rows to Hibernate; if that result is zero, it means the ITEM row is either gone or doesn\u2019t have the version 0 anymore. Hibernate detects this conflict during flushing, and a javax.persistence.OptimisticLockException is thrown.  Now imagine two users executing this unit of work at\u00ae the same time, as shown previously in figure 11.4. The first user to commit updates the name of the Item and flushes the incremented version 1 to the database. The second user\u2019s flush (and commit) will fail, beca\u00aeuse their UPDATE statement can\u2019t find the row in the database with version 0. The database version is 1. Hence, the first commit wins, and you can catch the OptimisticLockException and handle it specifically. For example, you \u00aecould show the following message to the second user: \u201cThe data you hav\u00aee been working with has been modified by so\u00aemeone else. Please start your unit of work again with fresh data. Click the Restart button to proceed.\u201d What modifications trigger the\u00ae increment of an entity\u2019s version? Hibernate\u00ae increments the version whenever an entity instance is dirty. This includes all dirty valuetyped properties of the entity, no matter if they\u2019re single-valued (like a String or int property), embedded (like an Address), or collections. The exceptions are @OneToMany and @ManyToMany association collections that have been made read-only with mappedBy. Adding or removing ele\u00aements to these collections doesn\u2019t increment t\u00aehe version number of the owning entity instance. You should know that none of this is standardized in JPA don\u2019t rely on two JPA providers implementing the same rules when accessing a shared database.",
    "page237": "If you don\u2019t want to increm\u00aeent the version of the entity instance when a particular property\u2019s value has changed, annotate the property with @org.hibernate.annotations .OptimisticLock(excluded true). You may not like the additional VERSION column in your database schema. Alternatively, you may already have a \u201clast updated\u201d timestamp property on your entity class and a matching database column. H\u00aeibernate can che\u00aeck versions with timestamps instead of the extra counter field If your database schema already contains a timestamp column such as LASTUPDATED or MODIFIED_ON, you can map it for automatic version\u00ae checking instead of using a numeric counter. This example maps the column LASTUPDA\u00aeTED to a java.util.Date property; a Calendar type would also work with Hibernate. The JPA standard doesn\u2019t define these types for version properties; JPA only considers java.sql.Timestamp portable. This is less attractive, because you\u2019d have to import that JDBC class in your domain model. You should try to keep implementation details such as JDBC out of the domain model classes so they can be tested, instantiated, cross-compiled (to JavaScript with GWT, for example), serialized, and deserialized in as many environments as possible.  In theory, versioning with a timestamp is slightly less safe, because two concurrent transactions may both load and update the same Item in the same millisecond; this is exacerbated by the fact that a JVM usually doesn\u2019t have millisecond accuracy (you should check your JVM and operating system documentation for the guaranteed precision). Furthermore, retrieving the current time from the JVM isn\u2019 t necessarily safe in a clustered environment, where the system time of nodes may not be synchronized, or time synchronization isn\u2019t as accurate as you\u2019d need for your transactional load.",
    "page238": " You can switch to retrieval of the current time from the database machine by placing an @org.hibernate.annotations.Type annotation on the version property. Hibernate now asks the database, with for example call current _timestamp() on H2, for the current time before updating. This gives you a single source of time for synchronization. Not all Hibernate SQL dialects support this, so check the source of your configured d\u00aeialect and whether it overrides the getCurrentTimestampSelectString() method. In addition, there is always the overhead of hitting the database for every increment.  We recommend that new projects rely on versioning with a numeric count\u00aeer, not timestamps. If you\u2019re working with a l\u00aeegacy database schema or existing Java classes, it may be impossible to introduce a version or timestamp property and column. If that\u2019s the case, Hibernate has an alternative strategy for you. If you don\u2019t have version or timestamp columns, Hibernate can still perform automatic versioning. This alternative implementation of versioning checks the current database state against the unmodified values of persistent properties at the time Hibernate retrieved the entity instance (or the last time the persistence context was flushed).  You enable this functionality with the proprietary Hibernate annotation @org.hibernate.annotations.OptimisticLocking: Hibernate lists all columns and their last known values in the WHERE clause. If any concurrent transaction has modified any of these values or even deleted the row, this statement returns with zero updated rows. Hibernate then throws an exception at flush time.  Alternatively, Hibernate includes only the modified properties in the restriction (only NAME, in this example) if you switch to OptimisticLockType.DIRTY. This means two units of work may modify the same Item concurrently, and Hibernate detects a conflict only if they both modify the same value-typed property (or a foreign key value).",
    "page239": "The WHERE clause of the last SQL excerpt would be reduced to where ID 123 and NAME = 'Old Name'. Someone else could concurrently modify the price, and Hibernate wouldn\u2019t detect any conflict. Only if the application modified the name concurrently would you get a javax.persistence.OptimisticLockException.  In most cases, checking only dirty properties isn\u2019t a good strategy\u00ae for busines\u00aes entities. It\u2019s probably not OK to change the price of an item if the description changes!  This strategy also doesn\u2019t work with detached entities and merging: if you merge a detached entity into a new persistence context, the \u201cold\u201d values aren\u2019t known. The detached entity instance will have to carry a version number or timestamp for optimistic concurrency control.  Automatic versioning in Java Persistence prevents lost updates when two concurrent transactions try to commit modifications on the same piece of data. Versioning can also help you to obtain additional isolation guarantees manually when you need them.  Here\u2019s a scenario that requires repeatable database reads: imagine you have some categories in your auction system and that each Item is in a Category. This is a regu\u00aelar @ManyToOne mapping of an Item#category entity association.  Let\u2019s say you want to sum up all item prices in several categories. This requires a query for all items in each category, to add up the prices. The problem is, what happens if someone moves an Item from one Category to another Category while you\u2019re still querying and iterating through all the categories and items? With read-committed isolation, the same Item might show up twice while your procedure runs!",
    "page240": "For each Category, query all Item instan\u00aeces with an OPTIMISTIC lo\u00aeck mode. Hibernate now knows it has to check each Item at flush time.  For each Item loaded earlier with the locking query, Hibernate executes a SELECT during flushing. It checks whether the database version of each ITEM row is still the same as when it\u00ae was loaded. If any ITEM row has a different version or the row no longer exists, an OptimisticLockException is thrown. Don\u2019t be confused by the locking terminology: The JPA specification\u00ae leaves open how exactly each LockModeType is implemented; for OPTIMISTIC, Hibernate performs version checking. There are no actual locks involved. You\u2019ll have to enable versioning on the Item entity class as explained earlier; \u00aeotherwis\u00aee, you can\u2019t use the optimistic LockModeTypes with Hibernate.  Hibernate doesn\u2019t batch or otherwise optimize the SELECT statements for manual version checking: If you sum up 100 items, you get 100 additional queries at flush tim\u00aee. A pessimistic approach, as we show la\u00aeter in this chapter, may be a better solution for this particular case. As shown in the previous example, the Query interface accepts a LockModeType. Explicit lock modes are also supported by the TypedQuery and the NamedQuery interfaces, with the sam\u00ae\u00aee setLockMode() method.  An additional optimistic lock mode is available in JPA, forcing an increment of an entity\u2019s version.",
    "page241": "What happens if two users place a bid for the same auction item at the same time? When a user makes a new bid, the application must \u00aedo several things: Retrieve the currently highest Bid for the Item from the database.  Compare the new Bid with the highest Bid; if the new Bid i\u00aes higher, it must be stored in the database. There is the potentia\u00ael for a race condition in between these two steps. If, in between reading the highest Bid and placing the new Bid, another Bid is made, you won\u2019t see it. This conflict isn\u2019t visible; even enabling versioning of the Item doesn\u2019t help. The Item is never modified during the procedure. Forcing a version increment of the Item makes the conflict detectable find() accepts a LockModeType. The OPTIMISTIC_FORCE_INCREMENT mode tells Hibernate that the version of the retrieved Item should be incremented after loading, even if it\u2019s never modified in the unit of work.  The code persists a new \u00aeBid instance; this doesn\u2019t affect any values of the Item instance. A new row is inserted int\u00aeo the BID table. Hibernate wouldn\u2019t det\u00aeect concurrently made bids without a forced version increment of the Item.  You use a checked exception to validate the new bid amount. It must be greater than the currently highest bid.  When flushing the persistence context, Hibernate executes an INSERT for the new Bid and forces an UPDATE of the Item with a version check. If someone modified the Item concurrently \u00aeor placed a Bid concurrently with this procedure, Hibernate throws an exception.",
    "page242": "For the auction system, plac\u00aeing bids conc\u00aeurrently is certainly a frequent operation. Incrementing a version manually is useful in many situations where\u00ae you insert or modify data and want the version of some root instance of an aggregate to be incremented.  Note that if instead of a Bid#item entity association with @ManyToOne, you have an @ElementCollection of Item#bids, adding a Bid to the collection will increment the Item version. The forced increment then isn\u2019t necessary. You may want to review the discussion of parent/child ambiguity and how aggregates and composition work with ORM in section 7.3.  So far, we\u2019ve focused on optimistic concurrency control: we expect that concurrent modifications are rare, so we don\u2019t prevent concurrent access\u00ae and detect conflicts late. Sometimes you know that conflicts will happen frequently, and you want to place an e\u00aexclusive lock on some data. This calls for a pessimistic approach. Let\u2019s repeat the procedure shown in the section \u201cManual version checking\u201d with a pessimistic lock instead of optimistic version checking. You again summarize the total price of all items in several categories. This is the same code as shown earlier in listing 11.5, with a different LockModeType. For each Category, query all Item instances in PESSIMISTIC_READ lock mode. Hibernate locks the rows in the database with the SQL query. If possible, wait 5 seconds if another transaction holds a conflicting lock. If the lock can\u2019t be obtained, the query throws an exception. If th\u00aee query returns successfully, you know that you hold an exclusive lock on the data and no other transaction can access it with an exclusive lock or modify it until this transaction commits.  Your locks are released after commit, when the transaction completes.",
    "page243": "The JPA specification de\u00aefines that the lock mode PESSIMISTIC_READ guarantees repeatable reads. JPA also standar\u00aedizes the PESSIMISTIC_W\u00aeRITE mode, with additional guarantees:\u00ae in \u00aeaddition to repeatable reads, the JPA provider must serialize data access, and no phantom reads can occur.  It\u2019s up to the JPA provider to implement these requirements. For both modes, Hibernate appends a \u201cfor update\u201d clause to the SQL query when loading data. This places a lock on the rows at the database level. What kind of lock Hibernate uses depends on the LockModeType and your Hibernate database dial\u00aeect.  On H2, for example, the query is SELECT * FROM ITEM ... FOR UPDATE. Because H2 supports only one type of exclusive lock, Hibernate generates the same SQL for all pessimistic modes.  PostgreSQL, on the other hand, supports shared read locks: the PESSIMISTIC_READ mode appends FOR SHARE to the SQL query. PESSIMISTIC_WRITE uses an exclusive write lock with FOR UPDATE. On MySQL, PESSIMISTIC_READ translates to LOCK IN SHARE MODE, and PESSIMISTIC_ WRITE to FOR \u00aeUPDATE. Check your database dialect. Thi\u00aes is configure\u00aed with the getReadLockString() and getWriteLockString() methods.  The duration of a pessimistic lock in JPA is a single database transaction. This means you can\u2019t use an exclusive lock to block con\u00aecurrent access for longer than a single database transaction. When the database lock can\u2019t be obtained, an exception is thrown. Compare this with an optimistic approach, where Hibernate throws an exception at commit time, not when you query.",
    "page244": "With a pessimistic strategy, you know that you can read and write the data safely as soon as your locking query succeeds. With an optimistic approach, you hope for the best and may be surprised later, when you commit. You can configure how long the database will wait to obtain the lock and block the query in milliseconds with the javax.persistence.lock.timeout hint. As usual with hints, Hibernate might ignore it, depending on your database product. H2,\u00ae for example, doesn\u2019t support lock timeouts per query, only a global lock timeout per connection (defaulting to 1 second). With some dialects, such as PostgreSQL and Oracle, a lock timeout of 0 appends the NOWAIT cl\u00aeause to the SQL string.  We\u2019ve shown the lock timeout hint applied to a Query. You can also set the timeout hint for find() operations: When a lock can\u2019t be obtained, Hibernate throws either a javax.persis\u00aetence.LockTimeoutException or a javax.persistence.PessimisticLockException. If Hibernate throws a PessimisticLoc\u00aekException, the transaction must be rolled back, and the unit of work ends. A timeout exception, on the other hand, isn\u2019t fatal for the transaction, as explained in section 11.1.\u00ae4. Which exception Hibernate throws again depends on the SQL dialect. For example, because H2 doesn\u2019t support per-statement lock timeouts, you always get a PessimisticLockException.  You can use both the PESSIMISTIC_READ and PESSIMISTIC_WRITE lock modes even if you haven\u2019t enabled entity versioning. They translate to SQL statements with database-level locks The special mode PESSIMISTIC_FORCE_INCREMENT requires versioned entities, however. In Hibernate, this mode executes a FOR UPDATE NOWAIT lock (or whatever your dialect supports; check its getForUpdateNowaitString() implementation). Then, immediately after the query returns, Hib\u00aeernate increments \u00aethe version and UPDATE (!) each returned entity instance.",
    "page245": "This indicates to any concurrent transaction that you have updated these rows, even if you haven\u2019t so far modified any\u00ae data. This mode is rarely useful, mostly for aggregate locking as explained in the section \u201cForcing a version increment.\u201d If you enable pessimistic locking, Hibernate locks only rows that correspond to entity instance state. In other words, if you lock an Item instance, Hibernate will lock its row in the ITEM table. If you have a joined inheritance mapping strategy, Hibernate will recognize this and lock the appropriate rows in super- and sub-tables. This also applies to any secondary table mapp\u00aeings of an entity. Because Hibernate locks entire rows, any relationship where the foreign key is in that row will also effectively be locked: The Item#seller association is locked if the SELLER_ID foreign key column is in the ITEM table. The actual Seller instance isn\u2019t locked! Neither are collections or other associations of the Item where the foreign key(s) are in other tables.  With exclusive locking in the DBMS, you may experience transaction failures because you run into deadlock situations. \u00aeDeadlocks can occur if your DBMS relies on exclusive locks to implement transaction isolation. Consider the following unit of work, updating two Item\u00ae entity instances in a particular order: With a deadlock, both transactions are blocked and can\u2019t move forward, each waiting for a lock to be released. The chance of a deadlock is usually small, but in highly concurrent applications, two Hibernate applicat\u00aeio\u00aens may execute this kind of interleaved update.",
    "page246": "Note that you may not see deadlocks during testing (unless you write the right kinds of tests). Deadlocks can suddenly \u00aeappear when the application has to handle a high transaction load in production. Usually the DBMS terminates one of the deadlocked transactions after a timeout period and fails; the other transaction can then proceed. Alternatively, depen\u00aeding on the DBMS, the DBMS may detect a deadlock situation automatically and immediately abort one of the transactions. You should try to avoid transaction failures, because they\u2019re difficult to recover from in application code. One solution is to run the database connection\u00ae in serializable mode when updating a single row locks the entire table. The concurrent transaction has to wait until the first transaction completes its work. Alternatively, the first t\u00aeransact\u00aeion can obtain an exclusive lock on all data when you SELECT the data, as shown in the previous section. Then any concurrent transaction also has to wait until these locks are released. An alternative pr\u00aeagmatic optimization that significantly reduces the\u00ae probability of deadlocks is to order the UPDATE statements by primary key value: Hibernate should always update the row with primary key 1 before updating row 2, no matter in what order the data was loaded and modified by the application. You can enable this optimization for the entire persistence unit with the configuration \u00aeproperty hibernate.order_updates. Hibernate then orders all UPDATE statements it executes in ascending order by primary key value of the modified entity instances and collection elements detected during flushing.",
    "page247": "(As mentioned earlier, make sure you fully understand the transactional and locking behavior of your DBMS product. Hibernate inherits most of its transaction guarantees from \u00aethe DBMS; for example, your MVCC database product may \u00aeavoid read locks but probably depends on exclusive locks for writer isolation, and you may see deadlocks.)  We didn\u2019t have an opportunity to mention the EntityManager#lock() method. It accepts an already-loaded persistent entity instance and a lock mode. It performs \u00aethe same locking you\u2019ve seen with find() and a Query, except that it doesn\u2019t l\u00aeoad the instance. Additionally, if a versioned entity is being locked pessimist\u00aeically, the lock() method performs an immediate version check on the database and potentially th\u00aerows an OptimisticLockException. If the database representation is no longer present, Hibernate throws an EntityNotFoundException. Finally, the EntityManager#refresh() method also accepts a lock mode, with the same se\u00aemantics.  We\u2019ve now covered concurrency control at the lowest level the database and the optimistic and pessimistic locking features of JPA. We still have one more aspect of concurrency to discuss: accessing data outside of a transaction. A JDBC Connection is by default in auto-commit mode. This mode is useful for executin\u00aeg ad hoc SQL.  Imagine that you connect to your database with an SQL console and that you run a few queries, and maybe even update and delete rows. This interactive data access is ad hoc; most of the time you don\u2019t have a plan or a sequence of statements that you consider a unit o\u00aef work. The default auto-commit mode on the database connection is perfect for this kind of data access after all, you don\u2019t want to type\u00ae begin transaction and end transaction for every SQL statement you write and execute.",
    "page248": "In autocommit mode, a (short) d\u00aeatabase transaction begins and ends for each SQL statement you send to the database. You\u2019re working effectively in nontransactional mode, because there are no atomicity or isolation guarantees for your session with the SQL console. (The only guarant\u00aeee is that a single SQL statement \u00aeis atomic.)  An application, by definition, always executes a planned sequence of statements. It seems reasonable that you therefore always create transaction boundaries to group your statements into units that are atomic and isolated from each other. In JPA, however, special behavior is associated with auto-commit mode, and you may need it to implement long-running conversations. You can access the database in auto-commit mode and read data. Consider the following example, which loads an Item instance, changes its name, and then rolls back that change by refreshing No transaction is active when you create the EntityManager. The persistence context is now i\u00aen a special unsynchronized mode; Hibernate won\u2019t flush automatically.  You can access the database to read data; this operation executes a SELECT, sent to the database in auto-commit mode. Usually Hibernate flushes the persistence context when you execute a Query. But because t\u00aehe context is unsynchronized, flushing doesn\u2019t occur, and the query returns the old, original database value. Queries with scalar results aren\u2019t repeatable: you see whatever values are in the database and given to Hibernate in\u00ae the ResultSet. This also isn\u2019\u00aet a repeatable read if you\u2019re in synchronized mode.",
    "page249": "Retrieving a managed entity instance involves a lookup during JDBC result-set marshaling, in the current persistence context. The already-loaded Item instance with the changed name is returned from the persistence context; values from the database are ignored. This is a repeatable read of an entity instance, even without a system transaction.  If you try to flush the persistence context manually, to store the new Item#name, Hibernate throws a javax.persistence.TransactionRequiredException. You can\u2019t execute an UPDATE in unsynchronized mode, because you wouldn\u2019t be able to roll back the change.  You can roll back the change you made with the refresh() method. It loads the \u00aecurrent Item state from the database and overwrites the change you made in memory. \u00aeWith an unsynchronized persistence context, you read data in auto-commit mode with find(), getReference(\u00ae), refresh(), or queries. You can load \u00aedata on demand as well: proxies are initialized if you access them, and collections are load\u00aeed if you start iterating through their elements. But if you try to flush the persistence context or lock data with anything but LockModeType.NONE, a TransactionRequiredException will occur  So far, auto-commit mode doesn\u2019t seem very useful. Indeed, many developers often rely on auto-commit for the wrong reasons: Many\u00ae small per-statement data\u00aebase transactions (that\u2019s what auto-commit means) won\u2019t improve the performance of your application. You won\u2019t improve the scalability of your application: a longer-running database transaction, i\u00aenstead of many small transactions for every SQL statement, may hold database locks for a longer time. This is a minor issue, because Hibernate writes to the database as late as possible within a transaction (flush at comm\u00aeit), so the database alread\u00aey holds write locks for a short time.",
    "page250": "You also have weaker isolation guarantees if the application modifies data concurrently. Repeatable reads based on read locks are impossible with auto-commit mode. (The persistence context cache helps here, naturally.) If your DBMS has MVCC (for example, Oracle, PostreSQL, Informix\u00ae, and Firebird), you likely want to use its capability for snapshot isolation to avoid unrepeatable and phantom reads. Each transaction gets its own personal snapshot of the data; you only see a (database-internal) vers\u00aeion of the data as it was before your transaction started. With auto-commit mode, snapshot isolation makes no sense, because there is no transaction scope\u00ae. Your code will be more difficult to understand. Any reader of your code now has to pay special attention to whether a persistence conte\u00aext is joined with a transaction, or if it\u2019s in unsynchronized mode. If you always group operations within a system transaction, even if you only read data, everyone can follow this simple rule, and the likelih\u00aeood of difficult-to-find concurrency issues is reduced. S\u00aeo, what are the benefits of an unsynchronized persistence context? If flushing doesn\u2019t happen\u00ae automatically, you can prepare and queue modifications outside of a transaction. You can call persist() to sav\u00aee a transient entity instance with an unsynchronized persistence context. Hibernate only fetches a new identifier value, typically by calling a database sequence, and assigns it to the instance. The instance is now in persis\u00aetent state in the context, but the SQL INSERT hasn\u2019t happened. Note that this is only possible with pre-insert identifier generators; see section 4.2.5.",
    "page251": " Hibernate provides the following ways to get data out of the database and into memory: Retrieving an entity instance by identifier is the \u00aemost convenient method when the unique identifier value of an entity instance is known: for example, entityManager.find(Item.class, 123). You can navigate the entity graph, starting from an already-loaded entity instance, by accessing the associated instances through property accessor methods such as someItem.getSeller().getAddress().getCity(), and so on. Elements of\u00ae mapped collections are also loaded on demand \u00aewhen you start iterating through a collection. Hibernate automatically loads nodes of the graph if the persistence context is still open. What and how dat\u00aea is loaded when you call accessors and iterate through collections is the focus of this chapter. You can use the Java Persistence Query Language (JPQL), a full object-oriented query language based on strings such as select i from Item i where i.id?. The Crit\u00aeeriaQuery interface provides a type-safe and object-oriented way to perform queries without string manipulation. You can write native SQL queries, call stored procedures, and le\u00aet Hibernate take care of mapping the JDBC result sets to instances of your domain model classes. In yo\u00aeur JPA applications, you\u2019ll use a combination of these techniques, but we won\u2019t discuss each retrieval method in much detail in this chapter. By now you should be familiar with the basic Java Persistence API for retrieval by identifier. We keep our JPQL and CriteriaQuery examp\u00aeles as simple as possible, and you won\u2019t need the SQL query-mapping features. Because these query options are sophisticated, \u00aewe\u2019ll explore them further in chapters 15 and 17.",
    "page252": "This chapter covers what happens behind the scenes when you navigate the graph of your domain model and Hibernate retrieves data on demand. In all the examples, we show you the SQL executed by Hibernate in a comment right immediately after the operation that triggered the SQL execution. What Hibernate loads depends on the fetch plan: you define the (sub)graph of the network of objects that should be loaded. Then you pick the right fetch strategy, defining how the data should be loaded. You can store your selection of plan and strategy as a fetch profile and reuse it.  Defining fetch plans and what data should be loaded by Hibernate relies on two fundamen\u00aetal techniques: lazy and eager loading of nodes in the network of objects. At some point, you must decide what data should be loaded into memory from the database. When you execute entityManager.find(Item.class, 123\u00ae), what is available in memory and load\u00aeed into the persistence context? What happens if you use EntityManager#getReference() instead? \u00ae In your domain-model mapping, you define the global default fetch plan, with the FetchType.LAZY and FetchType.EA\u00aeGER options on associations and collections. This plan is the default setting for all operations involving your persistent domain model classes. It\u2019s always active when you load an entity instance by identifier and when you navigate the entity graph by fol\u00aelowing associations and iterating through persistent collections.  Our recommended strategy is a lazy default fetch plan for all entities and collections.\u00ae If you map all of your associations and collections with FetchType.LAZY, Hibernate will only load the data you\u2019re accessing a\u00aet this time.",
    "page253": "Consider the getReference() method of the EntityManager API. In section 10.2.3, you had a first look at this operation and how it may return a proxy. Let\u2019s further explore this important feature and find out how proxies work: This code doesn\u2019t execute any SQL against the database. All Hibernate does is create an Item proxy: it looks (and smells) like the real thing, but it\u2019s only a placeholder. In the persistence context, in memory, you now have this proxy available in persistent state\u00ae, as shown in figure 12.1.  The proxy is an instance of a runtime-generated subclass of Item, carrying the identifier value of the entity instance it represents. This is why Hibe\u00aernate (in line with JPA) requires that entity classes have at least a public or protected no-argument constructor (\u00aethe class may have other constructors, too). The entity class and its methods must not be final; otherwise, Hibernate can\u2019t produce a proxy. Note that the JPA specification doesn\u2019t mention proxies; it\u2019s up to the JPA provider how lazy loading is implemented.  If you call any method on the proxy that isn\u2019t the \u201cidentifier getter,\u201d you trigger initialization of the proxy and hit the database. If you call item.getName(), the SQL SELECT to load the Item will be executed. The previous example called item.getId() without triggering initialization because getId() is the identifier getter method in the given mapping; the getId() method was annotated with @Id. If @Id was on a field, then calling getId(), just like calling any other method, would initialize the proxy!",
    "page254": "The first two calls produce proxies of Item and User, respectively. Then the item and bidder association properties of the transient Bid are set with the proxies. The persist() call queues one SQL INSERT when the persistence context is flushed, and no SELECT is necessary to create the new row in the BID table. All (foreign) key values are\u00ae available as identifier values of the Item and User proxy.  Runtime proxy generation as provided by Hibernate is an excellent choice for transparent lazy loading. Your domain model classes don\u2019t have to implement any special (super)type, as some older ORM solutions would require. No code generation or post-processing of bytecode is needed either, simplifying your build procedure. But you should be aware of s\u00aeome potentially negative aspects:  Cases where runtime proxies aren\u2019t completely transparent are polymorphic associations that are tested with inst\u00aean\u00aeceof, a problem shown in section 6.8.1. With entity proxies, you have to be careful not to access fields directly when writing custom equals() and hashCode() methods, as discussed in section 10.3.2.  Proxies can only be used to lazy-load entity associations. They can\u2019t be used to lazy load individual basic properties or embedded components, such as Item#description or User#homeAddress. If you set the @Basic(fetch FetchType.LAZY) hint on such a property, Hibernate ignores it; the value is eagerly loaded when the owning entity instance is loaded. Although possible with bytecode instrumentation and\u00ae interception, we consider this kind of optimization to be rarely useful. Optimizing at the level of indi\u00aevidual columns selected in SQL is unnecessary if yo\u00aeu aren\u2019t working with (a) a significant number of optional/nullable columns or (b) columns containing large values that have to be retrieved on demand because of the physical limitations of your \u00aesystem.",
    "page255": "You map persistent collections with either @ElementCollectio\u00aen for\u00ae a collection of elements of basic or embeddable type or with @OneToMany and @ManyToMany for manyvalued entity associations. These collections are, unlike @ManyToOne, lazy-loaded by default. You don\u2019t have to specify the FetchType.LAZY option on the mapping.  When you load an Item, Hibernate doesn\u2019t load its lazy collection of images right away. The lazy bids one-to-many collection is also only loaded on demand, when accessed and needed: The find() operation loads the Item entity instance into the persistence context, as you can see in figu\u00aere 12.2. The Item instance has a reference to an uninitialized User proxy: the seller. It also has a reference to an uninitialized Set of bids and an uninitialized List of images.  Hibernate implements lazy loading (and dirty checking) of collections with its own special implementations called collection wrappers. Although the bids certainly look like a Set, Hibernate replaced the implementation with an org.hibernate.collection.internal PersistentSet while you weren\u2019t looking It\u2019s not a HashSet, but it has the same behavio\u00aer. That\u2019s why it\u2019s so important to program with interfaces in your domain model and\u00ae only rely on Set and not HashSet. Lists and maps work the same way.  These special collections can detect when you access them and load their data at that time. As soon as you start iterating through the bids, the collection and all bids made for the item are loaded.",
    "page256": "The fundamental problem with lazy loading is that the JPA provider must know when to load the seller of an Item or the collection of bids. Instead of runtime-generated proxies and smart collections, many other JPA providers rely exclusively on interception of method calls. For example, when you call someItem.getSeller(), the JPA provider would \u00aeintercept this call and load the User ins\u00aetance representing the seller.  This approach req\u00aeuires special code in your Item class to implement the interception: the getSeller() method or the seller field must be wrapped. Because you don\u2019t want to write this code by hand, typically yo\u00aeu run a bytecode enhancer (bundled with your JPA provider) after compiling your domain model classes. This enhancer injects the necessary interception code into your compiled classes, manipulating the fields and methods at the bytecode level.  Let\u2019s discuss lazy loading based on interception with a\u00ae few examples. First\u00ae, you probably want to disable Hibernate\u2019s proxy generation: Instead, the proprietary LazyToOneOption.NO_PROXY setting tells Hibernate that the bytecode enhancer must add interception code for the seller property. Wi\u00aethout this option, \u00aeor if you don\u2019t run the bytecode enhancer, thi\u00aes association would be eagerly loaded and the field would be populated right away when the Item is loaded, because proxies for the User entity have been disabled.  If you run the bytecode enhancer, Hibernate intercepts access of the seller field and triggers loading when you touch the field: This is less lazy than prox\u00aeies. Remember that you could call User#getId() on a proxy without initializing the instance, as explained in the previous sections. With interception, any access of the seller field, and calling getSeller(), will trigger initialization.",
    "page257": " For lazy entity associations, proxies are usually a better choice than interception. A more common use case for interception is properties of basic type, such a String or byte[], with potentially large values. We might argue that LOBs (see \u201cBinary and large value types\u201d in chapter 5) should be preferred for large strings or binary data, but you might not want to have the java.sql.Blob or java.sql.Clob type in your domain model. With interception\u00ae and bytecode enhancement, you can load a simple String or byte[] field on demand: The Item#description will be lazy loaded if you run the bytecode enhancer on the compiled class. If you don\u2019t run the bytecode enhancer for example, during development the \u00aeString will be loaded together with the Item instance. If you rely on interception, be aware that Hibernate will load all lazy fields of an entity or embeddable class, even if only one has to be loaded: When Hibernate loads the description of the Item, it loads the seller and any other intercepted field right away, too. There are no fetch groups in Hibernate at the time of writing: it\u2019s all or nothing.  The downside of interception is the cost of running a bytecode enhancer every time you build your domain model classes, and waiting for the instrumentation to complete. \u00aeYou may decide to skip the instrumentation during development, if the behavior of your application doesn\u2019t depend on an item\u2019s description load state. Then, when building the testing and production package, you can execute the enhancer.",
    "page258": "We leave it to you to decide whether you want inte\u00aerception for lazy loading in our experience, good use cases are rare. Note that we haven\u2019t talked about co\u00aellection wrappers when discussing interception: although you could enable interception for collection fields, Hibernate would still use its smart collection wrappers. The reason is that these collection wrappers are, unlike entity proxies, needed for other purposes besides lazy loading. For example, Hibernate relies on them to track additions and removals of collection elements when dirty checking. You can\u2019t disable the collection wrappers in your mappings; they\u2019re always on. (Of course, you never have to map a persistent collection; they\u2019re a feature, not a requirement. See our earlier discussion in secti\u00aeon 7.1.) Persistent arrays, on the other hand, can only be lazily loaded with field interception they can\u2019t be wrapped like collections. You\u2019ve now seen all available options for lazy loading in Hibernate. Next, we look at the opposite of on-demand loading: the eager fetching of data. We\u2019ve recommended a lazy default fetch plan, with FetchType.LAZY on all your association and collection mappings. Sometimes, although not often, you want the opposit\u00aee: to specify that a particular entity association or collection should always be loaded. You want t\u00aehe guarantee that this data is available in memory without an additional database hit.  More important, you want a guarantee that, for example, you can access the seller of an Item once the Item instance is in detached state. When the persistence context is closed, laz\u00aey loading is no longer available. If seller were an uninitialized proxy, you\u2019d get a LazyInitializationException when you accessed it in detached state.",
    "page259": "For data to be available in detached state, you need t\u00aeo either load it manually while the persistence context is still open or, if you always want it loaded, change your fetch plan to be eager instead of lazy.  Let\u2019s assume that you always require loading of the seller and the bids of an Item: Unlike FetchType.LAZY, which is a hint the JPA provid\u00aeer can ignore, a FetchType.EAGER is a hard requirement. The provider has to guarantee that the data is loaded and available in detached state; it can\u2019t ignore the setting.  Consider the collection mapping: is it really a good idea to say, \u201cWhenever an item is loaded into memory, load the bids of the item right away, too\u201d? Even if you only want to display the item\u2019s name or find out when the auction ends, all bids will be loaded into memory. Always eager-loading collections, with FetchType.EAGER as the default fetch plan in the mapping, usually isn\u2019t a great strategy. You\u2019ll also see the Cartesian product problem appear if you eagerly load several collections\u00ae, which we discuss later in this chapter. It\u2019s best if you leave collections as the default FetchType.LAZY.  If you now find() an Item (or force the initialization of an Item proxy), both the seller and all the bids are loaded as persistent instances into your persistence context.",
    "page260": "Hibernate executes SQL SELECT statements to load data into memory. If you load an entity instance, one or more SELECT\u00ae(s) are executed, depending on the number of tables involved and the fetching strategy you\u2019ve applied. Your goal is to minimize the number of SQL statements and to simplify the SQL statements so that querying can be as efficient as possible.  Consider our recommended fetch plan from earlier in this chapter: every\u00ae association and collection should be loaded on demand, lazily. This default fetch plan will most likely result in\u00ae too many SQL statements, each loading only one small piece of data. This will lead to n 1 selects problems, and we discuss this issue first. The alternative fetch plan, using eager loading, will result in fewer SQL statements, because larger chunks of data are \u00aeloaded into memory with each SQL query. You might then see the Cartesian product problem, as SQL result sets become too large.  You need t\u00aeo find the middle ground between these two extremes: the ideal fetching strategy for each procedure and use case in your application. Like fetch plans, you can set a global fetching strategy in your mappings: the default setting\u00ae tha\u00aet is always active. Then, for a particular procedure, you might override the default fetching strategy with a custom JPQL, CriteriaQuery, or even SQL query.  First, let\u2019s discuss the fundamental problems you see, starting with the n 1 selects issue.",
    "page261": "This problem is easy to understand with some example code. Let\u2019s assume that you mapped a lazy fetch plan, so everything is loaded on demand. The following example code checks whether the seller of each Item has a username You see one SQL SELECT to load the Item entity instances. Then, while you iterate through all the items, retrieving each User requires an additional SELECT. This amounts to one query for the Item plus n queries depending on how many items you have and whether a particular User is selling more than one Item. Obviously, this is a very inefficient strategy if you know you\u2019ll access the seller of each Item.  You can see the same issue with lazily loaded collections. The following example checks whether each Item has some bids: Again, if you know you\u2019ll access each bids collection, loading only on\u00aee at a time is inefficient. If you have 100 items, you\u2019ll execute 101 SQL queries!  With what you know so far, you might be tempted to change the default fetch plan in your mappings and put a FetchType.EAGER on your seller or bids associations. But doing so can lead to our next topic: \u00aethe Cartesian product problem. If you look at your domain and data model and say, \u201cEvery time I need an Item, I\u00ae also need the seller of that Item,\u201d you can map the association w\u00aeith FetchType.EAGER instead of a lazy fetch plan. You want a guarantee that whenever an Item is loaded, the seller will be loaded right away you\u00ae want that data to be available when the Item is detached and the persistence context is closed.",
    "page262": "Eager fetching with the default JOIN strategy isn\u2019t problematic for @ManyToOne and @OneToOne associations. You can eagerly load, with one SQL query and JOINs, an Item, its seller, the User\u2019s Address, the City they live in, and so on. Even if you map all these associations with FetchType.EAGER, the result set will have only one row. Now, Hibernate has to stop following your FetchType.EAGER plan at some point. The number of tables joined depends on the global hibernate.max_fetch_depth configuration property. By default, no limit is set.\u00ae Reasonable values are small, usually between 1 and 5. You may even disable JOIN fetching of @ManyToOne and @OneToOne associations by setting the property to 0. If Hibernate reaches the limit, it will still eagerly load the data according to your fetch plan, but with additional SELECT statements. (Note that some database dialects may preset this property: for example, MySQLDialect sets it to 2.)  Eagerly loading collections with JOINs, on the other hand, can lead to serious performance issues. If you also s\u00aewitched to FetchType.EAGER for the bids and images collections, you\u2019d run into the Cartesian product problem.  This issue appears when you eagerly load two collections with one SQL query and a JOIN operation. First, let\u2019s create such a fetch plan and then look at the SQL problem: It doesn\u2019t matter whether both collections are @OneToMany, @ManyToMany, or @ElementCollection. \u00aeEager fetching more than one collection at once with the SQL JOIN operator is the fundamental issue, no matter what the collection content is. If you load an Item, Hibernate executes the problemati\u00aec SQL statement.",
    "page263": "As you can see, Hibernate obeyed your eager fetch plan, and you can access the bids and images collections in detached state. The problem is how they were loaded, with an SQL JOIN that results in a product. Look at the result set in figure 12.5.  This result set contains many redundant data items, and only the shaded cells are relevant for Hibernate. The Item has three bids and three images. The size of the product depends on the size of the collections you\u2019re retrieving: three times three is nine rows total. Now imagine that you have an Item with 50 \u00aebids and 5 images you\u2019ll see a result set with possibly 250 rows! You can create even larger SQL products when you write your own queries with JPQL or CriteriaQuery: imagine what happens if you load 500 items and eager-fetch dozens of bids and images with JOINs.  Considerable processing time and memory are required on the database server to create such results, which then must be transferred across the network. \u00aeIf you\u2019re hoping that the JDBC driver will compress the data on the wire somehow, you\u2019re probably expecting too much from database vendors. Hibernate immediately removes all duplicates when it marshals the result set into persistent instances and collections; information in cells that aren\u2019t shaded in figure 12.5 will be ignored. Obviously, you can\u2019t remove these duplicates at the SQL level; the SQL DISTINCT operator doesn\u2019t help here.  Instead of one SQL query with an extremely large result, three separate queries would be faster to retrieve an entity instance a\u00aend two collections at the same time. Next, we focus on this kind of optimization and how you find and implement the best fetch strategy.",
    "page264": "If Hibernate fetches every entity association and collection only on demand, many additional SQL SELECT statements may be necessary to complete a particular procedure. As before, consider a routine that checks whether the sel\u00aeler of each Item has a username. With lazy loading, this would require one SELECT to get all Item instances and n more SELECTs to initialize the seller proxy of each Item.  Hibernate offers algorithms that can prefetch data. The first algorithm we discuss is batch fetching, and it works as follows: if Hiberna\u00aete must initialize one User proxy, go ahead and initialize several with the same S\u00aeELECT. In other words, if you already know that there are several Item instances in the persistence context and that they all have a proxy applied to their seller association, you may as well initialize several proxies instead of just one if you make the round trip to the database.  Let\u2019s see how this works. First, enable batch fetching of User instances with a proprietary Hibernate annotation: This setting tells Hibernate that it may load up to 10 User proxies if one has to be loaded, all with the same SELECT. Batch fetching is often call\u00aeed a blind-guess optimization, because you don\u2019t know how many uninitialized User proxies may be in a particular persistence con\u00aetext. You can\u2019t say for sure that 10 is an ideal value it\u2019s a guess. You know that instead of n 1 SQL queries, you\u2019ll now see n 1/10 queries, a significant reduction. Reasonable values are usually small, because you don\u2019t want to load too much data into memor\u00aey \u00aeeither, especially if you aren\u2019t sure you\u2019ll need it.  This is the optimized procedure, w\u00aehich checks the username of each\u00ae seller.",
    "page265": "Note the SQL query that Hibernate \u00aeexecutes while you iterate through the items. When you call item.getSeller().getUserName() for the first time, Hibernate must i\u00aenitialize the first User proxy. Instead of only loading a single row from the USERS table, Hibernate retrieves several rows, and up to 10 User instances are loaded. Once you access the eleventh seller, another 10 are loaded in one batch, and so on, until the persistence context contains no uninitialized User proxies. Whe\u00aen you call item.getBids().size() for the first time while iterating, a whole batch of Bid collections are preloaded for the other Item instances.  Batch fetching is a simple and often smart optimization that can significantly reduce the number of SQL statements that would otherwise be necessary to initialize all your proxies and collections. Although you may prefetch data you won\u2019t need in the end and consume more memory, the reduction in database round trips can make a huge difference. Memory is cheap, but scaling database servers isn\u2019t.  Another prefetching algorithm \u00aethat isn\u2019t a blind guess uses subselects to initialize many collections with a sin\u00aegle statement. A potenti\u00aeally better strategy for loading all bids of several Item instances is prefetching with a subselect. To enable this optimization, add a Hibernate\u00ae anno\u00aetation to your collection mapping: Hibernate remembers the original query used to lo\u00aead the items. It then embeds this initial query (slightly modified) in a subselect, retrieving the collection of bids for each Item.",
    "page266": "When you\u2019re trying to fetch several collections with one SQL query and JOINs, you run into the Cartesian product problem, as explained earlier. Instead of a JOIN operation, you can tell Hibernate to eagerly load data with additional SELECT queries and hence avoid large results and SQL products with duplicates: Hibernate uses one SELECT to load a row from the ITEM table. It then immediately executes two more SELECTs: one loading a row from the USERS table (the seller\u00ae) and the other loading several rows from the BI\u00aeD table (the bids).  The additional SELECT queries aren\u2019t executed lazi\u00aely; the find() method produces several SQL queries. You can see how Hibernate followed the eager fetch plan: all data is available in detached state.  Still, all of these settings are global; they\u2019re always active. The danger is that adjusting one setting for one problematic case in your application might have negative side eff\u00aeects on some other procedure. Maintaining this balance can be difficult, so our recommendation is to map every entity association and collection as FetchType.LAZY, as mentioned before.  A better approach is to dynamically use eager fetching and JOIN operations only when needed, for a particular procedure. As in\u00ae the previous sections, let\u2019s say you have to check the \u00aeusername of each Item#seller. With a lazy global fetch plan, load the data you need for this procedure and apply a dynamic\u00ae eager fetch strategy in a query.",
    "page267": "The important keywords in this JPQL query are join fetch, telling Hibernate to use a SQL JOIN (an INNER JOIN, actually) to retrieve the seller of each Item in the same query. The same query can be expressed with the CriteriaQuery API instead of a JPQL string: Note that \u00aefor collectio\u00aen fetching, a LEFT OUTER JOIN is necessary, because you also want rows from the ITEM table if there are no bids. We\u2019ll have much more to say about fetching with JPQL and CriteriaQuery\u00ae later in this book, in chapter 15. You\u2019ll see many more examples\u00ae then of inner, \u00aeouter, left, and right joins, so don\u2019t worry too much about these details now. Writing queries by hand isn\u2019t the only available option if you want to override the global fetch plan of your domain model dynamically. You can write fetch profiles declaratively Fetch profiles complement the fetching options in the query languages and APIs. They allow you to maintain your profile definitions in either XML or annotation metadata. Early Hibernate versions didn\u2019t have support for special fetch p\u00aerofiles, but today Hibernate supports the following:  Fetch profiles A proprietary API based on declaration of the profile with @org.hibernate.annotations.FetchProfile and execution with Session #enableFetchProfile(). This simple mechanism currently supports overriding lazy-mapped entity associations and collections selectively, enabling a JOIN eager fetching strategy for a particular unit of work.",
    "page268": "Entity graphs Specified in JPA 2.1, you can declare a graph of entity attributes and associations with the @EntityGraph annotation. This fetch plan, or a combination of plans, can be enabled as a hint when executing EntityManager #find() or queries (JPQL, criteria). The provided graph controls what should be loaded; unfortunately it doesn\u2019t control how it should be loaded. It\u2019s fair to say that there is roo\u00aem for improvement here, and we expect future versions of Hibernate and JPA to offer a unified and more powerful API.  Don\u2019t forget that you can externalize JPQL and SQL statements and move them to metadata (see section 14.4). A JPQL query is a declarative (named) fetch profile; what you\u2019re \u00aemissing is the ability to overlay different p\u00aelans easily on the same base query. We\u00ae\u2019ve seen some creative solutions with string manipulation that are best avoided. With criteria queries, on the other hand, you already have the full power of Java available to organize your query-building code. Then the value of entity graphs is being able to reuse fetch plans across any kind of query.  Let\u2019s talk about Hibernate fetch profiles first and how you can override a global lazy fetch plan for a particular unit o\u00aef work.",
    "page269": "Hibernate fetch profiles are global metadata: they\u2019re declared for the entire persistence unit. Although you could place the @FetchProfile annotation on a class, we prefer it as package-level metadata in a package-info.java: Each profile has a name. This is a simple string isolated in a con\u00aestant. Each override in a profile names one enti\u00aety association or collection. The only supported mode at the time of writing is JOIN. The profiles can now be enabled for a unit of work: The Item#seller is mapped lazy, so the default fetch plan only retrieves the Item instance.  You need the Hibernate API to enable a profile. It\u2019s then active for any operation in that unit of work. The Item#seller is fetched with a join in the same SQL statement whenever an Item is loaded with this EntityManager.  You can overlay another profile on the same unit of work. Now the Item#seller and the Item#bids collection are fetched with a join in the same SQL statement whenever an Item is loaded. Although basic, Hibernate fetch profiles can be an easy solution for fetching optimization in smaller or simpler applications. With JPA 2.1, the introduction of entity graphs enables simi\u00aelar functionality in a standard fashion An entity graph is a declaration of entity nodes and attributes, overri\u00aeding or augmenting the default fetch plan when you execute an EntityManager#find() or with a hint on query operations. This is an example of a retrieval operation using an entity graph: The name of the entity graph you\u2019re \u00aeusing is Item, and the hint for the find() operation indicates it should be the load graph.",
    "page270": " This means attributes that are specified by attribute nodes of the entity graph are treated as FetchType.EAGER, and attributes that aren\u2019t specified are treated according t\u00aeo their specified or default FetchType in the mapping.  This is the declaration of thi\u00aes graph and the default fetch p\u00aelan of the entity class: Entity graphs in metadata have names and are associated with an entity class; they\u2019re usually declared in annotations on top of an entity class. You can put them in XML if you like. If you don\u2019t give an entity graph a name, it gets the simple name of its owning entity class, which here is Item. If you don\u2019t specify any attribute nodes in the graph, like the empty entity graph in the last example, the defaults of the entity class are used. In Item, all associations and collections are mapped l\u00aeazy; this is the default fetch plan. Hence, what you\u2019ve done so far makes little difference, and the find() operation without any hints will produce the same result: the Item instance is loaded, and the seller, bids, and images aren\u2019t.  Alternatively, you can build an entity graph with an API: This is again an empty entity graph\u00ae with no attribute nodes, given directly to a retrieval operation.  Let\u2019s say you want to write an entity graph that changes the lazy default of Item#seller to eager fetching, when enabled.",
    "page271": " We explore the following data-filtering features and APIs: In section 13.1, you learn to react to state changes of an entity instance and cascade the state change t\u00aeo associated entities. For example, when a User is saved, Hibernate ca\u00aen transitively and automatically save\u00ae all related BillingDetails. When an Item is deleted, Hibernate can delete all Bid instances associated with that Item. You can ena\u00aeble this standard JPA feature with special attributes in your entity association and collection mappings. The Java Persistence standard includes life cycle callbacks and event listeners. An event listener is a class you write with special methods, called by Hibernate when an entity instance changes state: for example\u00ae, after Hibernates loads it or is about to delete it from the database. These callback methods can also be on your entity classes and marked with special annotations. This gives you an opportunity to execute custom side effects when a transition occurs. Hibernate also has several proprietary extension points that allow interception of life cycle events at a lower level within its engine, which we discuss in section 13.2. A common side effect is writing an audit log; such a log typically contains information about the data that was changed, when the change was made, and who made the modification. A more\u00ae sophisticated auditing system might require storing several versions of \u00aedata and temporal views; you might want to ask Hibernate to load data \u201cas it was last week.\u201d This being a complex problem, we introduce Hibernate Envers in section 13.3, a subproject dedicated to versioning and auditing in JPA applications.",
    "page272": "In section 13.4, you see that data filters are also available as a proprietary Hibernate API. These filters add custom restrictions to SQL SELECT statements executed \u00aeby Hibernate. Hence, you can effectively define a custom limited view of the data in the application tier. For example, you could apply \u00aea filter that restricts loaded data by sales region, or any other authorizat\u00aeion criteria. When an entity instance changes state for example, when it was transient and becomes persistent associated entity instances may also be included in this state transition. This cascading of state transitions isn\u2019t enabled by default; each entity instance has an independent life cycle. But for some associations between entities, you may want to implement fine-grained life cycle dependencies.  For example, in section 7.3, you created an association between the Item and Bid entity classes. In this case, not only did you make the bids of an Item automatically persistent when they were added to an Item, but they were also automatically deleted when the owning I\u00aetem was deleted. You effectively made Bid an entity class that was dependent on another entity, Item.  The cascading settings you enabled in this association mapping were CascadeType.PERSIST and CascadeType.REMOVE. We also talked about the special s\u00aewitch orphanRemoval and how cascading deletion at the database level (with the foreign key ON DELETE option) affects your application.  You should review this association mapping and its cascading settings; we won\u2019t repeat it here. In this section, we look at some other, rarely used cascading options.",
    "page273": "If you\u2019re curious, you\u2019ll find more cascading options defined in the org.hibernate .annotations.CascadeType enumeration. Today, though, the only interesting option is REPLICATE and the Session#replicate() operation. All other Session operations have a standardized equivalent or alternative on \u00aethe EntityManager API, so you can ignore these settings.  We\u2019ve already covered the PERSIST and REMOVE options. Let\u2019s look at transitive detachment, merging, refreshing, and replication. Let\u2019s say you want to retrieve an Item and its bids fr\u00aeom the database and work with this data in detached state. The Bid class maps this association with an @ManyToOne. It\u2019s bidirectional with this @OneToMany collection mapping in Item: Transitive detachment and merging is enabled with the DETACH and MERGE ca\u00aescade types. Now you load the Item and initialize its bids collection: The EntityManager#detach() operation is cascaded: it evicts the Item instance from the persistence context as well as all bids in the collection. If the bids aren\u2019t loaded, they aren\u2019t detached. (Of course, you could have closed the persistence context, effectively detaching all loaded entity instances.)  In detached state, you change the Item#name, create a new Bid, and link it with the Item: Because you\u2019re working with detached entity state and collections, you have to pay extra attention to identity and equality. As explained in section 10.3, you should override the equals() and hashCode() methods on the Bid entity class: Two Bid instances are equal when they have the sam\u00aee amount and are linked with the same Item.  After you\u2019re done with your modifications in detached state, the next step is to store the changes. Using a new persistence context, merge the detached Item and let Hibernate detect the changes.",
    "page274": "Hibernate merges the detached item. First it checks whether the persistence context already contains an Item with the given identifier\u00ae value. In this case, there isn\u2019t any, so the Item is loaded from the database. Hibernate is smart enough to know that it will also need the bids during merging, so it fetches them right away in the same SQL query. Hibernate then copies the detached item values onto the lo\u00aeaded i\u00aenstance, which it returns to you in persistent state. The same procedure is applied to every Bid, and Hibernate will detect that one of the bids is new. Hibernate made the new Bid persistent during merging. It now has an identifier value assigned.  When you flush the persistence context, Hibernate detects that the name of the Item changed during merging. The new Bid will also be stored. Cascaded merging with collections is a powerful fe\u00aeature; consider how much code you would have to write with\u00aeout Hibernate to implement this functionality.  The User entity class has a one-to-many relationship with BillingDetails: each user of the application may have several \u00aecredit cards, bank accounts, and so on. If you aren\u2019t \u00aefamiliar with the BillingDetails class, review the mappings in chapter 6.  You can map the relationship between User and BillingDetails as a unidirectional one-to-ma\u00aeny entity association (there is no @ManyToOne): The cascading options enabled for this association are PERSIST and REFRESH. The PERSIST option simplifies storing billing details; they become persistent when you add an instance of BillingDetails to the collection of an already persistent User.",
    "page275": " In section 18.3, we\u2019ll discuss an architecture where the persistence context may be open for a long time, leading to managed entity instances in the context becoming sta\u00aele. Therefore, in some long-running conversations, you\u2019ll want to reload them from the database. The REFRESH cascading option ensures that when you reload the state of a User instance, Hibernate will also refresh the state of each BillingDetails instance linked to the User: An instance of User \u00aeis loaded from the database.  Its lazy billingDetails collection is initialized when you iterate through the elements or when you call size().  When you refresh() the managed User instance, Hibernate cascades the operation to the managed BillingDetails and refreshes each with an SQL SELECT. If none of these instances remain in the database, Hibernate throw\u00aes an EntityNotFoundException. Then, Hibernate refreshes the User instance and eagerly loads the entire billingDetails collection to discover any new BillingDetails. This is a case where Hibernate isn\u2019t as smart as it could be. First it executes an SQL SELECT for each BillingDetails instance in the persistence context and referenced by the collection. Then it loads the entire collection again to find any added BillingDetails. Hibernate could obviously do this with one SELECT.  The last cascading option is for the Hibernate-only replicate() operation.",
    "page276": "You first saw replication in section 10.2.7. This nonstandard operation is available on the Hibernate Session API. The main use case is copying data from one database into another.  Consider this many-to-one entity association mapping between Item and User: When you call replicate() on the detached Item, Hibernate executes SQL SELECT statements to find out whether the Item and its seller are already present in the database. Then, on commit, when the p\u00aeersistence context is flushed, Hibernate writes t\u00aehe values of the Item and the seller into the target database. In the previous example, these rows were already present, so you see an UPDATE of each, overwriting the values in the database. If the target database doesn\u2019t contain the Item or User, two INSERTs are made.  The last cascading option we\u2019re going to discuss is a global setting, enabling transitive persistence for all entity associations. An object persistence layer is said to implement persistence by reachability if any instance becomes persistent whenever the application creates a reference to the instance from another instance that is already persistent. In the purest form of persistence by reachability, the database has some top-level or root object from which all persistent objects are reachable. Ideally, an instance should become transient and be deleted from the database if it isn\u2019t reach\u00aeable via references from the root persistent object.",
    "page277": "Neither Hibernate nor any other ORM solutions implement this. In fact, there is no analogue of the root pers\u00aeistent object in any SQL database, and no persistent garbage collector can detect unreferenced inst\u00aeances. Object-oriented (network) data stores may implement a garbage-collection algorithm, similar to the one implemented for in-memory objects by the JVM; but this option isn\u2019t available in the ORM world, and scanning all tables for unreferenced rows won\u2019t perform acceptably.  Still, there is some value in the concept of persistence by reachability. It helps you make transient instances persistent and propagate their state to the database without many call\u00aes to the persistence manager.  You can enable cascaded persistence for all entity associations in your orm.xml mapping metadata, as a default setting of the persistence unit: Hibernate now considers all entity associations in the domain model mapped by this persistence unit as CascadeType.PERSIST. Whenever you create a reference from an already persistent entity instance to a transient entity instance, Hibernate automatically makes that transient instance persistent.  Cascading options are effectively predefined reactions to life cycle events in the persistence engine. If you ne\u00aeed to implement a custom procedure when data is stored or loaded, yo\u00aeu can implement your own event listeners and interceptors. In this section, we discuss three different APIs for custom event listeners and persistence life cycle interceptors available in JPA and Hibernate. You can  Use the standard JPA life cycle callback methods and event listeners. Write a proprietary org.hibernate.Intercep\u00aetor and activate it on a Session.  Use extension points of the Hibernate core engine with the org.hibernate.event SPI. Let\u2019s start with the standard JPA callbacks. They offer easy access to persist, load, and remove life cycle events.",
    "page278": "Let\u2019s say you want\u00ae to send a notification email to a system administrator whenever a new entity instance is stored. First, write a life cycle event listener with a callback method, annotated with @PostPersist, as shown in the following listing An entity listener class must have either no constructo\u00aer or a public no-argument constructor. It doesn\u2019t have to implement any\u00ae special interfaces. An entity listener is stateless; the JPA engine automatically \u00aecreates and destroys it. You may annotate\u00ae any method of an entity listener class as a callback method for persistence life cycle events. The notifyAdmin() method is invoked after a new entity instance is stored in the database.  Because event listener classes are stateless, it can be difficult to get more contextual information when y\u00aeou need it. Here, you want the currently logged-in user and access to the email system to send a notification. A primitive solution is to use thread-local variables and singletons; you can find the source for CurrentUser and Mail in the example code A callback method of an entity listener class has a single Object parameter: the entity instance involved in the state change. If you only enable the callback for a particular entity type, you may declare the argument as that specific type. The callback method may have any kind of access; it doesn\u2019t have to be public. It must not be static or final and return nothing. If a callback method throws an unchecked RuntimeException, Hibernate will abort the operation and mark the current transaction for rollback. If a callback method declares and throws a chec\u00aeked Exception, Hibernate will wrap and treat it as a RuntimeException.",
    "page279": "Let\u2019s assume that you want to write an audit log of data modifications in a separate database table. For example, you may record information about creation and update events for each Item. The audit log includes the user, the date and time of the event, what type of event occurred, and the identifier of the Item that was changed.  Audit logs are often handled using database triggers. On the other hand, it\u2019s sometimes better for t\u00aehe application to take responsibility, especially if portability between different databases is required.  You need several elements to implement audit logging. First, y\u00aeou have to mark the entity classes for which you want to enable audit logging. Next, you define what information to log, such as the user, date, time, and type of modification. Finally, you tie it all together with an org.hibernate.Interceptor that automatically creates the audit t\u00aerail.  First, create a marker interface, Auditable: This interface requires that a persistent entity class expose its identifier with a getter method; you need this property to log the audit trail. Enabling audit logging for a particular persistent class is then trivial. You \u00aeadd it to the class declaration, such as for Item: You want to store an instanc\u00aee of AuditLogRecord whenever Hibernate inserts or updates an Item in the database. A Hibernate interceptor can handle this automatically. Instead of implementing all methods in org.hibernat\u00aee.Interceptor, extend the EmptyInterceptor and override only the methods you need, as shown next.",
    "page280": "You need to access the database to write the audit log, so this interceptor needs a Hibernate Session. You also want to store the identifier of the currently logged-in user in each audit log record. The inserts and updates instance variables are collections where this interceptor will hold its internal state.  This method is called when an entity instance is ma\u00aede persistent.  This method is called when an entity instance is detected as dirty during flushing of the persistence context. The interceptor collec\u00aets the modified Auditable instances in inserts and updates. Note that in onSave(), there may not be an identifier value assigned to the given entity instance. Hibernate guarantees to set entity identifiers during flushing, so the actual audit log trail is written in the postFlush() callback, which isn\u2019t shown i\u00aen listing 13.2: This method is called after flushing of the persistence context is complete. Here, you write the audit log records for all insertions and updates you collected ea\u00aerlier.  You can\u2019t access the original persistence context: the Session that is currently executing this interceptor. The Session is in a fragile state during interceptor calls. Hibernate lets you create a new Session that inherits some information from the original Session with the sessionWithOptions() method. The new temporary Session works with the same transaction and\u00ae database connection as the original Session. You store a new AuditLogRec\u00aeord for each insertion and update using the temporary Session.  You flush and close the temporary Session independently from\u00ae the original Session You\u2019re now ready to enable this interceptor with a Hibernate property when creating an EntityMa\u00aenager.",
    "page281": "This EntityManager now has an enabled AuditLogInterceptor, but the interceptor must also be configured with the current Session and logged-i\u00aen user identifier. This involves some typecasts to access the Hibernate API: The EntityManager is now ready for\u00ae use, and an audit trail wi\u00aell be written whenever you store or modify an Item instance with it.  Hibernate interceptors are flexible, and, unlike JPA event listeners and callback methods, you have access to much more contextual information when an event occurs. Having said that, Hibernate allows you to hook even deeper into its core with the extensible event system it\u2019s based on.  The Hibernate core engine is based on a model of events and listeners. For example, if Hibernate needs to save an entity instance, it triggers an eve\u00aent. Whoever listens to this kind of event can catch it and \u00aehand\u00aele sa\u00aeving the data. Hibernate therefore implements all of its core functionality as a set of default listeners, which can handle all Hibernate events.  Hibernate is open by design: you can write and enable your own listeners for Hibernate events. You can either replace the existing default listeners or extend them and execute a side effect or additional procedure. Replacing the event listeners is rare; doing so implies that your own listener implementation can take care of a piece of Hibernate core functionality.  Essentially, all the methods of the Se\u00aession interface (and its narrower cousin, the EntityManager\u00ae) correlate to an event. The find() and load() methods trigger a LoadEvent, and by default this event is processed with the DefaultLoadEventListener.",
    "page282": "A custom listener should implement the appropriate interface for the event it wants to process and/or extend one of the convenience base classes provided by Hibernate, or any of the default even\u00aet listeners. Here\u00ae\u2019s an example of a custom load event listener. This listener performs custom authorization\u00ae code. A listener should be considered effectively a singleton, meaning it\u2019s shared between persistence contexts and thus shouldn\u2019t save any transaction-related state as instance variables. For a list of all events and listener interfaces in native Hibernate, see the API Javadoc of the org.hibernate .event package.  You enable listeners for each core event in your persistence.xml, in a <persistenceunit>: The property name of the configuration setting always starts\u00ae with hibernate .ejb.event, followed by the type of event you want to listen to. You can find a list of all event types in org.hibernate.event.spi.Even\u00aetType. The value of the property can be a comma-separated list of listener class names; Hibernate will call each listener in the specified order.  You rarely have t\u00aeo extend the Hibernate core event system with your own functionality. Most of the time, an org.hibernate.Interceptor is flexible enough. It helps to have more option\u00aes and to be able to replace any piece of the Hibernate core engine in a modular fashion.  The audit-logging implementation you saw in the previous section was very simple. If you need to log more information for auditing, such as the actual changed property values of an entity, consider Hibernat\u00aee Envers.",
    "page283": "Envers is a project of the Hibernate suite dedicated to audit logging and keeping multiple versions of data in the data\u00aebase. This is similar to version control systems you may already be familiar with, such as Subversion and Git.  With Envers enab\u00aeled, a copy of your data is automatically stored in separate database tables when you add, modify, or delete data in the main tables of the application. Envers internally uses the Hibernate event SPI you saw in the previous section. Envers listens to Hibernate events, and when Hibernate stores changes in the database, Envers creates a copy of the data and logs a revision in its own tables.  Envers groups all data modifications in a unit of work that is, in a transaction  as a change set with a revision number. You can write queries with the Envers API to retrieve historical data given a revision number or timestamp: for example, \u201cfind all Item instan\u00aeces as they were last Friday.\u201d Fir\u00aest you have to enable Envers in your application. Envers is available without further configuration as soon as you put its JAR file on your classpath (or, as shown in the example code of this book, include it as a Maven depend\u00aeency). You enable audit logging selectively for an entity class with the @org.hibernate .envers.Audited annotation. You\u2019ve now enabled audit logging for Item instances and all properties of the entity. To disable audit logging for a particular property,\u00ae annotate it with @NotAudited. In this case, Envers ignores the bids but audits the seller. You also have to enable auditing with @Audited on t\u00aehe User cl\u00aeass.",
    "page284": "Hibernate will now generate (or expect) additional database tables to hold historical data for each Item and User. Figure 13.1 shows the schema for these tables.  The ITEM_AUD and USERS_AUD tables are where the modification history of Item and User instances is stored. When you modify data and commit a transaction, Hibernate inserts a new revision number with a timestamp into the REVINFO table. Then, for each modified and audited entity instance involved in the change set, a copy of its data is stored in the audit tables. Foreign keys on revision number columns link the change set together. Th\u00aee REVTYPE column holds the type of change: whether the entity instance was inserted, updated, or deleted in the transaction. Envers never automatically removes any revision information or historical data; even after you remove() an Item instance, you still have its previous versions stored in ITEM_AUD.  Let\u2019s run through some transactions to see how this works. In the following code examples, you see several transactions involving an Item and its s\u00aeeller, a User. You create and store an Item and User, then modify both, and then finally delete the Item.  You should already be familiar with this code. Envers automatically creates an audit trail when you work\u00ae with the EntityManager: Envers transparently writes the audit trail for this sequence of transactions by logging three change sets. To access this historical data, you first have to obtain the number of the revision, representing the change set you\u2019d like to access.",
    "page285": "The main Envers \u00aeAPI is AuditReader. It can be accessed with an EntityManager.  Given a timestamp, you can find the revision number of a change set made before or on that timestamp.  If you don\u2019t have a timestamp, you can get all revision numbers in which a particular audited entity instance was involved. This operation finds all change sets where the given Item was created, modified, or deleted. In our example, we created, modified, and then deleted the Item. Hen\u00aece, we have three revisions.  If you have a revision number, you can get the timestamp when Envers logged the change set.  We created and modified the User, so there are two revisions. In listing 13.5, we assumed that either you know the (approximate) timestamp for a transaction or you have the identifier value of an entity so you can obtain its revisions. If you have neither, you may want to explore the audit log with queries. This is also useful if you have to show a list of all change sets in the user interface of \u00aeyour application The following code discovers all revisions of the Item entity class a\u00aend loads each Item version and the audit log information for that change set: If you don\u2019t know modification timestamps or revision numbers, you can write a query with forRevisionsOfEntity() to obtain all a\u00aeudit trail details of a particular entity. This query returns the audit trail details as a List of Object[].  Each result tuple contains the entity instance for a particular revision, the revision details (including revision number and timestamp), as well as the revision type. The revision t\u00aeype indicates why Envers created the revision, because the entity instance was inserted, modified, or deleted in the database.",
    "page286": "The find() method r\u00aeeturns an audited entity inst\u00aeance version, given a revision. This operation loads the Item as it was after creation.  This operation loads the Item after it w\u00aeas updated. Note how the modified seller of this change set is also retrieved automatically.  In this revision, the Item was deleted, so fin\u00aed() returns null.  The example didn\u2019t modify the User in this revision, so Envers returns its closest historical revision. The AuditReader#find() operation retrieves only a single entity instance, like EntityManager#find(). But the returned entity instances are not in persistent state: the persistence context doesn\u2019t manage them. If you modify an older version of Item, Hibernate won\u2019t update the database. Consider the entity instances returned by the AuditReader API to be detached, or read-only. AuditReader also has an API for execution of \u00aearbitrary queries, similar to the native Hibernate Criteria API (see section 16.3). This query returns Item instances restricted to a particular revision and change set.  You can add further restrictions to the query; here the Item#name must start with \u201cBa\u201d.  Restrictions can include entity associations: for example, you\u2019re looking for the revision of an Item sold by a particular User.  You can order query results.  You can paginate through large results. Envers supports projection. The following query retrieves only the Item#name of a particular version: Finally, you may want to roll back an entity instance to an older version. This can be accomplished with the Session#replicate() operation and overwriting an existing row. The following example loads the User instance from the first change set and then overwrites the current User in the database with the older version.",
    "page287": "Envers will also track this change as an update in the audit log; it\u2019s just another\u00ae new revision of the User instance.  Temporal data is a complex subject, and we encourage you to read the Envers referenc\u00aee documentation for more information. Adding details to the audit log, such as the user who made a change, isn\u2019t \u00aedifficult. The documentation also shows how you can configure different tracking strategi\u00aees and customize the database schema used by Envers.  Next, imagine that you don\u2019t want to see all the data in your database. For example, the currently logged-in application user may not have the rights to see everything. Usually, you add a \u00aecondition to your queries and restrict the result dynamically. This becomes difficult if you have to handle a concern such as security, because you\u2019d have to customize most of the queries in your application. You ca\u00aen centralize and isolate these restrictions with Hibernate\u2019s dynamic data filters.  The first use case for dynamic data filtering relates to data security. A User in CaveatEmptor m\u00aeay have a ranking property, which is a simple integer: Now assume that users can only bid on items that other users offer with an equal or lower rank. In business terms, you have several groups of users that are defined \u00aeby an arbitrary rank (a number\u00ae), and users can trade only with people who have the same or lower rank.  To implement this requirement, you\u2019d have to customize all queries that load Item instances from the database. You\u2019d check whether the Item#seller you want to load has an equal or lower rank than the currently logged-in\u00ae user. Hibernate can do this work for you with a dynamic filter.",
    "page288": "First, you define your filter wi\u00aeth a name and the dynamic runtime parameters it accepts. You can place the Hibernate annotation for this definition on any entity class of your domain model or in a package-info.java\u00ae metadata file: This example names this filter limitByU\u00aeserRank; note that filter names must be unique in a persistence unit. It accepts one runtime argument of type int. If you have several filter definitions, declare them within @org.hibernate.annotations.FilterDefs. The filter is inactive now;\u00ae nothing indicates that it\u2019s supposed to apply to Item instances. You must apply and implement the filter on the classes or collectio\u00aens you want to filter You want to apply the defined filter on the Item class so that no items are visibl\u00aee if the logged-in user doesn\u2019t have the necessary rank: The condition is an SQL expression that\u2019s passed through directly to the database system, so you can use any SQL operator or function. It must evaluate to true if a record should pass the filter. In this example, you use a subquery to obtain the rank of the seller of the item.\u00ae Unqualified columns, such as SELLER_ID, refer to the table mapped to the entity class. If the currently logged-in user\u2019s rank isn\u2019t greater than or equal to the rank returned by the subquery, the Item instance is filtered out. \u00aeYou can apply several filters by grouping them in an @org.hibernate.annotations.Filters.  A defined and applied filter, if enabled for a particular unit of work, filters out any Item instance that doesn\u2019t pass the condition. Let\u2019s enable it.",
    "page289": "You\u2019ve defined a data filter and applied it to a persistent entity class. It\u2019s still not filtering anything it must be enabled and parameterize\u00aed in the application for a particular unit of work, with the Session API: You enable the filter by name; the method returns a Filter on which you set the runtime arguments dynamically. You must set the parameters you\u2019ve defined; here it\u2019s set to rank 0. This example then filters out Items sold by a User w\u00aeith a higher rank in this Session. Other useful methods of the Filter are getFilterDefinition() (which allows you to iterate through the parameter names an\u00aed types) and validate() (which throws a HibernateException if you forget to set a parameter). You can also set a list of arguments with setParameterList(); this is mostly useful if your SQL restriction contains an expression with a quantifier operator (the IN operator, for example).  Now, every JPQL or criteria query that you execute on the filtered persistence context restricts the returned Item instances: Note how Hibernate dynamically appends the SQL restriction conditions to the statement \u00aegenerated.  When you first experiment with dynamic filters, you\u2019ll most likely run into an issue with retrieval by identifier. You might expect that em.find(Item.class, ITEM_ID) will be filtered as well. This is not the case, though: Hibernate \u00aedoesn\u2019t apply filters to retrieval by identifier operations. One of the reasons is that data-filter conditions are SQL fragments, and lookup by identifier may be resolved completely in memory, in the first-level persistence context cache. Similar reasoning applies to filtering of manyto-one or one-to-one associations.",
    "page290": " Common to all APIs, a query must be prepared in application code before execution. There are three distinct steps:  Create the query, with any arbitrary selection, restriction, and projection of data that you want to retrieve.  Prepare the query: bind runtime arguments to query parameters, set hints, and set paging options. You can reuse the query with changing settings.  Execute the prepared query against the database and retrieve the data. You can control how the query is executed and how data should be retrieved into memory (all at once or piecemeal, for example). Depending on the query options you use, your starting point for query creation is either the EntityManager or the native Session API. First up is creating the query. JPA represents a query with a javax.persistence.Query or javax.persistence .TypedQuery instance. You create queries with the EntityManager#createQuery() method and its variants. You can write the query in the Java Persistence Query Language (JPQL), construct it with the CriteriaBuilder and CriteriaQuery APIs, or use plain SQL. (There is also javax.persistence.StoredProcedureQuery, covered in section 17.4\u00ae.)  H\u00aeibernate has its own, older API to represent queries: org.hibernate.Query and org.hibernate.SQLQuery. We talk more about these in a moment. Let\u2019s start with the JPA standard interfaces and query languages. Say you want to retrieve all Item entity instances from the database. With JPQL, this simple query \u00aestring looks quite a bit like the SQL you know: The JPA provider returns a fresh Query; so far, Hibernate hasn\u2019t sent any SQL to the database. Remember that further preparation and execution of the query are separate steps.",
    "page291": "JPQL is compact and will be familiar to anyo\u00aene with SQL \u00aeexperience. Instead of table and column names, JPQL relies on entity c\u00aelass and property names. Except f\u00aeor these class and property names, JPQL is case-insensitive, so it doesn\u2019t matter whether you write SeLEct or select. JPQL (and SQL) query strings can be simple Java literals in your code, as you saw in the previous example. Alternatively, especially in larger applications, you can move the query strings out of your data-access code and into annotations or XML. A query is then acc\u00aeessed by name with EntityManager#createNamedQuery(). We discuss externalized queries separately later in this chapte\u00aer; there are many options to consider.  A significant disadvantage of JPQL surfaces as problems during refactoring of the domain model: if you r\u00aeename the Item class, your JPQL query will break. (Some IDEs can detect\u00ae and refactor JPQL strings, though.) First you get a CriteriaBuilder from your EntityManager by calling getCriteriaBuilder(). If you don\u2019t have an EntityManager ready, perhaps because you want to create the query independently from a particular persistence context, you may obtain the Cri\u00aeteriaBuilder from the usually globally shared EntityManagerFactory.  You then use the builder to create any number of CriteriaQuery instances. Each CriteriaQuery has at least one root class specified with from(); in the last example, that\u2019s Item.class. This is called selection; we\u2019ll talk more about it in the next ch\u00aeapter. The shown query returns all Item instances from the database.",
    "page292": "The CriteriaQuery API will appear seamless in your application, without string manipulation. It\u2019s the best choice when you can\u2019t fully specify \u00aethe query at development time and the application must create it dynamically at runtime. Imagine that you have to implement a search mask in your application, with many check boxes, input fields, and switches the user can enable. You must dynamically create a database query from the user\u2019s chosen search options. With JPQL and string concatenation, such code would be difficult to write and maintain.  You can write strongly typed CriteriaQuery calls, without strings, using the static JPA metamodel. This means your queries will be safe and included in refactoring operations, as already shown in the section \u201cUsing a static metamodel\u201d in chapter 3. After execution of this SQL query, Hibernate reads the java.sql.ResultSet and creates a List of managed Item entity instances. Of course, all columns necessary to construct an Item must be available in the result, and an error is thrown if your SQL query doesn\u2019t return them properly.  In practice, the majority of the queries in your application will be trivial easily expressed in JPQL or with a CriteriaQuery. Then, possibly during optimization, you\u2019ll fi\u00aend a handful of complex and performance-critical queries. You may have to use special and proprietary SQL keywords to control the optimizer of your DBMS product. Most developers then write SQL instead of JPQL and move such complex queries into an XML file, where, with the help of a DBA, you change them independently from Java code.\u00ae Hibernate can still handle \u00aethe query result for you; hence you integrate SQL into your JPA application.",
    "page293": "There is nothing wrong with using SQL in Hibernate; don\u2019t let some kind of ORM \u201cpurity\u201d get in your way. When you have a sp\u00aeecial case, don\u2019t try to hide it, but rather expose and document it pr\u00aeo\u00aeperly so the next engineer will understand what\u2019s going on.  In certain cases, it\u2019s useful to specify the type of data returned from a\u00ae query. A query has several aspects: it defines what data should be loaded from the database and the restrictions that apply, such as \u00aethe identifier of an Item or the name of a User. W\u00aehen you write a query, you shouldn\u2019t code these arguments into the query string using string concatenation. You should use parameter placeholders instead and then bind the argument values before execution. This allows you to reuse the query with different argument values while keeping you\u2019re safe from SQL injection attacks.  Depending on your user inter\u00aeface, you frequent\u00ael\u00aey also need paging. You limit the number of rows returned from the database by your query. For example, you \u00aemay want to return only result rows 1 to 20 because you can only show so much data on each screen, then a bit later you want rows 21 to 40, and so on.  Let\u2019s start with parameter binding.",
    "page294": "You should never write code like this, because a malicious user could craft a search string to execute code on the database you didn\u2019t expect or want that is, by entering the value of searchString in a search dialog box as foo' and callSomeStoredProcedure() and 'bar''bar.  As you can\u00ae see, the original searchString is no longer a simple search for a string but also executes a stored procedure in the database! The quote characters aren\u2019t escaped; hence the call to the stored procedure is another valid expression in\u00ae the query. If you write a query like this, you open a major security hole in your application b\u00aey allowing the execution of arbitrary code on your database. This is an SQL injection attack. Never pass unchecked values from user input to the database! Fortunately, a simple mechanism prevents this mistake.  The JDBC API includes functionality for safely binding values to SQL parameters. It knows exactly what characters in the parameter value to escape so the previous vulnerability doesn\u2019t exist. For example, the database\u00ae driver escapes the single-quote characters in the given se\u00aearchString and no longer treats them as control characters but as \u00aea part of the search string value. Furthermore, when you use parameters, the database can \u00aeefficiently cache precompiled prepared statements, improving performance significantly.  There are two approaches to parameter binding: named and positional parameters. JPA support both options, but you can\u2019t use both at the same time for a particular query.",
    "page295": "A commonly used\u00ae technique to process large result sets is paging\u00ae. Users may see the result of their search request (for example, for specific items) as a page. This page shows a limited subset (say, 10 items) at a time, and users can navigate to the next and previous pages manually to view the rest of the result.  The Query interface supports paging of the\u00ae query result. In this query, the requested\u00ae page starts in the middle of the result set: Starting from the fortieth row, you retrieve the next 10 rows. The call to setFirstResults(40) starts the result set a\u00aet row 40. The call to setMaxResults(10) limits the query result set to 10 rows returned by the database. Because there is no standard way to express paging in SQL, Hibernate knows the tricks to make this work efficiently on your particular DBMS.  It\u2019s crucially important to remember that paging operates at the SQL level, on result rows. Limiting a result to 10 rows isn\u2019t necessarily the same as limiting the result to 10 inst\u00aeances of Item! In section 15.4.5, you\u2019ll see some queries with dynamic fetching that can\u2019t be combined with row-based paging at the SQL level, and we\u2019ll discuss this issue again.  You can even add this flexible paging option to an SQL query: Hibernate will rewrite your SQL query to include the necessary keywords and clauses for limiting the number of returned rows to the page you specified.  In practice, you frequently combine paging with a special count-query. If you show a page of items, you also let the user know the total count of items. In addition, you need this information to decide whether there are more pages to sho\u00aew and whether the user can click to t\u00aehe next page.",
    "page296": "Maintaining two almost identical queries is overhead you should avoid. A popular trick is to write only one query but execute it with a database cursor first to get the total result count:  Unwrap the Hibernate API to use scrollable cursors.  Execute the query with a database cursor; this doesn\u2019t retriev\u00aee the result set into memory.  Jump to the last row of the result in the database, and then get the row number. Because row numbers are zero-based, add 1 to get the total count of rows.  You must close the database curso\u00aer.  Execute the query again, and retrieve an arbitrary page of data. There is o\u00aene significant problem with this convenient strategy: your JDBC driver and/or DBMS may not support database cursors. Even worse, cursors s\u00aeeem to work, but the data is silently retrieved into application memory; the cursor isn\u2019t operating directly on the database. Oracle and MySQL drivers are known to be problematic, and we have more to say about scrolling and cursors in \u00aethe next section. Later in this book, in section 19.2, we\u2019ll further discuss paging strategies in an application environment.  Your query is now ready for execution. Once you\u2019ve created and prepared a Query, you\u2019re ready to execute it and retrieve the resul\u00aet into memory. Retrieving the entire result set into memory in one go is the most common way to execute a query; we call this listing. Some other options are av\u00aeailable that we also discuss next, such as scrolling and iterating.",
    "page297": "Hibernate executes one or several SQL SELECT statements immediately, depending on your fetch plan. If you map any associations or collections with FetchType.EAGER, Hibernate must fetch them in addition to the data you want ret\u00aerieved with your query. All data is loaded into memory, and any entity instances that Hibernate retrieves are in persistent state and managed by the persistence context.  Of course, the persistence context doesn\u2019t manage scalar projection results. The following query returns a List of Strings: With some queries, you know the result is only \u00aea single result for example, if you want only the highest Bid or only one Item. Plain JDBC provides a feature called scrollable result sets. Thi\u00aes technique uses a cursor that the database management system holds. The cursor points to a particular row in the result of a query, and the application can move the cursor forward and backward. You c\u00aean even directly jump to a row with the cursor.  One of the situations where you should scroll through the results of a query instead of loading them all into memory involves result sets that are too large to fit into memory. Usually you try to restrict the result further b\u00aey tightening the con\u00aeditions in the query. Sometimes this \u00aeisn\u2019t possible, maybe because you need all the data but want to retrieve it in several steps. We\u2019ll show such a batch-processing routi\u00aene in section 20.1. JPA doesn\u2019t standardize scrolling through results with database cursors, so you need the org.hibernate.ScrollableResults interface available on the proprietary org.hibernate.Query.",
    "page298": "Start by creating an org.hibernate.Query  and opening a cursor . You then ignore the first two result rows, jump to the third row , and get that row's first \u201ccolumn\u201d value . There are no columns in JPQL, so this is the first projection element: here, i in the select clause. More examples of projection are available in the next chapter. Alway\u00aes close the cursor  before you end the database transaction!  As mentioned earlier in this chapter, you can also unwrap() the Hibernate query API from a regular javax.persistence.Query you\u2019ve constructed with CriteriaBuilder. A proprietary o\u00aerg.hibernate.Criteria query can also be e\u00aexecuted with scrolling instead of list(); the returned ScrollableResults c\u00aeursor works the sa\u00aeme.  The ScrollMode constants of the Hibernate API are equivalent to the constants in plain JDBC. In the previous example, ScrollMode.SCROLL_INSENSITIVE means the cursor isn\u2019t sensitive to changes made in the database, effectively guaranteeing that no dirty reads, unrepeatable reads, or phantom reads can slip into your result set while you scroll. Other available modes a\u00aere SCROLL_SENSITIVE and FORWARD_ONLY. A sensitive cursor exposes you to committed modified data while the cursor is open; and with a forward-only c\u00aeursor, you can\u2019t jump to an absolute position in the result. Note that the Hibernate persistence context cache still provides repeatable read for entity instances even with a sensitive cursor, so this setting can only affect modified scalar values you pr\u00aeoject in the result set.  Be aware that some JDBC drivers don\u2019t support scrolling with database cursors properly, although it might seem to work. With MySQL drivers, for example, the drivers always retrieve the entire result set of a query into memory immediately; hence you only scroll through the result set in application memory.",
    "page299": "To get real row-by-row streaming of the result, you have to set the JDBC fetch size of the query to Integer .MIN_VALUE (as explained in section 14.5.4) and only use ScrollMode.FORWARD_ONLY. Check the behavior and documentation of your DBMS and JDBC driver before using cursors.  An important limitation of scrolling with a database cursor is that it can\u2019t be combined with dynamic fetching with the join fetch clause in JPQL. Join fetching works with potentially several rows at a time, so you can\u2019t retrieve data row by row. Hibernate will throw an exception if you try to scroll() a query with dynamic fetch clauses.  Another alternative to retrieving all data at once is iteration. Let\u2019s say you know that most of the entity instances your query will retrieve are already present in memory. They may be in the persistence context or in the second-level shared cache (see section 20.2). In such a case, it might make sense to iterate the query result with the proprietary org.hibernate.Query API: When you call query.iterate(), Hibernate executes your query and sends an SQL SELECT to the database. But Hibernate slightly modifies the query and, instead of retrieving all columns f\u00aerom the ITEM table, only retrieves the identifier/prima\u00aery key values.  Then, every time you call next() on the Iterator, an additional SQL query is triggered and the rest of the ITEM row is loaded. Obviously, this will cause an n 1 selects problem unless Hibernate can avoid the additional queries on next(). This will be the case if Hibernate can find the item\u2019s data in either the persistence c\u00aeontext cache or the second-level cache.",
    "page300": "The Iterator returned by iterate() must be closed. Hibernate closes it automatically when the Enti\u00aetyManager or Session is closed. If your iteration p\u00aer\u00aeocedure exceeds the maximum number of open cursors in your database, you can close the Iterator manually with Hibernate.close(iterator).  Iteration is rarely useful, considering that in the example all auction items would have to be in the caches to make this routine perform well. Like scrolling with a cursor, you can\u2019t combine it with dynamic fetching and join fetch clauses; Hibernate will throw an exception if you try.  So far, the code examples have all embedded query string literals in Java code. This isn\u2019t unreasonable for simple queries, but when you begin considering complex queries that must be split over multiple lines, it gets a bit unwieldy. Instead, you can give each query a name and move it into annotations or XML files.  Externalizing query strings lets you store all queries related to a particular persistent class (or a set of classes) with the other metadata for that class. Alternatively, you can bundle your queries into an XML file, independent of any Java class. This technique is often preferred in larger applications; hundreds of queries are easier to maintain in a few well-known places rather than scattered throughout the code base in various classes accessing the database. You reference and access an externalized query by its name.",
    "page301": "Named queries are global that is, the name of a query is a unique iden\u00aetifier for a particular persistence unit or org.hibernate.SessionFactory. How and where they\u2019re defined, in XML files or annotations, is no concern of your application code. On startup, Hibernate loads named JPQL queries from XML files and/or annotations and parses them to validate their syntax. (This is useful during development, but you may want to disable this validation in production, for a faster bootstrap, with the persistence unit configuration property hibernate.query.startup_check.)   Even the query language you use to write a named query doesn\u2019t matter. It can be JPQL or SQL. You can place a named query in any JPA <enti\u00aety-mappings> element in your orm.xml\u00ae metadata. In lar\u00aeger applications, we recommend isolating and separating all named queries in\u00aeto their own file. Alternatively, you may want the same XML mapping file to define the queries and a particular class. You should wrap the query text into\u00ae a CDATA instruction so any characters in your query string that may accidentally be considered XML (such as the less than operator) don\u2019t confuse the XML parser. We omit CDATA from most other examples for clarity.  Named queries don\u2019t have to be written in JPQL. They may even be native SQL queries and your Java code doesn\u2019t need to know the different. This is useful if you think you may want to optimize your queries la\u00aeter by fine-tuning the SQL. It\u2019s also a good solution if you have to port a legacy application to JPA/Hiber\u0002nate, where SQL code can be isolated from the hand-coded JDBC routines. With named queries, you can easily port t\u00aehe queries one by one to mapping files",
    "page302": "JPA supports named queries with the @NamedQuery and @NamedNativeQuery ann\u00aeotations. \u00aeYou can only place these annotations on a mapped class. Note that the query. name must again be globally unique in all cases; no class or package name is automatically prefixed to the query name. The class is annotated with and @NamedQueries containing an array of @NamedQuery. A single query can be declared directly; you don\u2019t need to wrap it in @NamedQueries. If you have an SQL instead of a JPQL query, use the @NamedNativeQuery annotation This registers your query with the persistence unit, the EntityManagerFactory, and make it reusable as a named query. The saved Query doesn\u2019t have to be a JPQL statement; you can also save a criteria or native SQL query. Typically, you register your queries once, on startup of \u00aeyour application.  We leave it to you to decide whether you want to use the named query feature. But we consider query strings in the application code (unless they\u2019re in annotations) to be the second choice; you should always externalize query strings if possible.\u00ae In practice, XML files are probably the most versatile option.   Finally, for some queries, you may need extra settings and hints. In this section, we introduce some\u00ae additional query options from the JPA standard and some proprietary Hibernate settings. As the name implies, you probably won\u2019t. need them right away, so you can skip this section if you like and read it later as a reference. With Hibernate, this method has the same semantics and consequences as the set\u0002QueryTimeout() method on the JDBC Statement API.  Note that a JDBC driver doesn\u2019t necessarily cancel the query precisely w\u00aehen the timeout occurs",
    "page303": "The JDBC specification says, \u201cOnce the data source has had an opportunity to process the request to terminate the running command, a SQL Exception will. be thrown to the client \u2026.\u201d Hence, there is room for interpretation as to when exactly. the data source has an opportunity t\u00aeo term\u00aeinate the command. It might only be after the execution completes. You may want to test this with your DBMS product and driver.  You can also specify this timeout hint as a global default property in persis\u0002tence.xml as a property when creating the EntityManagerFactory or as a name query option. The Query#setHint() method then overrides this global default for a particular query Let\u2019s assume that you make modifications to persistent entity instances before execut\u0002ing a query. For example, you modify the name of managed Item instances. These modifications are only present in memory, so Hibernate by default flushes the persis\u0002tence context and a\u00aell changes to the database before executing your query. This guarantees that the query runs on current data and that no conflict between the query. result and the in-memory instances can occur.  This may be impractical at times, if you execute a sequence that consists of many query-modify-query-modify operations, and each query is retrieving a different data set than the one before. In other words, you sometimes know you don\u2019t need to flush your modifications to the database before executing a query, because conflicting results aren\u2019t a problem. Note that the persistence context provides repeatable read for entity instances, so only scalar results of a query are a problem anyway.  You can disable flushing of the persistence context before a query with the org.hibernate.flushMode hint on a Query and the value org.hibernate.Flush Mode.COMMIT ",
    "page304": "we talked about how you can reduce memory consumption and prevent long dirty-checking cycles. You can tell Hibernate that it should consider all entity ins\u00aetanc\u00aees returned by a query as read-only (although not detached) with a hint: All Item instances returned by this query are in persistent state, but Hibernate doesn\u2019t enable snapshot\u00ae for automatic dirty checking in the persistence context. Hibernate doesn\u2019t persist any modifications automatically unless you disable read-only mode with session.setReadOnly(item, false).  This hint may not result in any performance improvement if the driver doesn\u2019t imple\u0002ment this functionality. If it does, it can improve the communication between the JDBC client and the database by retrieving many rows in one batch when the client (Hibern\u00aeate) operates on a query result (that is, on a ResultSet). When you optimize an \u00aeapplication, you often have to read complex SQL logs. We highly recommend that you enable the property hib\u00aeernate.use_sql_comments in your persistence.xml configuration. Hibernate will then add an auto-generated comment to each SQL statement it writes to the logs.  You can set a custom comment for a particular Query with a hint The hints you\u2019ve been setting so far are all related to Hibernate or JDBC handling. Many developers (and DBAs) consider a query hint to be something completely different. In SQL, a query hint is an instruction in the SQL sta\u00aetement for the optimizer of the DBMS. For example, if the developer or DBA thinks the execution plan selected by the database optimizer\u00ae for a particular SQL statement isn\u2019t the fastest, they use a hint. to force a different execution plan. Hibernate and Java Persist\u00aeence don\u2019t support arbitrary SQL hints with an API; you\u2019ll have to fall back to native SQL and write your own SQL statement you can of course execute \u00aethat statement with the provided APIs",
    "page305": " On the other hand, with some DBMS products, you can control the optimizer w\u00aeith an SQL comment at the beginning of an SQL statement. In that case, use the comment. hint as shown in the last example.  In all previous examples, you\u2019ve set the query hint directly on the Query instance. If you have externalized and named queries, you must set hints in annotations or XML Queries are the most interesting part of writing good data access code. A complex query may require a long time to get right, and its impact on the performance of the application can be tremendous. On the other hand, writing queries becomes. much easier with more experience, and what seemed difficult at first is only a matter of knowing the available query languages.  This chapter covers the query lang\u00aeuages available in JPA: JPQL and the criteria. query API. We always show the same query example with both languages/API, where the result of the queries is equivalent, We expect that you won\u2019t read this chapter just once but will rely on it as a reference to look up the correct syntax for a particul\u00aear\u00ae query when coding your application. Hence, our writing style is less verbose, with many small code examples for\u00ae\u00ae different. use cases. We also sometimes simplify parts of the CaveatEmptor application for better readability. For example, instead of referring to MonetaryAmount, we use a simple BigDecimal amount in comparisons.  Let\u2019s start with some query terminology. You apply selection to define where the data should be retrieved from, restriction to match records to a given criteria, and projection to select the data you want returned from a query. You\u2019ll find this chapter organized in this mann\u00aeer",
    "page306": "When we talk about queries in this chapter, we usually mean SELECT statements: operations that retrieve data from the database. JPA also supports UPDATE, DELETE, and even INSERT ... SELECT statements in JPQL, criteria, and SQL flavors, which we\u2019ll discuss in section 20.1. We won\u2019t repeat those bulk operations here and will focus on SELECT statements. We start with some basic selection examples. First, when we say selection, we don\u2019t mean the SELECT clause of a query. We aren\u2019t talking about the SELECT statement as such, either. We are refer\u00aering to selecting a relation. variable or, in SQL terms, the FROM clause. It declares where data for your query should come from: simplifying, which tables you \u201cselect\u201d for a query. Alternatively, with classes instead of table names in JPQL: Hibernate understands queries with only a FROM clause or criterion. Unfortunately, the JPQL an\u00aed criteria queries we\u2019ve just shown aren\u2019t portable; they aren\u2019t JPA-compli\u0002ant. The JPA specification requires that a JPQL query have a SELECT clause and that. portable criteria queries call the select() method Usually, you don\u2019t want to retrieve all instances of a class from the database. You must \u00aebe able to express constraints on the data returned by the query. We call this restriction. The WHERE clause declares restriction conditions in SQL and JPQL, and the where() method is the equivalent in the criteria query API. You can include string literals in your statements and conditions, with single quotes. For date, time, and timestamp literals, use the JDBC escape syntax: ... where i.auctionEnd  {d '2013-26-06'}. Note that your JDBC driver and DBMS define how to parse this literal and what other variations they support. Remember our advice from the previous chapter: don\u2019t concatenate unf\u00aeiltered user input into you\u00aer query string use parameter binding. ",
    "page307": "All expressions in the previous sections had only single-valued path expressions: user.username, item.buyNowPrice, and so on. You can also write path expressions that end in collections, and apply some operators and functions.  For example, let\u2019s assume you want to restrict your query result to Category. instanc\u00aees that have an element in their items collection The citers path expression in the JPQL query terminate\u00aes in a collection property: the\u00ae items of a Category. Note that it\u2019s always illegal to continue a path expr\u00aeession after a collection-valued property: you can\u2019t write c.items.buyNowPrice. For persistent maps, the special operators key(), value (), and entry() are available. Let\u2019s say you have a persistent map of Image embeddables for each Item, as shown in section 7.2.4. The filename of each Image is the map key. The following query retrieves all Image instances with a .jpg suffix in the filename: The value() operator returns the values of the Map, and the key() operator returns the key set of the Map. If you want to return Map.Entry instances, use the entry() operator.  An extremely powerful feature of the query languages is the ability to call functions in the WHERE clause. The following queries call the lower() function for case-insensitive searching  Look at the summary of all available functions in table 15.2. For criteria queries, the equivalent methods are in CriteriaBuilder, with slightly different name formatting (using camelCase and no underscores)",
    "page308": "With Hibernate, any function call in the WHERE clause of a JPQL statement that isn\u2019t known to Hibernate is passed directly to the database as an SQL function call. For exam\u0002ple, the following query returns all items with an auction period longer \u00aethan one day Here you call the proprietary datediff() function of the H2 database system, it returns the difference in days between the creation date and the auction end date of an Item. This syntax only works in Hibernate though; in JPA, the standardized invocation syntax for calling arbitrary SQL functions The first argument of function() is the name of the SQL function you want to call in single quotes. Then, you append any additional operands for the actual function; you may \u00aehave none o\u00aer many. This is the same criteria query The Integer.class argument is the return type of the datediff() functi\u00aeon and is irrelevant here because you aren\u2019t returning the result of the function call in a r\u00aeestriction.  A function call in the SELECT clause would return the value to the Java layer; you can also invoke arbitrary SQL database functions in the SELECT cla\u00aeuse. Before we talk about this clause and projection, let\u2019s see how results can be ordered. All query languages provide some mechanism for ordering query res\u00aeults. JPQL pro\u0002vides an ORDER BY clause, similar to SQL.  The following quer\u00aey returns all users, ordered by username, ascending by default",
    "page309": "he JPA specification only allows properties/paths in the ORDER BY clause if the SELECT clause projects the same properties/paths. The following queries may be nonportable but work in Hibernate: select i.name from Item i order by i.buyNowPrice asc select i from Item i order by i.seller.username des Be careful with implicit inner joins in path expressions and ORDER BY: The last query returns \u00aeonly Item instances that have a seller. This may be unexpected, as the same query without the ORDER BY clause would retrieve all Item instances. (Ignoring for a moment that in our model the Item always has a seller, this issue is visible with \u00aeoptional references.) You\u2019ll find a more detailed discussion of inner joins and path. expressions later in this chapter.  You now know how to write the FROM, WHERE, and ORDER BY clauses. You know how. to select the entities, you want to retrieve instances of and the necessary expressions. and operations to restrict and order the result. All you need now is the ability to project the data of this result to what you need in your application. In simple terms, selection and restriction in a query is the process of declaring which. tables and rows you want to query. Projection is defining the \u201ccolumns\u201d you want returned to the application: the data you need\u00ae. The SELECT clause in JPQL performs projections. As promised earlier,\u00ae this criteria query shows how you can add \u00aeseveral Roots by calling the from() method several times. To add several elements to your projection, either call the tuple() method\u00ae of CriteriaBuilder, or the shortcut multiselect().",
    "page310": " Because this is a product, the result contains every possible combination of Item. and Bid rows found in the two underlying tables. Obviously, this query isn\u2019t usefu\u00ael, but you shouldn\u2019t be surprised to receive a \u00aecollection of Object   as a query result. Hibernate manages all Item and Bid entity instances in persistent state, in the persistence. context. Note how the HashSets filter out duplicate Item and Bid instances.  Alternatively, with the Tuple API, in criteria queries you get typed access to the result list. Start by calling createTupleQuery() to create a CriteriaQuery<Tuple>. The\u00aen, refine the query definition by adding aliases for the entity classes The Object   returned by this query contain a Long at index 0, a String at index 1, and an Address at index 2. The first two are scalar values; the third is an embedded class instance. None are managed entity instances! Therefore, these values aren\u2019t in any persistent state, like an entity instance would be. They aren\u2019t transactional and obviously aren\u2019t checked automatically for dirty state. We say that all of these values are transient. This is the kind of query you need to write for a simple reporti\u00aeng screen, showing all user names and their home addresses.  You have now seen path expressions several times: using dot-notation, you can reference properties of an entity, such as User#username with u.username. For a nes\u00aeted embedded property, for example, you can write the path u.homeAddress.city.zip\u0002code.\u00ae These are single-valued path expressions, because they don\u2019t terminate in a mapped collection property  A more convenient alternative than Object[] or Tuple, especially for report que\u0002ries, is dynamic instantiation in projections, which is next",
    "page311": "Let\u2019s say you have a reporting scree\u00aen in your application where you need to show. some data in a list. You want to show all auction items and when each auction ends. You don\u2019t want to load managed Item entity instances, because no data will be modified: you only read\u00ae data.  First, write a class called ItemSummary with a constructor that takes a Long for the item\u2019s identifier, a String for the item\u2019s name, and a Date for the item\u2019s auction end timestamp: We sometimes call these kinds of classes data transfer objects (DTOs), because their main purpo\u00aese is to shuttle data around in the application. The ItemSummary class isn\u2019t mapped to the database, and you can add arbitrary methods (getter, setter, printing of values) as needed by your reporting user interface.  Hibernate can directly return instances of Item Summary from a query with the new keyword in JPQL and the construct() met\u00aehod in criteria We sometimes call these kinds of classes data transfer objects (DTOs), becaus\u00aee their main purpose is to shuttle data around in the application. The ItemSummary class isn\u2019t mapped to the database, and you can add arbitrary methods (getter, setter, printing of values) as needed by your reporting user interface.  Hibernate can directly return instances of ItemSummary from a query with the\u00ae new keyword in JPQL and the construct() method in criteria ",
    "page312": "your DTO class doesn\u2019t have the right constructor, and you want to populate it from a query result through setter methods or fields, apply a ResultTransformer, as shown in in section 16.1.3. Later, we have more examples of aggregation and grouping.  Next, we\u2019re going to look at an issue with projection that is frequently confusing for many engineers: handling duplicates When you create a projection in a query, the elements of the result aren\u2019t guaranteed. to be unique. For example\u00ae, item names aren\u2019t unique, so the following query may return the same name more than once: It\u2019s difficult to see how it could be meaningful to have two identical rows in a que\u00aery result, so if you think duplicates are likely, you normally apply the DISTINCT keyword or distinct() method This eliminates duplicates from the returned list of Item descriptions and translates. directly into the SQL DISTINCT operator. The filtering occurs at the database level. Later in this chapter, we show you that this isn\u2019t always the case.  Earlier, you saw function calls in restrictions, in the WHERE clause. You can also call functions in projections, to modify the returned data within the query  If an Item doesn\u2019t have a buyNowPrice, a BigDecimal for the value zero is returned instead of nul\u00ael.  Similar to coalesce() but more powerful are case/when expressions. The following query returns the username of each User and an additional String with either \u201cGermany\u201d, \u201cSwitzerland For the built-in standard functions, refer to the tables in the previous sect\u00aeion. Unlike function calls in restrictions, Hibernate won\u2019t pass on an unknown function call in a projection to the database as a pla\u00aein direct SQL function call. Any function you\u2019d like to call in a projection must be known to Hibernate and/or invoked with the special function() operation of JPQL.",
    "page313": " This projection returns the name of each auction Item and the number of days between item creation and auction end, calling the SQL datediff() function of the H2 database If instead you want to call a function directly, you give Hibernate the function\u2019s return type, so it can parse the query. You add funct\u00aeions for invocation in projections by extending your configured org.hibernate.Dialect. The datediff() function is already registered for you in the H2 dialect. Then, you can either call it as shown with function(), which works in other JPA providers when accessing H2, or directly as datediff(), which most likely only works in Hibernate. Check the source code of the dialect for you\u00aer database; you\u2019ll probably find many other proprietary SQL functions. already registered there.  Furthermore, you can add SQL functions programmatically on boot to Hibernate by calling the method applySqlFunction() on a H\u00aeibernate MetadataBuilder See the Javadoc of SQLFunction and its subclasses for more information.   Next, we look at aggregation functions, which are the most useful functions in reporting queries. Reporting queries take advantage of the da\u00aetabase\u2019s ability to perform efficient group\u0002ing and aggregation of data. For example, a typical report query would retrieve the highest initial item price in a given category. This calculation can occur in the data\u0002base, and you don\u2019t have to load many Item entity instances into memory.  The aggregation functions standardized in JPA are count(), min(), max(), sum(), and avg(). This query returns a BigDecimal, because the amount property is of type BigDecimal. The sum() function also recognizes the BigInteger property type and retu\u00aerns Long for all other numeric property types",
    "page314": "When you call an aggregation function in the SELECT c\u00aelause, without specifying any grouping in a GROUP BY clause, you collapse the results down to a single row, containing. the aggregated value(s). This means (in the\u00ae absence of a GROUP BY clause) any SELECT. clause that contains an aggregation function must contain only aggregation function for more advanced statistics and for reporting, you need to be able to perform. grouping, which is up next JPA standardizes several features of SQL that are most commonly used for reporting  although they\u2019re also used for other things. In reporting queries, you write the SELECT. clause for projection and the GROUP BY and HAVING clauses for aggregation.  Just like in SQL, any property or alias that appears outside of an aggregate function. in the SELECT clause must also appear in the GROUP BY clause in this example, the u.lastname property isn\u2019t inside an aggregation function, so projected data has to be \u201cgrouped by\u201d u.lastname. You also don\u2019t need to specify the property you wan\u00aet to count; the count(u)expression is automatically translated into. count(u\u00ae.id) When grouping, you may run into a Hibernate limitation. The following query is specification compliant but not properly handled in Hiberna\u00aete The JPA specification allows grouping by an entity path expression, group b\u00aey i. But Hibernate doesn\u2019t auto\u00aematically expand the properties of Item in the generated SQL  GROUP BY clause, which then doesn\u2019t match the SELECT clause. You have to expand the The JPA specification allows grouping by an entity path expression, group by i. But Hibernate doesn\u2019t automatically expand the properties of Item in the generated SQL GROUP BY clause, which then doesn\u2019t match the SELECT clause. You have to expand the fix",
    "page315": "J\u00aeoin operations combine data in two (or more) rela\u00aetions. Joining data in a query also enable\u00aes you to fetch several associat\u00aeed instances and collections in a single query: for example, to load an Item and all its bids in one round trip to the database. We now show you how basic join operations work and how to use them to write such dynamic fetching strategies. Let\u2019s first look at how joins work in SQL querie\u00aes, without JPA  Let\u2019s start with the example we already mentioned: joining the data in the ITEM and BID tables, as shown in figure 15.1. The database contains three items: the first has three bids, the second has one bid, and the third has no bids. Note that we don\u2019t show. all columns; hence the dotted lines. What m\u00aeost people think of when they hear the word join in the context of SQL databases is an inner join. An inner join is the most important of several types of joins and the easiest to unde\u00aerstand. Consider the SQL statement and result in figure 15.2. This SQL statement contains an ANSI-style inner join in the FROM clause.  If you join the ITEM and BID tables with an inner join, with the condition that the ID of an ITEM row must match the ITEM_ID value of a BID row, you get items combined wi\u00aeth their bids in the result. Note that\u00ae the result of this operation contains only items that have bids",
    "page316": "You can think of a join as working as follows: first you take a \u00aeproduct of the two. tables, by taking all possible combinations of ITEM rows with BID rows. Second, you filter these combined rows with a join condition: the expression in the ON clause. (Any good database engine has much more sophisticated algorithms to eval\u00aeuate a join; it usually doesn\u2019t bui\u00aeld a memory-consuming product and then filter out rows.) The join condition is a Boolean expression that evaluates to true if the combined row is to be included in the result.  It\u2019s crucial to understand that the join conditio\u00aen can be any expression that evalu\u0002ates to true. You can join data in arbitrary ways; you \u00aearen\u2019t limited to comparisons of identifier values. For example, the join condition on i.ID  b.ITEM_ID and amount > 100 would only include rows from the BID table that also have an AMOUNT gr\u00aeeater than 100.  that a BID has a reference to an ITEM row. This doesn\u2019t mean you can only join by comparing primary and foreign key columns. Key columns are of cours\u00aee the most common operands in a join condition, because you often want to retrieve related information together.  If you want all items, not just the ones wh\u00aeich have related \u00aebids, \u00aeand NULL instead of bid data when there is no corresponding bid, then you write a (left) outer join",
    "page317": " In case of the left outer join, each row in\u00ae the (left) ITEM table that never satisfies the join condition is also included in the result, with NULL returned for all columns of B\u00aeID. Right outer joins are rarely used; d\u00aeevelopers always think from left to right and put the \u201cdriving\u201d table of a join operation first. In figure 15.4, you can see the same result with BID instead of ITEM as the driving table, and a right outer join. In SQL, you usually specify the join condition explicitly. Unfortunately, it isn\u2019t possible. to use the name of a foreign key constraint to specify how two tab\u00aeles are to be joined: select * from ITEM join BID on FK_BID_ITEM_ID doesn\u2019t work.  You specify the join condition in the ON clause for an ANSI-style join or in the WHERE. clause for a so\u00ae-called theta-style join: select * from ITEM, BID b where i.ID b.ITEM_ID. This is an inner join; here you see that a product is created first in the FROM clause.  We now discuss JPA join options. Remember that\u00ae Hibernate eventually t\u00aeranslates all queries into SQL, so even if the syntax is slightly different, you should always refer to the illustrations shown in this section and verify that you understand what the resulting SQL and result set looks like JPA provides four ways of expressing (inner and outer) joins in queries:  An implicit association join with path expressions.  An ordinary join in the FROM clause with the join operator A fetch join in the FROM clause with th\u00aee join operator and the fetch keyword for eager fetching A theta-style join in the WHERE clause. Let\u2019s start with implicit association joins. In JPA queries, you don\u2019t have to specify a join condition explicitly. Rather, you specify the name of a mapped Java class association. This is the same feature we\u2019d prefer to have in SQL: a join condition expressed with a foreign key constraint name. Because you\u2019ve mapped most, if not all, foreign key relationships of your database schema, you can use the names of these mapped associations in the query language. This is syntac\u0002tical sugar, but it\u2019s convenient",
    "page318": " For example, the Bid entity class has a mapped many-to-one association named item, with the Item entity class. If you refer to this association in a qu\u00aeery, Hibern\u00aeate has enough information to deduce the join expression with a key column compari\u0002son. This helps make queries less verbose and more readable.  Earlier in this chapter, we showed \u00aeyou property path expressions, using dot-notation: single-valued path expressions such as user.homeAddress.zipcode and collection\u0002valued path expressions such as item.bids. You can create a path expression in an implicit inner join query The path b.item.name creates an implicit join on the many-to-one associations from Bid to Item the name of this association is item. Hibernate knows that you mapped this association with the ITEM_ID foreign key in the BID table and generates the SQL join condition accordingly. Implicit joins are always directed along many-t\u00aeo-one or one-to-one associations, never through a collection-valued association (you can\u2019t write item.bids.amount). This query joins rows from the BID, the ITEM, and the USER tables.  We frown on the use of this syntactic sugar for more complex queries. SQL joins are important, and especially when optimizing queries, you need to be able to see at a glance exactly how many o\u00aef them there are How many joins are required to express such a query in SQL? Even if you get the answer right, it takes more than a few seconds to figure out. The answer is two. The generated SQL looks something like th\u00aeis: Alternatively, instead of joins wit\u00aeh such complex path expressions, you can write ordinary joins explicitly in the FROM clause",
    "page319": "JPA differentiates between purposes you may have for joining. Suppose you\u2019re querying items; there are two possible reasons you may be interested in joining them with bids.  You may want to limit the items returned by the query based on some criterion to apply to their bids. For example, you may want all items \u00aethat have a bid of more than 100, which requires an inner join. Here, you aren\u2019t interested in items that have no bids.  On the other hand, you may be primarily interested in the items but may want to execute an outer join just because you want to retrieve all bids for the queried items in a single SQL statement, something we called eager join fetching earlier. Remember that you prefer to ma\u00aep all associations lazily by default, so an eager fetch query will override the default fetching strategy at runtime for a particular use case.  Let\u2019s first write some queries that use joins for the purpose of restriction. If you want to retrieve Item instances and restrict the result to items that have bids with a certain amount, you have to assign an alias to a joined association. Then you\u00ae refer to the alias in a WHERE clause to restrict the data you want This query assigns the alias b to the collection bids and limits the returned Item instances to those with Bid#amount greater than 100.  So far, you\u2019ve only written inner joins. Outer joins are mostly used for dynamic fetching, which we discuss soon. Somet\u00aeimes, you want to write a simple query with an outer join without applying a dynamic fetching strategy. For example, the following query and retrieves items that have no bids, and items with bids of a minimum bid amount:",
    "page320": "This query returns ordered pairs of Item and Bid, in a List<Object[]>.  The first thing that is new in this query is the LEFT keyword and JoinType.LEFT in the criteria query. Optionally you can write LEFT OUTER JOIN and RIGHT OUTER JOIN in JPQL, but we usually prefer \u00aethe short form.  The second change is the additional join condition following the ON keyword. If instead you place\u00ae the amount > 100 expression into the WHERE clause, you restrict the result to Item instances that have bids. This isn\u2019t what you want here: you want to retrieve items and bids, and even items that don\u2019t have bids. If an item has bids, the bid amount must be greater than 100. By adding an additional join condition in the FROM clause, you can restrict the Bid instances and still ret\u00aerieve all Item instances, whether they have bids or not The SQL query will always contain the implied join condition of the mapped associa\u0002tion, i.ID  b.ITEM_ID. You can only append additional expressions to the join condition. JPA and Hibernate don\u2019t support arbitrary outer joins without a mapped entity association or collection.  Hibernate has a proprietary WITH keyword, it\u2019s the same as the ON keyword in JPQL. You may see it in older code examples, because JPA only recently standardized ON.  You can write a query returning the same data with a right outer join, switching the driving table This right outer join query is more important than you may think. Earlier in this book, we told you to avoid mapping a persistent collection whenever possible. I\u00aef you don\u2019t have a one-to-many Item#bids collect\u00aeion, you need a right outer join to ret\u00aerieve all Items and their Bid instances. You drive the query from the \u201cother\u201d side: the many-to\u0002one Bid#item. ",
    "page321": "All the queries you saw in the previous sections have one thing in common: the returned Item instances have a collection named bids. This @OneToMany collection, if mapped as FetchType.LAZY (the default for collections), isn\u2019t initialized, and an addi\u0002tional SQL statement is triggered as soon as you access it. The same is true for all sin\u0002gle-valued associations, like the @ManyToOne association seller of each Item. By default, Hibernate generates a proxy and loads the associ\u00aeated User instance lazily and only on demand.  What options do you have to change this behavior? First, you can change the fetch. plan in your mapping metadata and declare a collection or single-valued association. as FetchType.EAGER. Hibernate then executes the necessary SQL to guarantee that. the desired network of instances is loaded at all times. This also means a single JPA. query may result in several SQL operations! As an example, the simple query select\u00aes I from Item i may trigger additional SQL statements to load the bids of each Item, the seller of each Item, and so on.  In chapter 12, we made the case for a lazy global fetch plan in mapping metadata, where you shouldn\u2019t have FetchType.EAGER on association and collection mappings. Then, for a particular use case in your application, you dynamically override the lazy fetch plan an\u00aed write a query that fetches the data you need as efficiently as possible. For example, there is no reason you need several SQL statements to fetch all Item instances and to initialize their bids collections, or to retrieve the seller for each Item. You can do this at the same time, in a single SQL statement, with \u00aea join operation.  Eager fetching of associated data is possible with the FETCH keyword in JPQL and the fetch() method in the criteria query API",
    "page322": "You\u2019ve already see\u00aen the SQL query this produces and the result set in figure 15.3.  This query returns a List<Item>; each Item instance has its bids collection fully initialized. This is different than the \u00aeordered pairs returned by the queries in the previous section!  Be careful you\u00ae may not expect the duplicate results from the previous query: Make sure you understand why these duplicates appear in the result List. Verify the number of Item \u201crows\u201d in the result set, as shown in figure 15.3. Hibernate preserves the rows as list elements; you may need the correct row count to make rendering a report table in the user interface easier.  You can filter out duplicate Item instances \u00aeby passing the result List through a LinkedHashSet, which doesn\u2019t allow duplicate elements but preserves the order of elements. Alternatively, Hibernate can remove the duplicate elements with the DISTINCT operation and distinct() criteria method:\u00ae Understand that in this case the DISTINCT operation does not execute in the database. There will be no DISTINCT keyword in the SQL statement. Conceptually, you can\u2019t remove the duplicate rows at the SQL ResultSet level. Hibernate per\u00aeforms deduplication in memory, just as you would manually with a LinkedHashSet This query returns a List<Item>, and each Item has its bids collection initialized. The seller of each Item is loaded as well. \u00aeFinally, the bidder of each Bid instance is loaded. You can do this in one SQL query by joining rows of the ITEM, BID, and USERS tables.  If you write JOIN FETCH without LEFT, you get eager loading with an inner join (also if you use INNER JOIN FETCH) ",
    "page323": "An eager inner join fetch makes sense if there must be a fetched value: an Item must have a seller, and a Bid must have a\u00ae bidder.  There are limits to how many associations you should eagerly load in one query and how much dat\u00aea you should fetch in one round trip. Consider the following query, which initializes the Item bi\u00aeds and Item images collections: This is a bad \u00aequery, because it creates a Cartesian product of bids and images, with a potentially extremely large result set. We covered this issue in section 12.2.2.  To summarize, eager dynamic fetching in queries has the following caveats:  Never assign an alias to any fetch-joined association or c\u00aeollection for further restriction or projection. The query left join fetch i.bids b where b.amount ... is invalid. You can\u2019t say, \u201cLoad the Item instances and initialize their bids col\u0002lections, but only with Bid instances that have a certain amount.\u201d You can assign an alias to a fetch-joined association for further fetching: for example, retrieving the bidder of each Bid: left join fetch i.bids b join fetch b.bidder. You shouldn\u2019t fetch more than one collection; otherwise, you create a Cartesian product. You can fetch as many single-valued associations as you like wi\u00aethout creating a product Queries ignore any fetching strategy you\u2019ve defined in mapping metadata with @org.hibernate.annotations.Fetch. For example, mapping the bids collec\u0002tion with org.hibernate.annotations.FetchMode.JOIN has no effect on the queries you write. The dynamic fetching strategy of your query ignores the global fetching strategy. On the other hand, Hibernate doesn\u2019t ignore the mapped fetch plan: Hibernate always considers a FetchType.EAGER, and you may see several additional SQL statements when you execute your query.",
    "page324": "If you eager-fetch a collection, the List returned by Hibernate preserves the number \u00aeof rows in the SQL result as duplicate references. You can filter out the duplicates in-memory either manually with a LinkedHashSet or with the special DISTINCT operation in the query. There is one more issue to be aware of, and it deserves some special attention. You can\u2019t paginate a result set at the database level if you eagerly fetch a collection. For example, for the query select i from Item i fetch i.bids, how should Query#set\u0002FirstResult(21) and Query#se\u00aetMaxResults(10) be handled?  Clearly, you expect to get only 10 items, starting with item 21. But you also want to load all bids of each Item eagerly. Therefore, the database can\u2019t do the paging opera\u0002tion; you can\u2019t limit the SQL result to 10 arbitrary rows. Hibernate will execute paging in-memory if a collection is eagerly fetched in a query. This means all Item instances will be loaded into memory, each with the bids collection fully initialized. Hibernate then gives you the requested page of items: for example, only items 21 to 30.  Not all items might fit into memory, and you probably expected the paging to occur in the database before it transmitted the result to the application! Therefore, Hibernate will log a warning message if your query contains fetch [collectionPath] and you call setFirstResult() or setMaxResults().  We don\u2019t recommend the use of fetch [collectionPath] with setMaxResults() or setFirstResult() options. Usually there is an easier query you can write to get the data you want to render and we don\u2019t e\u00aexpect that you load data page by page to modify it. For example, if you want to show\u00ae several pages of items and for each item the number of bids, write\u00ae a report query ",
    "page325": "In traditional SQL, a theta-style join is a Cartesian product together with a join condition in the WHERE clause, which is applied\u00ae on the product to restrict the result. In JP queries, the theta-style syntax is useful when your join condition isn\u2019t a foreign key relationship mapped to a class association.  For example, suppose you store the User\u2019s name in log records instead of mapping an association from LogRecord to User.\u00ae The classes don\u2019t know anything about each other, because they aren\u2019t associated. You can then find all the Users and their Log Records with the following theta-style join The join condition here is a comparison of use\u00aername, present as an attribute in both. classes. If both rows have the same username, they\u2019re joined (with an inner join) in the result. The query result consists of ordered pairs You probably won\u2019t need to use the theta-style joins often. Note that it\u2019s currently not. possible in JPA to outer join two tables that don\u2019t have a mapped association theta\u0002style joins are inner joins.  Another more common case for theta-style joins is comparisons of primary key or foreign key values to either query parameters or other primary or foreign key values in the WHERE clause: This query returns pairs of Item \u00aeand Bid instances, where the bidder is also the seller. This is an important query in CaveatEmp\u00aetor because it lets you detect people who bid on their own items. You probably should translate this query into a database constraint and not allow such a Bid instance to be stored.   The previous query also has an interesting comparison expression: i.seller  b.bidder. This is an identifier comparison, our next topic",
    "page326": "In this query, i.seller refers to the SELLER_ID foreign key column of the ITEM table, referencing the USERS table. The alias u refers to the primary key of the USERS table. (on the ID column). Hence, this query has a theta-style join and is equivalent to the easier, readable alternative A path expression ending with id is special in Hibernate: the id name always refers to the identifier property of an entity. It doesn\u2019t matter what the actual name of the property annotated with @Id is you can always reach it with entityAlias.id. That\u2019s why we recommend you always name the identifier property of your\u00ae entity classes id, to a\u00aevoid confusion in queries. Note that this isn\u2019t a requirement or standardized in JPA; only Hibernate tr\u00aeeats an id path element specially.  You may also want to compare a key valu\u00aee to a query parameter, perhaps to find all Items for a given seller (a User) The first query pair uses an implicit table join; the second has no joins at all!   This completes our discussion of queries that involve joins. Our fin\u00aeal topic is nesting selects within selects: subselect Sub selects are an important and powerful featu\u00aere of SQL. A\u00ae subselect is a select query embedded in another query, usually in the SELECT, FROM, or WHERE clause. JPA supports subqueries in the WHERE clause. Subselects in the\u00ae FROM clause aren\u2019t supported because the query languages doesn\u2019t have transitive closure. The result of a query may not be usable for further selection in a FROM clause. The query languag\u00aee also doesn\u2019t support subselects in the SELECT clause, but y\u00aeou map can subselects to derived properties with @org.hibernate.annotations.Formula, as shown in section 5.1.3.  Subselects can be either correlated with the rest of the query or uncorrelated.",
    "page327": "The result of a subquery may contain either a single row or multiple rows. Typically, subqueries that return single rows perform aggregation. The following subquery returns the total number of items sold by a user; the outer query returns all users who have sold more than one item: The subquery in this example returns the maximum bid amount in the entire system; the outer query returns all bids whose amount is within one (U.S. dollar, Euro, and so on) of that amount. Note that in both cases, parentheses enclose the subquery in JPQL. This is always required Un\u00aecorrelated subqueries are harmless, and there is no reason not to use them when convenient. You can always rewrite them as two queries, because they don\u2019t. reference each other. You should think more carefully about the performance impact. of correlated subqueries. On a mature database, the performance cost of a simple correlated subquery is similar to the cost of a join. But it isn\u2019t necessarily possible to rewrite a correlated subquery using several separate queries.   If a subquery returns multiple rows, you combine it with quantification The following quantifiers are standardized: ALL The expression evaluates to true if the comparison is true for all values in the result of the subquer\u00aey. It evaluates to false if a single value of the subquery result fails the comparison test.  ANY The expression \u00aeevaluates to true if the comparison is true for some (any) value in the result of the subquery. If the subquery result is empty or no value satis\u0002fies the comparison, it evaluates to false. The keyword SOME is a synonym for ANY.  EXISTS Evaluates to true if the result of the subquery consists of one or more values",
    "page328": "This chapter explains query options that you may consider optional or advanced: transforming query results, filtering collections,\u00ae and the Hibernate criteria query API. First, we discuss Hibernate\u2019s ResultTransformer API, with which you can apply a re\u00aesult transformer to a query result to filter or marshal the result with your own code instead of Hi\u00aebernate\u2019s\u00ae default behavior.  In previous chapters, we always advised you to be careful when mapping collections, because it\u2019s rarely worth the effort. In this chapte\u00aer, we introduce collection filters, a native Hibernate feature that makes persistent collections more valuable. Finally, we look at another proprietary Hibernate feature, the org.hibernate.Cri\u0002teria query API, and some situations when you might prefer it to \u00aethe standard JPA query-by-criteria.  Let\u2019s start with th\u00aee transformation of query results.  Transforming query results You can apply a result transformer t\u00aeo a query result so that you can filter or marshal the result with your own procedure instead of the Hibernate default behavior. H\u00aeibernates default behavior provides a set of default transformers that you can replace and/or customize.  The result you\u2019re going to transform is that of a simple query, but you need to access the native Hibernate API org.hibernate.Query through the Session, as shown in the following listing Each object array is a \u201crow\u201d of the query result. Each element of that tuple can be accessed by index: here index 0 is a Long, index 1 a String, and index 2 a Date. The first result transformer we introduce instead returns a List of Lists",
    "page329": "In section 15.3.2, we showed how a query can return instances of a JavaBean dynamically by calling the ItemSummary constructor. In JPQL, you achieve this with the new operator. For criteria queries, you use the construct() method. The ItemSummary class must have a constructor that matches the projected query result.  Alternatively, if \u00aeyour JavaBean doesn\u2019t have the right constructor, you can still instantiate and populate its values through setters and/or fields with the AliasTo\u0002BeanResultTransformer.   You create the transformer with the JavaBean class you want to instantiate, here Item\u0002Summary. Hibernate requires that this class either has no constructor or a public nonargument constructor.  When transforming the query result, Hibernate looks for setter methods and fields with the same names as the aliases in the query. The ItemSummary class must either have the fields itemId, name, unauctioned, or the set\u00aeter methods setItemId(), setName(), and setAuctionEnd(). The fiel\u00aeds or setter method parameters must be of \u00aethe right typ\u00aee. If you have fields that map to some query aliases and setter methods for the rest, that\u2019s fine too.   You should also know how to write your \u00aeown ResultTransformer when none of the built-in ones suits you The built-in transformers in Hibernate aren\u2019t sophisticated; there isn\u2019t much differ\u0002ence between result tuples represented as lists, maps, or object arrays.   Next, we show you how to implement a ResultTransformer. Let\u2019s assume that you want a List<ItemSummary> returned from the query shown in listing 16.1, but you can\u2019t let Hibernate create an instance of ItemSummary through reflection on a con\u0002structor. Maybe your ItemSummary class is predefined and doesn\u2019t have the right con\u0002structor, fields, and setter methods. Instead, you have an ItemSummaryFactory to produce instances of ItemSummary",
    "page330": "For each result \u201crow,\u201d an Object[] tuple must be transformed into the desired result value for that row. Here you access each projection element by index in the tuple array and then call the ItemSummaryFactory to produce the query result value. Hibernate passes the method the aliases found in the query, for each tuple element. You don\u2019t need the aliases in this transformer, though. C You can wrap or modify the result list after transforming the tuples. Here you make. the returned List unmodifiable: ideal for a reporting screen where nothing should change the data. As you can see in the example, you transform query results in two steps: first you customize how to convert each \u201crow\u201d or tuple of the query result to whatever value you desire. Then you work on the entire List of these values, wrapping or converting again.  Next, we discuss another convenient Hibernate feature (where JPA doesn\u2019t have an equivalent): collection filters.  In chapter 7, you saw reasons you should (or rather, shouldn\u2019t) map a collection in your Java domain model. Th\u00aee main benefit of a collection mapping is easier access to data: you can call item.getI\u00aemages() or item.getBids() to access all images and bids associated with an Item. You don\u2019t have to write a JPQL or criteria query; Hibernate will execute the query for you when you start iterating through the collection elements.  The most obvious problem with this automatic data access is that Hibernate will always write the same query, retrieving all images or bids for an Item. Yo\u00aeu can customize the order of collection elements, but even that is a static mapping. What would you do to rend\u00aeer two lists of bids for an Item, in ascending and descending order by cre\u0002ation date? ",
    "page331": " Instead, you can use a Hibernate propri\u00aeetary feature, collection filters, that makes writing these queries easier, using the mapped collection. Let\u2019s say you have a persistent Item instance in memory, probably loaded with the EntityManager API. You want to list all bids made for this Item but further restrict the result to bids made by a particular User. You also want the list sorted in descending order by Bid#amount.The session.createFilter() \u00aemethod accepts a persistent collection and a JPQL query fragment. This query fragment doesn\u2019t require a select or from clause; here it only has a restriction with the where clause and an order by clause. The alias this always refers to elements of the collection, here Bid instances. The filter created is an ordinary org.hibernate.Query, prepared with a bound parameter and executed with list(), as usual.  Hibernate doesn\u2019t execute collection filters in memory. The Item bids collection may be uninitialized when you call the filter and, and if so, remains uninitialized. Furthermore, filters don\u2019t apply to transient collections or query results. You may only apply them to a mapped persistent collection currently referenced by an entity instance managed by the persistence context. The term filter is somewhat misleading, because the result of filtering is a comple\u00aetely new and different collection; the original collection isn\u2019t touched\u00ae.  To the great surprise of everyone, including th\u00aee designer of this feature, even trivial filters turn out to be useful. For example, you can use an empty query to paginate collection elements:",
    "page332": "Here, Hibernate executes the query, loading the collection elements and limiting the returned rows to two, starting with row zero of the result. Usually, you\u2019d use an order by with paginated queries.  You don\u2019t need a from clause in a collection filter, but you can have one if that\u2019s your style. A collection filter doesn\u2019t even need to return elements of the collection being filtered.  This next filter returns any Item sold by any of the bidders All this is a lot of fun, but the most important reason for the existence of collection filters is to allow your application to retrieve collection elements without initializing the entire collection. For large collections, this is important to achieve acceptable perfor\u0002mance. The following query retrieves all bids made for the Item with an amount greater or \u00aeequal to 100: Again, this doesn\u2019t initialize the Item#bids collection but returns a new collection.  Before JPA 2, query-by-criteria was only available as a proprietary Hibernate API. Today, the standardized JPA interfaces are equally as powerful as the old org.hiber\u0002nate.Criteria API, so you\u2019ll rarely need it. But several features are still only available in the Hibernate API, s\u00aeuch as query-by-example and embedding of arbitrary SQL frag\u0002ments. In the following section, you find a short overview of the org.hibernate .Criteria API and some of its unique options Using the org.hibernate.Criteria and org.hibernate.Example interfaces, you can build queries programmatically b\u00aey creating and combining org.hibernate.crite\u0002rion.* instances. You see how to use these APIs and how to express selection, restriction, joins, and projection. We assume that you\u2019ve read the previous chapter and that you know how these operations are translated into SQL.",
    "page333": ". All query examples shown here have an equivalent JPQL or JPA criteria example in the previous chapter, so you can easily flip back and\u00ae forth if you need to compare all three APIs.  Let\u2019s start with some basic selection examples. When you\u2019re ready to execute the query, \u201cattach\u201d it to a Session with getExecutable\u0002Criteria().  Note that thi\u00aes is a unique feature of the Hibernate criteria API. With JPA, you always need at least an EntityManagerFacto\u00aery to get a CriteriaBuilder.  You can declare the order of the results, equivalent to an order by clause in JPQL. The following query loads all User instances sorted in ascending order by first and last name: In this example, the code is written in the fluent style (using method chaining); method's such as add Order() return the original org.hibernate.Criteria.   Next, we look at restricting the selected records The Restrictions interface is the factory for individual Criterion you can add to the Criteria. Attributes are addressed with simple strings, here Item#name with \"name\".  You can also match substrings, similar to the like operator in \u00aeJPQL. The following query loads all User instances with username starting with \u201cj\u201d or \u201cJ\u201d A unique feature of the Hibernate Criteria API is the ability to write plain SQL frag\u0002ments in restrictions. This query loads all User instances with a username shorter than eight characters Hibernate sends the SQL fragment to th\u00aee database as is. You need the {alias} place\u0002holder to prefix any table alias in the final SQL; it always refers to the table the root entity is mapped to (USERS, in this case). You also apply a position parameter (named parameters aren\u2019t supported by this API) and specify its type as StandardBasic\u0002Types.INTEGER",
    "page334": "The result of this query is a List of Object[], one array for each tuple. Each array contains a Long (or whatever the type of the user\u2019s identifier is), a String, and an Address.  Just as with restrictions, you can add arbitrary SQL expressions and function c\u00aealls to projections This query returns a List of Strings, where strings have the form \u201c[Item name]:[Auc\u0002tion end date]\u201d. The seco\u00aend parameter for the projection is the name of the alias(es) you used in the query: Hibernate needs this to read the value of the ResultSet. The type of each projected element/alias is also needed: here, StandardBasic\u0002Types.STRING.  Hibernate supports grouping and aggregation. This query coun\u00aets users\u2019 last names This query returns all Bid instances of any Item sold by User \u201cjohndoe\u201d that doesn\u2019t have a buyNowPrice. The first inner j\u00aeoin of the Bid#item association is made with createCriteria(\"item\") on the root Criteria of the Bid. This nested Criteria now represents the association path, on which another\u00ae inner join is made with createCri\u0002teria(\"seller\"). Further restrictions are placed on each join Criteria; they will be combined with logical and in the where clause of the final SQL query. This query returns all Item instances, loads the Item#bids collection with an outer join, and loads Bid#bidder with an inner join. The Ite\u00aem#seller is also loaded: because it can\u2019t be null, it doesn\u2019t matter whether an inner or outer join is used. As always, don\u2019t fetch several collections in one query, or you\u2019ll create a Cartesian products (see section 15.4.5).   Next, you see that subqueries with criteria also work with nested Criteria instances.",
    "page335": "The DetachedCriteria is a query that returns the number of items sold restricted by a given User. The restriction relies on t\u00aehe alias u, so this is a correlated subquery. The \u201couter\u201d query then embeds the DetachedCriteria and provides the alias u. Note that the subquery is the\u00ae right operand of the lt() (less than) operation, which translates into 1 < ([Result of count query]) in SQL. Again, the position of the operands dictates that the comparison is based on geAll() (greater or equal than all) to find the bids with \u201cless or equal \u00aethan 10\u201d amount.  So far, there are a few good reasons to use the old org.hibernate.Criteria API. You really should use the standardized JPA query languages in new applications, though. The most interesting features of the old proprietary API we\u2019ve shown are embedded SQL expressions in restrictions and projections. Another Hibernate-only feature you may find interesting is query-by-example The idea behind example queries is that you provide an example entity instance, and Hibernate loads all entity instances that \u201clook like the example.\u201d This can be convenient if you have a complex search screen in your user interface, because you don\u2019t have to write extra classes to hold the e\u00aentered\u00ae search terms.  Let\u2019s say you have a search form in your application where you can search for User instances by last name. You can bind the form field for \u201clast name\u201d directly to the User#lastname property and \u00aethen tell Hibernate to load \u201csimilar\u201d User instances",
    "page336": "Create an \u201cempty\u201d instance of User as a template for your search, and set the property values you\u2019re looking for: people with the last name \u201cDoe\u201d Create an instance of \u00aeExample with the template. This API\u00ae allows you to fine-tune the search. You want the case of t\u00aehe last name to be ignored, and a substring search, so \u201cDoe\u201d, \u201cDex\u201d, or \u201cDoe Y\u201d will match.  D The User class has a Boolean property called activated. As a primitive, it can\u2019t be null, and its default value is false, so Hibernate would include it in the search and only return users that aren\u2019t activated. \u00aeYou want all users, so tell \u00aeHibernat\u00aee to ignore that property. E The Example is \u00aeadded to a Criteria as a restriction. Because you\u2019ve written the User entity class following JavaBean rules, binding it to a UI form should be trivial. It has regular getter and setter methods, and you ca\u00aen create an \u201cempty\u201d instance with the public no-argument constructor (remember our discus\u0002sion of constructor design in section 3.2.3.)  One obvious disadvantage of the Example API is that any string-matching options, such as ignoreCase() and enableLike(), apply to all string-valued properties of the template. If you searched for both last name and first name, both would be case insensitive substring matches. nsensitive substring matches.  By default, all non-null valued properties of the given entity template are added to the restriction of the example query. As shown in the last code snippet, you can manually exclude properties of the entity template by name wi\u00aeth excludePrope\u00aerty",
    "page337": "Other exclusion options are exclusion of zero-valued properties (such as int or long) with excludeZeroes() and disabling exclusion altogether with excludeNone(). If n\u00aeo properties are excluded, any null property of the template is added to the restriction in the SQL query with an is null check.  If you need more control over exclusion and inclusion of properties, you can extend Example and write your own PropertySelector:  After adding an Example restriction to a Criteria, you can add further restrictions to the query. Alternatively, you can add multiple example restrictions to a single query. The following query returns all Item instances with names starting with \u201cB\u201d or \u201cb\u201d and a seller matching a User example: You used the ResultTransformer API to write custom code to process a query result, returning a list of lists and a list of maps, and mapping aliases to bean properties.  We covered Hibernate\u2019s collection-filtering interfaces as well as making better use of mapped persistent collections. You explored the older Hi\u00aebernate Criteria query facility and when you might use it instead of the standardized criteria queries in JPA. We covered all the rela\u0002tional and Hibernate goodies using this API\u00ae: selection and ordering, restriction, projection and \u00aeaggregation, joins, subselects, and example queries.",
    "page338": "In this chapter, we cover customizing and embedding SQL in a Hibernate application. SQL was created in the 1970s, but ANSI didn\u2019t standardized it until 1986. Although each update of the SQL standard has seen new (and many controversial) features, every DBMS product that supports SQL does so in its own unique way. The burden of \u00aeportability is again on the database application developers. This is where Hibernate helps: its built-in query languages produce SQL that depends on the configured database dialect. Dialects also help produce all other automatically generated SQL (for example, when Hibernate has to retri\u00aeeve a collection on demand). With a simple dialect switch, you can run your application on a different DBMS. Hibernate generates all SQL statements for you, for all create, read, update, and delete (CRUD) operations.  Sometimes, though, you need more control than Hibernate and the\u00ae Java Persistence API provide: you need to work at a lower level of abstraction. With Hibernate, you can write your own SQL statements: Fall back to the JDBC API, and work directly with\u00ae the Connection, Prepare\u00aed\u0002Statement, and ResultSet interfaces. Hibernate provides the Connection, so you don\u2019t have to maintain a separate connection pool, and your SQL state\u0002ments execute within the same (current) transaction. \uf0a1 Write plain SQL SELECT statements, and either embed them within your Java code or externalize them (in XML files or annotations) as named queries. You execute these SQL\u00ae queries with the Java Persistence API, just like a regular JPQL query. Hibernate can then transform the query result according to your map\u0002ping. This also works with stored procedure calls.",
    "page339": "Replace SQL statements generated by Hibernate with your own hand-written SQL. For example, when Hibernate loads an entity instance with em.find() or loads a collection on-demand, your own SQL query can perform the load. You can also write your ow\u00aen Data Manipulation Language (DML) statements, such as UPDATE, INSERT, and DELETE. You might even call a stored procedure to preform a CRUD operation. You can replace all SQL statements automatically generated by Hibernate with custom statements. We start with JDBC fallback usage and then discuss Hibernate\u2019s automa\u00aetic result-map\u0002ping capabilities. Then, we show you how to override queries and DML statements in Hibernate. Last, we discuss integration with stored database procedures. Sometimes you want Hibernate to get out of \u00aethe way and directly access the database through the JDBC API. To do so, you need a java.sql.Connection interface to write and execute your own PreparedStatement and direct access to your statement ResultSet. Because Hibernate already knows how to obtain and close database con\u0002nections, it can provide your application with a Connection and release it when you\u2019re done.  This functionality is available with the org.hibernate.jdbc.Work API, a\u00ae callback\u0002style interface. You encapsulate your JDBC \u201cwork\u201d by implementing this interface; Hibernate calls your implementation providing a Connection. The following example executes an SQL SELECT and iterates through the ResultSet For this \u201cwork,\u201d an item identifier is needed, enforced with the final field and the con\u0002structor paramet",
    "page340": "The execute() method is called by Hibernate with a JDBC Connection. You don\u2019t have to close the connection when you\u2019re done. D You have to close and release other resources you\u2019ve obtained, though, such as the PreparedStatement and ResultSet In this case, Hibernate has already enlisted the JDBC Connection it provides with the current system transaction. Your statements are committed when the system transaction is committed, and all operations, whether executed with the EntityManager or Session API, are part of the same unit of work. Alternatively, if you want to return \u00aea value from your JDBC \u201cwork\u201d to the application, implement the interface org.hiber\u0002nate.jdbc.ReturningWork.  There are no limits on the JDBC operations you can perform in a Work implementation. Instead of a PreparedStatement, you may use a CallableStatement and execute a stored procedure in the database; you have full access to the JDBC API.  For simple queries and working with a ResultSet, such as the one in the previous example, a more convenient \u00aealter\u00aenative is available.  When you execute an SQL SELECT query with the JDBC API or execute a stored procedures that returns a ResultSet, you iterate through each row of this result set and retrieve the data you need. This is a labor-intensive task, and you end u\u00aep duplicating the same lines of code repeatedly. When you execute an SQL SELECT query with the JDBC API or execute a stored proce\u0002dure that returns a ResultSet, you iterate through each row of this result set and retrieve the data you need. This is a labor-intensive task, and you end up duplicating the same lines of code repeatedly.",
    "page341": "The returned Item instances are in persistent state, managed by the current persistence context. The result is therefore the same as with the JPQL query select i from Item i.  For this transformation, Hibernate reads the result set of the SQL query and tries to discover the column names and\u00ae types as defined in your entity mapping metadata. If the column AUCTIONEND is returned, and it\u2019s mapped to the Item#auctionEnd property, Hibernate knows how to populate that property and returns fully loaded entity instances.  Note that Hibernate expects the query to return all columns required to create an instance of Item, including all properties, embedded components, and foreign key\u00ae columns. If Hibernate can\u2019t find a mapped column (by name\u00ae) in the result set, an exception is thrown. You may have to use aliases in SQL to return the same column names as defined in your entity mapping metadata.  The interfaces javax.persistence.Query and org.hibernate.SQLQuery both support parameter binding. Th\u00aee following query returns only a single Item\u00ae entity instance Although available in Hibernate for both APIs, the JPA sp\u00aeecification doesn\u2019t consider named parameter binding for native queries portable. Therefore, some JPA providers may not support named parameters for native queries.   If your SQL query doesn\u2019t return the columns as mapped in your Java entity class, and you can\u2019t rewrite the query with aliases to rename columns in the result, you must create a result-set mapping",
    "page342": "The following query returns a List of managed Item entity instances. All columns of the ITEM table are included in the SQL projection, as required for the construction of an Item instance. But the query renames the NAME column to EXTENDED_NAME with an alias in the projection Hibernate can no longer automatically match the result set fields to Item properties: the NAME column is missing from the result set. You therefore specify a \u201cresult map\u0002ping\u201d with You map all fields of the result set to properties of the entity class. Even if only one field/column doesn\u2019t match the already mapped column name (here EXTENDED_NAME), all other columns and properties have to be mapped as wel\u00ael  SQL result mappings in annotations are difficult to read and as usual with JPA\u00ae annotations, they only work when declared on\u00ae a class, not in a package-info.java meta\u0002data file. We prefer externalizing such mappings into \u00aeXML files. The followin\u00aeg provides the same mapping: If both result-set mappings have the same name, the mapping declared in XML overrides the one defined with annotations.  You can also externalize the actual SQL query with @NamedNativeQuery or <named\u0002nativ\u00aee-query>, as shown in section 14.4. In all following examples, we keep the SQL statement embedded in the Java code, because this will make it easier for you to understand what the code does. But most of the time, you\u2019ll see result-set mappings in the more succinct XML syntax.",
    "page343": "With the Hibernate API, you can perform the result-set mapping directly within the query through alias placeholders. When calling addEntit\u00aey()\u00ae, you provide an alias, here i. In the SQL string, you then let Hibernate generate the actual aliases in the projection with placeholders such as {i.name} and {i.auctionEnd}, which refer to properties of the Item entity. No additional result-set mapping declaration is necessary; Hibernate generates\u00ae the aliases in the SQL string and knows how to read the property values from the query ResultSet. This is much more convenient than the JPA result-set mapping option.  Or, if you can\u2019t or don\u2019t want to \u00aemodify the SQL statement, use add Root() and add Property() on the org.hibernate.SQLQuery to per\u00aeform the mapping This is effectively an eager fetch of the association Item#seller. Hibernate knows that each row contains the fields for an Item\u00ae and a User enti\u00aety instance, linked by the SELLER_ID.  The duplicate columns in the result set would be i.ID and u.ID, which both have the same name. You\u2019ve renamed them with an a\u00aelias to ITEM_ID and USER_ID, so you have to map how the result set is to be transformed As before, you have to map all fields of each entity result to column names, even if only two have different names as the original entity mapping.  This query is much easier to map with the Hibernate API:",
    "page344": "Hibernate will add auto-generated unique aliases to the SQL statement for the {i.*} and {u.*} placeholders, so the query won\u2019t return duplicate column names.   You may have noticed the dot syntax in the previous JPA result mapping for the home Address embedded component in a User. Let\u2019s look at this special case again We\u2019ve shown this dot syntax several times before when discussing embedded components: you reference the street property of home Address with homeAddress.street. For nested embedded components, you can write homeAddress.city.name if City isn\u2019t just a string but another embed\u00aedable class.  Hibernate\u2019s SQL query A\u00aePI also supports the dot syntax in alias placeholders for component properties. Here are the same query and result-set mapping: The query (outer) joins the ITEM and BID tables. The projection returns all columns required to construct\u00ae Item and Bid instances. The query renames duplicate columns such as ID with aliases, so field names are unique in the result. C Because of the renamed fields, you have to map each column to its respective entity property. D Add a Fetch Return for the bids collection with the alias of the owning entity  and map the key a\u00aend element special properties to the foreign key column BID_ITEM_ID and the identifier of the Bid. Then the code maps each property of Bid to a field of the result set. Some fields are mapped twice, as required by Hibernate for construction of the collection The number of rows in the result set is a product: one item has three bids, one item has one bid, and the last item has no bids, for a total of five rows in the result. F The first element of the result tuple is the Item instance; Hibernate initialized the bids collection",
    "page345": "The second element of the result tuple is each Bid. Alternatively, if you don\u2019t have to manually map the result because the field names returned by your SQL query match the already-mapped columns of the entities, you can let Hibernate insert ali\u00aeases into your SQL statement with placeholder\u00aes: Eager fetching of collections with dynamic SQL result mappings is only available with the Hibernate API; it\u2019s not standardized in JPA  So far, you\u2019ve seen SQL queries returning managed entity instances. You can also return transient instances of any class with the right constructor. The returned column types have to match the constructor parameter types; Hibernate would default to BigInteger for the ID column, so you map it to a Long with the class attribute.  The Hibernate API gives you a choice. You can either use an existing result map\u0002ping for the query by name, or apply a r\u00aeesult transformer, as you saw for JPQL queries in section 16.1: You can use an existing result mapping. C Alternatively, you can map the fields returned by the query as scalar values. Without a result transformer, you\u2019d get an Object[] for each result row. D Apply a built-in result transformer to turn the Object[] into instances of ItemSummary. As explained in section 15.3.2, Hibernate can use any class constructor with such a mapping. Instead of ItemSummary, you can construct Item instances. They will be in either transient or detached state, depending on whether you return and map an identifier value in your query.   You can also mix different kinds of result mappings or return scalar values directly.",
    "page346": "Hibernate usually generates CRUD SQL statements at startup. It caches the SQL statements internally for future use, thus avoiding any runtime costs of \u00aeSQL generation forthe most c\u00aeommon operations. You\u2019ve seen how you can override the R in CRUD, sonow, let\u2019s do the same for CUD. For each entity, you can define custom CUD SQL statements with the Hibernate annotations @SQLInsert, @SQLUpdate, and @SQLDelete,respectivelyYou need to be careful with the binding of arguments for SQL statements and parameter placeholders (the ? in the statements). For CUD customization, Hibernate onlysupports positional parameters. What is the right order for the parameters? There\u2019s an internal order to howHibernate binds arguments to SQL parameters for CUD statements. The easiest way tofigure out the right SQL statement and parameter order is to let Hibernate generatean example for you. Without any custom\u00ae SQL statements, enable DEBUG logging forthe org.hibernate.persister.entity category, and search the Hibernate startupoutput for lines like the following:These automatically generated SQL statements show the right parameter order, andHibernate always binds the values in that order. Copy the SQL statements you want tocustomize into annotations, and make the necessary changes. A special case is properties of an entity class mapped to another table with@SecondaryTable. The CUD customization statements we\u2019ve shown so far only refer tothe columns of the main entity table. Hibernate still executes automatically generatedSQL statements for insertion, updates, and deletion of rows in the secondary table(s).",
    "page347": "statements in an XML file. This also simplifies ad hoc testing, because you can copyand paste SQL statements between an XML file and your SQL database console. You\u2019ve probably noticed that all the SQL examples in the previo\u00aeus sections weretrivial. In fact, none of the examples required a query written in SQL we could haveused JPQL in each case. To make the next example moreinteresting, we write a query that can\u2019t be expressed inJPQL, only in SQL. This is the mapping of the association a \u00aeregular@ManyToOne of the PARENT_I\u00aeD foreign key column:Categories form a tree. The root of the tree is a Category node without a parent. Thedatabase data for the ex\u00aeample tree is in figure 17.2. You can also represent this data as a tree diagram, as shown in figure 17.3. Alternatively, you can use a sequence of paths and the level of each node:Now, consider how your application loads Category instances. You may want\u00ae to findthe root Category of the tree. This is a trivial JPQL query:You can easily query for t\u00aehe categories in a particular level of the tree, such as all children of the root:This query\u00ae will only return direct children of the root: here, categories Two and Three. How can you load the entire tree (or a subtree) in one query? This isn\u2019t possible withJPQL, because it would require recursion: \u201c\u00aeLoad categories at this level, then all the children on the\u00ae next level, then all the children of those, and so on.\u201d In SQL, you can writesuch a query, using a common table expression (CTE), a feature also\u00ae known as subquery factoring.",
    "page348": "It\u2019s a complex query\u00ae, and we won\u2019t spend too much time on it here. To understand it,read the last SELECT, querying the CATEGORY_LINK view. Each row in that view represents a node in the tree\u00ae. The view is declared here in the WITH() AS operation. TheCATEGORY_LINK view is a combined (union) result of two other SELECTs. You add additional information to the view during recursion, such as the PATH and the LEVEL ofeach node.The XML maps the ID, CAT_NAME, and PARENT_ID fields to properties of the Categoryentity. The mapping returns the \u00aePATH and LEVEL as additional scalar values. To execute the named SQL query and access the result, write the following:Each tuple contains a managed, persistent Category instance; its path in the tree as astring (such as /One, /One/Two, and so on); and the tree level of the node. Alterna\u00aetively, you can declare and map an SQL query in a Hibernate XML metadata file:We left out the SQL query in this snippet; it\u2019s the same as the SQL statement shownearlier in the JPA example. As mentioned in section 14.4, with regard to the execution in Java code, it doesn\u2019tmatter which syntax you declare your named queries in: XML file or annotations. Eventhe language doesn\u2019t matter it can be JPQL or SQL. Both Hibernate and JPA queryinterfaces have methods to \u201cget a named query\u201d and execute it independently fromhow you defined it. This concludes our discussion of SQL result mapping for queries. The next subjectis customization of SQL statements for CRUD operations, replacing the SQL automatically generated by Hibernate for creating, reading, updating, and deleting data in thedatabase.",
    "page349": "The first custom SQL you write loads an entity instance of the User class. All the following code examples show the same SQL that Hibern\u00aeate executes au\u00aetomatically bydefault, without much customization this helps you understand the mapping \u00aetechnique more quickly. You can customize retrieval of an entity instance with a loaderHibernate has two requirements when you override an SQL query to load an entityinstance: Write a named query that retrieves the entity instance. We show an example inSQL, but as always, you can also write named queries in JPQL. For an SQL query,you may need a custom result mapping, as shown earlier in this chapter.Activate the query on \u00aean entity class with @org.hibernate.annotationsLoader. This enables the query as the replacement for\u00ae the Hibernate-generated query.Let\u2019s override how Hibernate loads an instance of the User entity, as shown in the following listing.Annotations declare the query to load an instance of User; you can also declare it inan XML file (JPA or Hibernate metadata). You can call this named query directly inyour data-access\u00ae code when needed. The query must have exactly one parameter placeholder, which Hibernate sets as theidentifier value of the instance to load. Here it\u2019s a positional parameter, but a namedparameter would also work. For this trivial query, you don\u2019t need a custom result-set mapping. The User class mapsall fields returned by the query. Hibernate can automatically transform the result.",
    "page350": "Setting the loader for an entity class to a named query enables the query for all operations that r\u00aeetrieve an instance of User from the database. There\u2019s no in\u00aedication of thequery language or where you declared it; this is independent of the loader declaration.In \u00aea named loader query for an entity, you have to SELECT (that is, perform a projection for) the following properties of the entity class: The value of the identifier property or properties, if a composite primary key isused. All scalar properties of basic type. All properties of embedded components. An entity identifier valu\u00aee \u00aefor each @JoinColumn of each mapped entity association such as @ManyToOne owned by the loaded entity class. All scalar properties, embedded component properti\u00aees, and association joinreferences that are inside a @SecondaryTable annotation. If you enable lazy loading for some properties, through interception and bytecodeinstrumentation, you don\u2019t need to load the lazy properties (see section 12.1.3).Hibernate always calls the enabled loader query wh\u00aeen a User has to be retrieved fromthe database by identifier. When you call em.find(User.class, USER_ID), your custom query will execute. When you call someItem.getSeller().getUsername(), andthe Item#seller proxy has to be initialized, your custom query will load the data. You may also want to customize how Hibernate creates, updates, and deletes aninstanc\u00aee of User in the database.",
    "page351": "Hibernate usually generates \u00aeCRUD SQL statements at startup. It caches the SQL statements internally for future use, thus avoiding any ru\u00aentime costs of SQL generation forthe most common operations\u00ae. You\u2019ve seen h\u00aeow you can override the R in CRUD, sonow, let\u2019s do the same for CUD. For each entity, you can define custom CUD SQL statements with the Hibernate annotations @SQLInsert, @SQLUpdate, and @SQLDelete,respectivelyYou need to be careful with the binding of arguments for SQL statements and paramete\u00aer placeholders (the ? in the statements). For CUD customization, Hibernate onlysupports positional parameters. What is the right order for the parameters? There\u2019s an internal order to howHibernate binds arguments to SQL parameters for CUD statements. The easiest way tofigure out the right SQL statement and parameter order is to let Hibernate generatean example for you. Without any custom SQL statements, enable DEBUG logging forthe org.hibernate.persister.entity category, and search the Hibernate startupoutput for lines like the following:These automatically generated SQL statements show the right parameter order, andHibernate always binds the values in that order. Copy the SQL statements you\u00ae want tocustomize into annotations, and make the necessary changes. A special case is properties of an entity class mapped to another table with@SecondaryTable. The CUD customization statements we\u2019ve shown so far only refer tothe columns of the main entity table. Hibernate still executes automatically generatedSQL statements for insertion, updates, and deletion of rows in the secondary table(s).",
    "page352": "You can customize th\u00aeis SQL \u00aeby adding the @org.hibernate.annotations.Table annotation to your entity class and setting its sqlInsert, sqlUpdate, and sqlDelete attributes. If you prefer to have your CUD SQL statements\u00ae in XML, your only choice is to mapthe entire entity in a Hibernate XML metadata file. The elements in this proprietarymapping format for custom CUD statements are <sql-insert>, <sql-update>, and<sql-delete>. Fortunately, CUD statements are usually much more trivial than queries, so annotations are fine in most applications. You\u2019ve now added custom SQL statements for CRUD operations of an entityinstance. Next, we show how to override SQL statements  loading and modifying acollection.Let\u2019s override the SQL stafortement Hibernate uses when loading the Item#images collection. This is a collection of embeddable components mapped with @ElementCollection; the procedure is the same for collections of basic types or many\u00ae-valuedentity associations (@OneToMany or @ManyToMany)As before, you declare that a named query will load the collection. This time, however,you must declare and map the result of the query in a Hibernate XML metadata file,which is \u00aethe only facility that supports mapping of query results to collection properties:The query has to have one (positional or named) parameter. Hibernate \u00aesets its valueto the entity identifier that owns the collection. Whenever Hibernate need to initializethe Item#images collection, Hibernate now executes your custom SQL query.",
    "page353": "Sometimes you don\u2019t have to override the entire SQL statement for loading a collection: for example, if you only want to add a restriction to the generated SQL statement. Let\u2019s say the Category entity has a collection of Item references, and the Itemhas an activation flag. If the property Item#active is false, you don\u2019t want to load itwhen accessing the Category#items collection. You can append this restriction to theSQL statement with the Hibernate @Where annotation on the collec\u00aetion mapping, as aplain SQL fragment:To find the right parameter order, enable DEBUG logging for the org.hibern\u00aeate.persister.collection category and search the Hibernate startup output for th\u00aee generated SQL statements for this collection, before you add your custom SQL annotations. A new annotation here \u00aeis @SQLDeleteAll, which only applies to collections ofbasic or embeddable types. Hibernate executes this SQL statement when the entirecollection has to be removed from the database: for example, when you call someItem.getImages().clear() or someItem.setImages(new HashSet()). No @SQLUpdate statement is necessary for this collection, because Hibernatedoesn\u2019t update rows for this collection of embeddable type. When an Image propertyvalue changes, Hibernate detects this as a new Image in the collection (recall thatimages are compared \u201cby value\u201d of all their properties). Hibernate will DELETE the oldrow and INSERT a new row to persist this change. Instead of lazily loading collection elements, you can eagerly fetch them when theowning entity is loaded. You can also override this query with a custom SQL statement.",
    "page354": "Let\u2019s consider the\u00ae Item#bids collection and how it\u2019s loaded. Hibernate enables lazyloading by default because you mapped with @OneToMany, so it\u2019s only when you beginiterating through the collection\u2019s elements that Hibernate will execute a query andretrieve the data. Hence, when loading the Item entity, you don\u2019t have to load any collection data. If instead you want to fetch the Item#bids collection eagerly when the Item isloaded, firs\u00aet enable a custom loader query on the Item class:As in the previous section, you must declare this named query in a Hibernate XMLmetadata file; no annotations are available to fetch collections with named queries.Here is the SQL statement to load an Item and its bids collection in a single OUTER JOIN:You saw th\u00aeis query and result mapping in Java code earlier in this chapter in the section \u201cEager-fetching collections.\u201d Here, y\u00aeou apply an additional restriction to only onerow of ITE,\u00ae with th\u00aee given primary key value. You can also eagerly lo\u00aead single-valued entity associations such as a @ManyToOnewith a custom SQL statement. Let\u2019s say you want to eagerly load the bidder when a Bidentity is retrieved from the database. First, enable a named query as the entity loader:Unlike custom queries used to load collections, you can declare this named querywith standard annotations (of course, you can also have it in an XML metadata file, ineither the JPA or Hibernate syntax).",
    "page355": "An INNER JOIN is app\u00aeropriate for\u00ae this SQL query, because a Bid always has a bidderand the BIDDER_ID foreign key column is neve\u00aer NULL. You rename duplicate ID columns in the \u00aequery, and because you rename them to BID_ID and USER_ID, a customresult mapping is ne\u00aecessaryHibernate executes this custom SQL query and maps the result when loading aninstance of the Bid class, either through em.find(\u00aeBid.class, BID_ID) or when it hasto initialize a Bid proxy. Hibernate loads the Bid#bidder right away and overrides theFetchType.LAZY setting on the association. You\u2019ve now customized Hibernate operations with your own SQL statements. Let\u2019scontinue with stored procedures and explore the options for integrating them intoyour Hibernate application.Stored procedures are common\u00ae i\u00aen database application development. Moving codecloser to the data and executing it inside the database has distinct advantages. Youend up not duplicating functionality and logic in each program that accesses the data.A different point of view is that a lot of business logic shouldn\u2019t be duplicated, so itcan be applied all the time. This includes procedures that guarantee the integrity ofthe data: for example, con\u00aestraints tha\u00aet are too complex to implement declaratively.You\u2019ll usually also find triggers in a database that contain procedural integrity rules. Stored procedures have advantages for all processing on large amounts of data,such as reporting and statistical analysis. You should always try to avoid moving largedata sets on your network and between your database and application servers, so astored procedure is the natural choice for mass data operations.",
    "page356": "There are of course (legacy) systems that implement even the most basic CRUDoperations with stored procedures. In a variation on this theme, some systems don\u2019tallow any direct\u00ae use of SQL INSERT, UPDATE, or DELETE, but only stored procedure calls;these systems also had (and sometimes still have) their place. In some DBMSs, you can declare user-defined functions\u00ae, in addition to, or insteadof, stored procedures. The summary in table 17.1 shows some of \u00aethe differencesbetween procedures and functionsIt\u2019s difficult\u00ae to generalize and compare procedures and functions beyond these obvious differences. This is one area where D\u00aeBMS support differs widely; some DBMSsdon\u2019t support stored procedures or user-defined functions, whereas others roll bothinto one (for example, Postgre\u00aeSQL has only user-defined functions). Programminglanguages for stored procedures are usually proprietary\u00ae. Some database\u00aes even supportstored procedures written in Java. Standardizing Java stored procedures was part ofthe SQLJ effort, whic\u00aeh unfortunately hasn\u2019t been successful. In this section, we show you how to integrate Hibernate with MySQL stored procedures and PostgreSQL user-defined functions. First, we look at defining and callingstored procedures with the standardized Java Persistence API and the native HibernateAPI. Then, we customize and replace Hibernate CRUD operations with procedurecalls. It\u2019s important that you read the previous sections before this one, because theintegration of stored procedures relies on the same mapping options as other SQLcustomization in Hibernate.",
    "page357": " As before in this chapter, the actual SQL stored procedures we cover in the examples are trivial so you can focus on the more important parts how to call the procedures and use the API in your application. When calling a stored procedure, you typically want to provide input and receivethe output of the procedure. You can distinguish between procedure\u00aes that Return a result set Return multiple result sets Update data and return the count of updated rows Accept input and/or output parameters Return a cursor, referencing a resu\u00aelt in the databaseLet\u2019s start with the simplest case: a stored procedure that doesn\u2019t have any parametersand only returns data in a result set.As you\u2019ve previously seen in this chapter, Hibernate automatically maps the columnsreturned in the result set to properties of the Item class. The Item instances returnedby this query will be managed and in persistent stat\u00aee. To customize the mapping ofreturned columns, provide the name of a result-set mapping instead of the Item.class parameter.The Hibernate getCurrent() method already indicates that a procedure may returnmore than a sing\u00aele ResultSet. A procedure may return multiple result sets and evenreturn update counts if it modified data.The following MySQL procedure returns all rows of the ITEM table that weren\u2019tapproved and all rows that were already approved, and also sets the APPROVED flag forthese rows.",
    "page358": "In the application, you get two result sets and an update count. Accessing and processing the results of a procedure call is a bit more involved, b\u00aeut JPA is closely alignedto plain JDBC, so this kind of code should be familiar if you\u2019ve worked with storedprocedures:Execute the procedure call with execute(). This method returns true if the firstresult of the call is\u00ae a result set and false if the first result is an updat\u00aee count.Process all results of the\u00ae call in a loop. Stop looping when no more results are available, which is always indicated by hasMoreResults() returning false and getUpdateCount() returning -1.If the current result is a result set, read and process it. Hibernate maps the columns ine\u00aeach result set to managed instances of the Item class. Alternatively, provide a resultset mapping name applicable to all result sets returned by the call.If the current result is an update count, getUpdateCount() returns a value greaterthan -1.hasMoreResults()\u00ae advances to the next result and indicates the type of that result.The alternative procedure execution with the Hibernate API may seem morestraightforward. It hides some of the complexity of testing the type of each result andwhether there is more procedure output to process:As long as getCurrent() doesn\u2019t return null, there are more outputs to process. An output may be a result set: test and cast it. If an output isn\u2019t a result set, it\u2019s an update count. Proceed with the next output, if any.Next, we consider stored procedures with input and output parameters.",
    "page359": "The following MySQL procedure returns a row for the given identifier f\u00aerom the ITEMtable, as well as the total number of items:This next procedure returns a result set\u00ae with the data of the ITE\u00aeM row. Additionally,the output parameter PARAM_TOTAL is set. To call this procedure in JPA, you must firstregister all parameters:Register all parameters by position (starting at 1) and their type. Bind values to the input parameters.Retrieve the result set returned \u00aeby the procedure. After you\u2019ve retrieved the result sets, you can access the output parameter values.You can also register and use named parameters, but you can\u2019t mix named and positional parameters in a particular call. Also, note that any parameter names you choosein your Java code don\u2019t have to match the names of the parameters in your stored procedure declaration. Ultimately, you must register the parameters in the same order asdeclared in the signature of the procedure. Register all parameters; you can bind input values directly. Output parameter registratio\u00aens can be reused later to read the output value. Process all returned result sets before you access any ou\u00aetput parameters. Access the output parameter value through\u00ae the registration.The following MySQL procedure uses input parameters to update a row in the ITEMtable with a new item name:In this example, you can also see how named parameters work and that names in theJava code don\u2019t have to match names in the stored proced\u00aeure declaration. The orderof parameter registrations is still important, though; PARAM_ITEM_ID must be first, andPARAM_ITEM_NAME must be second.",
    "page360": "A shortcut for calling procedures that don\u2019t return a result set but modify datais executeUpdate(): its return value is your update count. Alternatively, you canexecute() the procedure and call getUpdateCount(). This is the same procedure execution using the Hibernate API:Because you know there is no result set returned by this procedure, you can directlycast the first (current) output to UpdateCountOutputNext, instead of returning a resu\u00aelt set, we see procedures that return a cursor r\u00aeeference.MySQL doesn\u2019t support returning cursors from stored procedures. The followingexample only works on PostgreSQL. This stored procedure (or, because this is thesame in PostgreSQL, this user-defined function) returns a cursor to all rows of theITEM table:The type of the parameter is void, because its only purpose is t\u00aeo prepare the call internally for reading data with the cursor. When you call getResultList(), Hibernateknows how to get the desired output.Supporting database cursors across DBMS dialects is difficult, and Hibernate has somelimitations. \u00aeFor example, with PostgreSQL, a cursor parameter must always be the firstregistered parameter, and (because it\u2019s a function) only one cursor should bereturned by the database. With the PostgreSQL dialect, Hibernate doesn\u2019t supportnamed parameter binding if a cursor return is involved: you must use positionalparameter binding. Consult your Hibernate SQL dialect for more information; the relevant methods are Dialect#getResultSet(CallableStatement) and so on. This completes our discuss\u00aeion of APIs for direct stored procedure calls. Next, youc\u00aean also use stored procedures to override the statements generated by Hibernatewhen it loads or store\u00aes data.",
    "page361": "The first customized CRUD operation you write loads an entity ins\u00aetance of the Userclass. Previously in this chapter, you used a native SQL query with a loader to implement this requirement. If you have to call a stored procedure to load an instance ofUser, the process is equally straightforward.First, write a named query that calls a stored procedure for examp\u00aele, in annotationson the User class:Compare this with the previous customization in section 17.3.1: the declaration of theloader is still the \u00aesame, and it relies \u00aeon a defined named query in any supported language. You\u2019ve only changed the named query, which you can also move into an XMLmetadata file to further isolate and separate this concern.JPA doesn\u2019t standardize what goes into a @NamedNativeQuery, you\u2019re free to writeany SQL statement.\u00ae With the JDBC escape syntax in curly braces, you\u2019re saying, \u201cLetthe JDBC driver figure out wha\u00aet to do here.\u201d If your JDBC driver and DBMS understandstored procedures, you can invoke a procedure with {call PROCEDURE}. Hibernateexpects that the procedure will return a result set, and the first row in the re\u00aesult\u00ae set isexpected to have the columns necessary to construct a User instance. We listed therequired columns and properties earlier, in section 17.3.1. Also remember that youcan always apply a result-set mapping if the column (names) returned by your procedure aren\u2019t quite right or you can\u2019t change the pr\u00aeocedure code. The stored procedure must have a signature matching the call with a single argument. Hibe\u00aernate sets this identifier argument when loading an instance of User.Here\u2019s an example of a stored procedure on MySQL with such a signature.",
    "page362": "You use the Hibernate annotations @SQLInsert, @SQLUpdate, and @SQLDelete\u00ae to customize how Hibernate creates, updates, and deletes an entity instance in the database.Instead of a custom SQL statement, you can call a stored procedure to perform theoperation:You have to indicate that Hibernate must execute an operation with a JDBC CallableStatement instead of a PreparedStatement; hence set the callable true option. As explained in section 17.3.2, argument binding for procedure calls is only possible with positional parameters, and you must declare them in the order \u00aeHiber\u00aenateexpects. Your stored procedures must have a matching signature. Here are some procedure examples for MySQL that insert, update, and delete a row in the USERS table:When a stored pro\u00aecedure inserts, updates, or deletes an instance of User, Hibernatehas to know whether the call was successful. Usually, for dynamically generated SQL,Hibernate looks at the number of updated rows returned from an operation. If youenabled versioning (see section 11.2.2), and the operation didn\u2019t or couldn\u2019t updateany rows, an optimistic locking failure occurs.If you write your own SQL, you can customize this behavior as well. It\u2019s up to the stored procedure \u00aeto perform the versioncheck agai\u00aenst the database state when updating or deleting rows. With the checkoption in your annotations, you can let Hibernate know how the procedure will implement this requirement.",
    "page363": "The default is ResultCheckStyle.NONE, and the following settings are available: NONE The procedure will throw an exception if the operation fails. Hibernatedoesn\u2019t perform any explicit checks but instead relies on the procedure code todo the right thing. If you enable versioning, your procedure must compare/increment versions and throw an exceptio\u00aen if it detects a version mismatch. COUNT The procedure will perform any required version increments andchecks and return the number of updated rows to Hibernate as an updatecount. Hibernate uses CallableStatement#getUpdateCount() to access theresult. PARAM The procedure will perform any required version increments andchecks and return the number of updated rows to Hibernate in its first outputparameter. For this check style, you need to add an additional question mark toyour call and, in your stored procedure, return the row count of your DMLoperation in this (first) output parameter. Hibernate automatically registers theparameter and reads its value when the call completes.Finally, remem\u00aeber that stored procedures and functions sometimes can\u2019t be mappedin \u00aeHibernate. In such cases, you have to fall back to plain JDBC. Sometimes you canwrap a legacy stored procedure with another stored procedure that has the parameterinterface Hibernate expects.You saw how to fall back to the JDBC API when necessary. Even for custom SQLqueries, Hibernate can do the h\u00aeeavy lifting and transform the ResultSet intoinstances of your domain model classes\u00ae, with flexible mapping options, includi\u00aeng customizing result mapping. You can also externalize native queries for acleaner setup. We discussed how to override and provide your own SQL statements for regularcreate, read, update, and delete (CRUD) operations, as well as for collectionoperations.",
    "page364": "Most JPA developers build client/server applications with a Java-based server accessing the database tier through Hibernate. Knowing how the EntityManager and system transactions work, you could probably come up with your own serverarchitecture. You\u2019d have to figure out where to create the EntityManager, whenand how to close it, and how to set transaction boundaries. You may be wondering what the relationship is between requests and responsesfrom and to your client, and the persistence context and transactions on the server.Should a single system transaction handle each client request? Can several consecutive requests hold a persis\u00aetence context open? How does detached entity state fitinto this picture? Can you and should you serialize entity data between client andserver? How will these decisions affect\u00ae your client design?Before we start answering these questions, we have to mention that we won\u2019t talkabout any specific frameworks besides \u00aeJPA and EJB in this chapter. There are severalreasons the code examples use EJBs in addition to JPA: Our goal is to focus on client/server design patterns with JPA. Many cross-cutting concerns, such as data serialization between client and server, are standardized in EJB, so we don\u2019t have to solve every problem immediately. We know youpro\u00aebably won\u2019t write an EJB client application. With the example EJB cli\u00aeentcode i\u00aen this chapter, though, you\u2019ll have the found\u00aeation to make informed decisions when choosing and working with a different framework. We\u2019ll discuss custom serialization procedures in the next chapter and explain how to exchangeyour JPA-managed data with any client.",
    "page365": "We can\u2019t possibly cover every combination of client/server frameworks in theJava space. Note that we haven\u2019t even narrowed our scope to web server applications. Of course, web applications are important, so we\u2019ll dedicate the nextchapter to JPA with JSF and JAX-RS. In this chapter, we\u00ae\u2019re concerned with any client/server system relying on JPA for persistence, and abstractions like \u00aethe DAOpattern, which are useful no matter what frameworks you use. EJBs are effective even if you only use them on the server side. They offer transaction management, and you can bind the persistence context to stateful session beans. We\u2019ll discuss these detail\u00aes as well, so if your application architecturecalls for EJBs on the server side, you\u2019ll know how to build them.Throughout this chapter, you implement two s\u00aeimple use cases with\u00ae straightforwardworkflows as an actual working application: editing an auction item, and placing bidsfor an item. First we look at the persistence layer and how you can encapsulate JPAoperations into reusable components: in particular, using the DAO pattern. This willgive you a solid foundation to build more application functionality. Then you implement \u00aethe use cases as conversations: unit\u00aes of work from the perspective of your application users. You see the code for stateless and stateful server-side components and the impact this has on client design and overall application architecture.This affects not only the behavior of your application but also its scalability and robustness. We repeat all the examples with both strategies and highlight the differences. Let\u2019s start with fleshing out a persistence layer and the DAO pattern.",
    "page366": "In section 3.1.1, we introduced the encapsulation of persistence code in a separatelayer. Although JPA already provides a certain level of abstraction, there are severalreasons you should consider hiding JPA calls behind a facade: A custom persistence layer can provide a higher level of abstraction for dataaccess operations. Instead of basic CRUD and query operations as exposed bythe EntityManager, you can expose higher-level operations, such as getMaximumBid(Item i) and findItems(User soldBy) methods. This abstractionis the primary reason to create a persistence layer in larger applications: to support reuse of the same data-access operations. The persistence layer can have a generic interface without exposing implementation details. In other words, you can hide the fact that you\u2019re using Hibernate(or Java Persistence) to implement the data-access operations from any client ofthe per\u00aesistence layer. We consider persistence-layer portability an unimportantconcern because full object/relational mapping solutions like Hibernatealr\u00aeeady provide database portability. It\u2019s highly unlikely that you\u2019ll rewrite yourpersistence layer with different software in the future and still not want tochange any client code. Furthermore, Java Persistence is a standardized andfully portable API; there is little harm in occasionally exposing it to clients of thepersistence layer.The persistence layer can unify data-access operations. This concern relates to portability, but from a slightly different angle. Imagine that you have to deal with mixeddata-access code, such as JPA and JDBC operations. By unifying the facade that clientssee and use, you can hide this implementation detail from th\u00aee client. If yo\u00aeu ha\u00aeve todeal with different types of data stores, this is a valid reason to write a persistence layer.",
    "page367": "The persistence layer can uni\u00aefy data-access operations. This concern relates to portability, but from a slightly different angle. Imagine that \u00aeyou have to deal with mixeddata-access code, such as JPA and JDBC operations. By unifying the facade that clientssee and use, you can hide this implementation detail from the client. If you have todeal with different types of data stores, this is a valid reason to write a persistence layer. If you consider portab\u00aeility and unification to be side effects of creating a persistence layer, your primary motivation is achieving a higher level of abstraction an\u00aedimprove the maintainability and reuse of data-access code. These are good reasons,and we encourage you to create a persistence layer with a generic facade in all but thesimplest applications. But always first consider using JPA directly without any additional layering. Keep it as simple as possible, and create a lean persistence layer on topof JPA when you realize you\u2019re duplicating the same query and persistence operations. Many tools available claim to simplify creating a persistence layer for JPA or Hibernate. We recommend that you try to work without such tools first, and only buy into aproduct when you need a particular feature. Be e\u00aespecially wary of code and query generators: the frequently heard claims of a holistic solution to every problem, in the longterm, can become a significant restriction and maintenance burden. There can alsobe a huge impact on productivity if the development process depends on running acode-generation tool. This is of course also true for Hibernate\u2019s own tools: for example, if you have to generate the entity class source from an SQL schema every time youmake a change.",
    "page368": "The persistence \u00aelayer is an important part of your application, andyou must be aware of the commitment you\u2019re ma\u00aeking by introducing additionaldependencies. You see in this chapter and the next how to avoid the repetitive codeo\u00aeften associated with persistence-layer components without using any additional tools. There is more than one way to design a persistence layer facade some smallapplications have a single DataAccess class; others mix data-access operations intodomain classes (the Active Record pattern, not discussed in this book) but we \u00aepreferthe DAO pattern.The DAO design pattern \u00aeoriginated in Sun\u2019s Java Blueprints more than 15 years ago;it\u2019s had a long history. A DAO class defines an interface to persistence operations relating to a particular entity; it advises you to group together code that relates to the persistence of that entity. Given\u00ae its age, there are many variations of the DAO pattern. Thebasic structure of our recommended design is shown in figure 18.1.We designed the persistence layer with two parallel hierarchies: interfaces on oneside, implementations on the other. The b\u00aeasic instance-storage and -retrieval operations are grouped in a generic super-interface and a superclass that implements theseoperations with a particular persistence solution (using Hibernate, of course). Thegeneric interface is extended by interfaces for particular entities that require additional business-related data-access operations. Again, you may have one or severalimplementations of an entity DAO interface.",
    "page369": "Let\u2019s quickly look at some of the interfaces and methods shown in this illustration.There are a bunch of finder methods. These typically return managed (in persistentstate) entity instances, but they may also return arbitrary data-transfer objects such asItemBidSummary. Finder methods are your biggest code duplication issue; you mayend up with dozens if you don\u2019t plan carefully. The first step is to try to make them asgeneric as possible and move them up in the hierarchy, ideally into the top-level interface. Consider the findByName() method in the ItemDAO: you\u2019ll probably have to addmore options for item searches soon, or you may want the result presorted by the database, or you may implement some kind of paging feature. We\u2019ll elaborate on thisagain later and show you a generic solution for sorting and paging in sect\u00aeion 19.2. The methods offered by the DAO API indicate clearly that this is a state-managingpersistence layer. Methods such as makePersistent() and makeTransient() changean entity instance\u2019s state (or the state of many instances at once, with cascadingenabled). A client can expect that updates are executed automatically (flushed) bythe\u00ae persistence engine when an entity instance is modified (there is \u00aeno performUpdate() method). You\u2019d write a completely different DAO interface if your persis\u00aetence layer were statement-oriented: for example, if you weren\u2019t using Hibernate toimplement it, but rather only plain JDBC.",
    "page370": "The persistence layer facade we introduce here doesn\u2019t expose any Hibernate orJ\u00aeava Persistence interface to the client, so theoretically you can implement it with anysoftware without making ch\u00aeanges to the client code. You may not want or need persistence-layer portability, as explained earlier. In that case, you should consider exposingHibernate or Java Persistence in\u00aeterfaces for example, you could allow c\u00aelients toaccess the JPA CriteriaBuilder and then have a generic findBy(CriteriaQuery)method. This decision is up to you; you may decide that exposing Java Persistenceinterfaces is a safer choice than exposing Hibernate interfaces. You s\u00aehould know, however, that although it\u2019s possible to change the implementation of the persistence layerfrom one JPA provider to another, it\u2019s almost impossible to rewrite a persistence layerthat is state-oriented with plain JDBC statements. Next, you implement th\u00aee DAO interfaces.This generic implementation needs two things to work: an EntityManager and anentity class. A subclass must provide \u00aethe entity class as\u00ae a constructor argument. TheEntityManager, however, can be provided e\u00aeither by a runtime container that understands the @PersistenceContext injection annotation (for example, any standardJava EE container) or through setEntityManager().You can see how the code uses the entity class to perform the query operations. We\u2019vewritten some simple criteria queries, but you could use JPQL or SQL. Finally, here are the stat\u00aee-management operations.",
    "page371": "An important decision is how you implement the makePersistent() method. Herewe\u2019ve chosen EntityManager#merge() because it\u2019s the most versatile. If the givenargument is a transient entity instance, merging will return a persistent instance. If theargument is a \u00aedetached entity instance, merging will also return a persistent instance.This provides clients with a consistent API without worrying about the state of an entityinstance b\u00aeefore calling makePersistent(). But the clien\u00aet needs to be aware that thereturned value of makePersistent() is always the current instance and that the argument i\u00aet has given must now be thrown away  You\u2019ve now comp\u00aeleted building the basic machinery of the persistence layer andthe generic interface it exposes to the upper layer of the system. In the next \u00aestep, youcreate entity-related DAO interfaces and imple\u00aementations by extending the genericinterface and implementation.Everything you\u2019ve created so far is abstract and generic you can\u2019t even instantiateGenericDAOImpl. You now implement the ItemDAO interface by extending GenericDAOImpl with a concrete class. First you must make choices about how callers will access t\u00aehe DAOs. You also needto think about the life cycle of a DAO instance. With the current design, the DAOclasses are stateless except for the EntityManager member. Caller threads can share a DAO instance. In a multithreaded Java EE environment,for example, the automatically injected EntityManager is effectively thread-safe,because internally it\u2019s often implemented as a proxy that delegates to some thread- ortransaction-bound persistence context. Of course, if you call setEntityManager() ona DAO, that instance can\u2019t be shared and should only be used by one (for example,integration/unit test) thread.",
    "page372": "You shouldn\u2019\u00aet have any problem writing these queries after reading the previous chapters; they\u2019re straightforward: Either use the criteria query APIs or call externalizedJPQL queries by name. You should consider the static metamodel for criteria queries,as explained in the section \u201cUsing a static metamodel\u201d in chapter 3. With the ItemDAO finished, you can move on to BidDAO:As you can see, this is an empty DAO implementation that only inherits generic methods. In the next section, we discuss some operations you could potentially move intothis DAO class. We also haven\u2019t shown any UserDAO or CategoryDAO code and assumethat you\u2019ll write these DAO interfaces and implementations as needed. Our next topic is testing this persistence lay\u00aeer: Should you, and if so,\u00ae how?We\u2019ve pulled almost all the examples in this book so far directly from actual\u00ae test code.We continue to do so in all future examples, but we have to ask: should you write testsfor the persistence layer to validate its functionality? In our experience, it doesn\u2019t usually make sense to test the persistence layer separately. You could instantiate your domain DAO classes and provide a mock Ent\u00aeityManager. Such a unit test would be of limited value and quite a lot of work to write.Instead, we recommend that you create integration tests, which test a larger part of theapplication stack and involve the database system. All the rest of the examples in thischapter are from such integration tests; they simulate a client calling the server application, with an actual database back end.",
    "page373": "Hence, you\u2019re testing what\u2019s important: thecorrect behavior of your services, the business logic of the domain model they rely on,and database access through your DAOs, all together. The problem then is preparing such integration tests. You want to test in a real JavaEE environment, in \u00aethe actual runtime container. For this, we use Arquillian(http://arquillian.org), a tool that integrates with TestNG. With Arquillian, youprepare a virtual archive in your test code and then execute it on a real applicationserver. Look at the examples to s\u00aeee how this work\u00aes. A more interesting prob\u00aelem is preparing test data for integration tests. Most meaningful tests require that some data exists in the database. You want to load that test datainto the database before your test runs, and each test should work with a clean and\u00aewell-defined data set so you can write reliable as\u00aesertions\u00ae. Based on our experience, here are three common techniques to import test data:Your test fixture executes a method before every test to obtain an EntityManager. You manually instantiate your tes\u00aet data entities and persist them withthe EntityManager API. The major advantage of this strategy is that you testquite a few of your mappings as a side effect. Another advantage is easy programmatic access to test data.",
    "page374": "For example, if you need the identifier value of aparticular test Item in your test code, it\u2019s already there in Java because you ca\u00aenpass it back fro\u00aem your data-import method. The disadvantage is that test datacan be hard to m aintain,\u00ae because Java code isn\u2019t a great data format. You canclear test data from the database by dropping and re-creating the schema afterevery test using Hibernate\u2019s sc\u00aehema-export feature. All integration tests in thisbook so far have used this approach; you can find the test data-i\u00aem\u00aeport procedure next to each test in the example co\u00aede. Arquillian can execute a DbUnit (http://dbunit.sourceforge.net) data-setimport before every test run. DbUnit offers several formats for writing data sets,including the commonly used flat XML syntax. This isn\u2019t the most compact format but is easy to read and maintain. The examples in this chapter use thisapproach. You can find Arquillian\u2019s @UsingDataSet on the test classes with apath to the XML file to import. Hibernate generates and drops the SQL schema,and Arquillian, with the help of DbUnit, loads the test data into the database. Ifyou like to keep your test data independent of tests, this may be the rightapproach for you. If you don\u2019t use Arquillian, manually importing a data set ispretty easy with DbUnit see the SampleDataImporter in this chapter\u2019s examples. We deploy this importer when running the example applications duringdevelopment, to have the same data available for interactive use as in automated tests.",
    "page375": "In section 9.1.1, you saw how to\u00ae execute custom SQL scripts when Hibernatestarts. The load scr\u00aeipt executes after Hibernate generates\u00ae t\u00aehe schema; this is agreat utility for importing test data with plain INSERT SQL statements. Theexamples in the next chapter use this approach. The major advantage is thatyou can copy/paste the INSERT statements from an SQL console into\u00ae your testfixture and vice versa. Furthermore, if your database supports the SQL rowvalue constructor syntax, you can write compact multirow insertion statementslike insert into MY_TABLE (MY_COLUMN) values (1), (2), (3), ....We leave it up to you to pick a\u00ae strategy. This is frequently a matter of taste and howmuch test data you have to maintain. Note that we\u2019re talking about test data forintegration tests, not performance or scalability tests. If you need \u00aelarge amounts of(mostly random) test data for load testing, consider data-generation tools such as Ben\u00aeerato\u00aer (http://databene.org/databene-benerator.html). This completes the first iteration of the persistence layer. You can now obtainItemDAO instances and work with a higher level of abstra\u00aection when accessing the database. Let\u2019s write a client that calls this persistence layer and implement the rest of theapplication.The application will be a stateless server application, which means no applicationstate will be managed on the server between client requests. The application will besimple, in that it supports only two use cases: editing an auction item and placing abid for an item.",
    "page376": "Consider these workflows to be conversations: units of work from the perspective ofthe application user. The point of view of application users isn\u2019t necessarily the samethat we as developers have on the system; developers usually consider one systemtransaction to be a unit of work. We now focus on this mismatch and how a user\u2019s perspective influences the design of server and client code. We start with the first conversation: editing an item.The client is a trivial text-based EJB console application. Look at the \u201cedit an auctionitem\u201d user conversation with this client in figure 18.2. The client presents the user with\u00ae a list of auction items; the user picks one. Then theclient asks which operation the user would like to perform. Finally, after entering a newname, the client shows a success confirmation message. The system is now ready for thenext conversation. The example client starts again and presents a list of auction items.The sequence of calls for this conversation is shown in figure 18.3. This is your roadmap for the rest of the section. Let\u2019s have a closer \u00aelook at this in code; you can refer to the bullet items in the illustration to keep track of where we are. The code you see next is from a test case simulating the client, followed by code from the server-side components handling theseclient calls. The client retrieves a list of Item instances from the server to start the conversation  and also requests with true that the Item#bids collection be eagerly fetched.Because the server doesn\u2019t hold th\u00aee conve\u00aersation state, the client must do this job.",
    "page377": "(You can ignore the interfaces declared here; they\u2019re trivial but necessary for remotecalls and local testing of an EJB.) Because no transaction is active when the client callsgetItems(), a new transaction is started. The transaction is com\u00aemitted automaticallywhen the method returns. The @TransactionAttribute annotation is optional in thiscase; the default behavior requires a transaction on EJB method calls. The getItems() EJB method call\u00aes the ItemDAO to retrieve a List of Item instancesThe Java EE container automatically looks up and injects the ItemDAO, and theEntityManager is set on the DAO. Because no EntityManager or persistence contex\u00aet isassociated with the current transaction, a new persistence context is started andjoined with the transaction. The persistence context is flushed and closed when thetransaction commits. This is a convenient feature of stateless EJBs; you don\u2019t have \u00aetodo much to use JPA in a transaction. A List of Item instances in detached state (after the persistence context is closed)is returned to the client . You don\u2019t have to worry about serialization right now; aslong as List and Item and all other reachable types are Serializable, the EJB framework takes\u00ae care of it. Next, the client\u00ae sets the new name of a selected Item and asks the server to storethat c\u00aehange by sending the detached and modified\u00ae Item.",
    "page378": "The updated state the result of the merge is return\u00aeed to the client. The\u00ae conversation is complete, and the client may ignore the returned updatedItem. But the client knows that this return value is the latest state and that any previous state it was holding during the conversation, such as the List of Item instances, isoutdated and should probably be discarded. A subsequent conversation should beginwith fresh state: using the latest returned Item, or by obtaining a fresh list. You\u2019ve now seen h\u00aeow to implement a single conversation the entire unit of work,from the user\u2019s perspective with two system transactions on the server. Because youonly loaded data in the first system transaction and deferred writing changes to thelast transaction, the conversation was atomic: changes aren\u2019t permanent until the l\u00aeaststep completes successfully. Let\u2019s expand on this with the second use case: placing abid for an item.In the console client, a user\u2019s \u201cplacing a bid\u201d conversation looks like figure 18.4. The client presents the user with a list of auction items again and asks the user to pick one. Theuser can place a bid and receives a success-confirmation messa\u00aege if the bid was storedsuccessfully. The sequence of calls and the code road map are shown in figure 18.5.",
    "page379": "We again step through the test client and server-side code. First , the client gets a listof Item instances and eagerly fetches the Item#bids collection. You saw the code forthis in the previous section. Then, the client creates a new Bid instance after receiving user input for theamount , linking the new transient Bid with the detached selected Item. The clienthas to store the new Bid and send it to the \u00aeserver. If you don\u2019t document y\u00aeour serviceAPI properly, a client may attempt to send the detached Item:Here, \u00aethe client assumes that the server knows it added the new Bid to the Item#bidscollection and that it must be stored. Your server could implement this functionality,maybe with merge cascading enabled on the @OneToMany mapping of that collection.Then, the storeItem() method of the service would work as in the previous section,taking the Item and cal\u00aeling the ItemDAO to make it (and its transitive dependencies)persistent. This isn\u2019t the case in this application: the service offers a separate placeBid()method. You have to perform additional validation before a bid is stored in the database, such as checking whether i\u00aet was higher than the last bid. You also want to forcean increment of the Item v\u00aeersion, to prevent concurrent bids. Hence, you documentthe cascading behavior of your domain model entity associations: Item#bids isn\u2019ttransitive, and new Bid instances must be sto\u00aered through the service\u2019s placeBid()method exclusively.",
    "page380": "Two interesting things are happening here. First, a transaction is started and spans theplaceBid() method call. The nested EJB method calls to ItemDAO and BidDAO are inthat same transa\u00aection context and inherit the transaction. The same is true for thepersistence context: it has the same scope as the transaction . Both DAO classesdeclare that they\u00ae need the current @PersistenceContext injected; the runtime container provides the persistence context bound to the current transaction. Transactionand persistence-context creation and propagation with stateless EJBs is straightforward, always \u201calon\u00aeg with the call.\u201d Second, the validation of the new Bid is business logic encapsulated in the domainmodel classes. The service calls Item#isValid(Bid) and delegates the responsibilityfor validation to the Item dom\u00aeain model class. Here\u2019s how you \u00aeimplement this in theItem class:The isValid() method perfor\u00aems several checks to find out whether the Bid is higherthan the last bid. If your auction system has to support a \u201clowest bid wins\u201d strategy atsome point, all you have to do is change the Item domain-model implementation; theservices and DAOs using that class won\u2019t know the difference. (Obviously, you\u2019d need adifferent message for the InvalidBidException.) What is debatable is th\u00aee\u00ae efficienc\u00aey of the getHighestBid() method. It loads theentire bids collection into memory, sorts it there, and then takes just one Bi\u00aed. Anoptimized variation could look like this.",
    "page381": "The service (o\u00aer controller, if you like) is still completely unaware of any businesslogic it doesn\u2019t need to know whether a new bid must be higher or lower than thelast one. The service implemen\u00aetation must provide the currentHighestBid andcurrentLowestBid when calling the Item#isValid() method. This is what we hintedat earlier: that you may want to add operations to the BidDAO. You could write database queries to find those bids in the most efficient way possible without loading allthe item bids into memory an\u00aed sorting them there.The application is now complete. It supports the two use cases you set out to implement. Let\u2019s take a step back and analyze the resultYou\u2019ve implemented code to support conversations: units of work from the perspective of the users. The users expect to perform a series of steps in a workflow and thateach step will be only temporary until they finalize the conversation with the last step.That last step is usual\u00aely a final request from the client to the server, ending the conversation. This sounds a lot like transactions, but you may have to perform several systemtransactions on the server to complete a particular conversation. The question is ho\u00aewto provide atomicity across several requests and system transactions.",
    "page382": " Conversations by users can be of arbitrary complexity and duration. More thanone client request in a conver\u00aesation\u2019s flow may load detached data. Because you\u2019re incontrol of the detached instances on the client, you can easily make a conversationatomic if you don\u2019t merge, persist, or remove any entity instances on the server untilthe final request in yo\u00aeur conversation workflow. It\u2019s up to you to somehow queue modifications and manage detached data where the list of items is held during user think\u00aetime. Just don\u2019t call any service operation from the client that makes permanentchanges on the server until you\u2019re sure you want to \u201ccommit\u201d the conversation. One issue you have to keep an eye on is equality of\u00ae detached references: for example, if you load several Item instances and put them in a Set or use them as keys in a Map.Because you\u2019re t\u00aehen comparing instances outside the guaranteed scope of object identity the persist\u00aeence context you must override the equals() and hashCode() methods on the Item entity class as explained in section 10.3.1. In the trivial conversationswith only one detached list of Item instances, this wasn\u2019t nece\u00aessary. You nev\u00aeer comparedthem in a Set, used them as keys in a HashMap, or tested them explicitly f\u00aeor equality. You should enable versioning of the Item entity for multiuser applications, asexplaine\u00aed in the section \u201cEnabling versioning\u201d in chapter 11. When entity modifications are merged in AuctionService#storeItem(), Hibernate increments the Item\u2019sversion (it doesn\u2019t if the Item wasn\u2019t modified, though).",
    "page383": " Hence, if another user haschanged the name of an Item concurrently, Hibernate will throw an exception whenthe system transaction is committed and the persistence context is flushed. The first\u00aeuser to commit their conversation always wins with this optimistic strategy. The seconduser should be shown the usual error message: \u201cSorry, someone else modified thesame data; please restart your conversation.\u201d What you\u2019ve created is a system with a rich client or thick client; the client isn\u2019t a dumbinput/output terminal but an application with an internal state independent of theserver (recall that the server doesn\u2019t \u00aehold any application state). One of the advantagesof such a stateless se\u00aerver is that any server can handle any client request. If a server fails,you can route the next request to a different server, and the conversation process cont\u00aeinues. The servers in a cluster share nothing; you can easily scale up your system horizontally by adding more servers. Obviously, all application servers still share thedatabase system, but at least you only have to worry about scaling up one tier of servers.The downside is that you need to write rich-client applic\u00aeations, and you have to dealwith network communication and data-serialization issues. Complexity shifts from theserver side to the client side, and you have to optimize communication between theclient and server.",
    "page384": " If, instead of on an EJB client, your (JavaScript) client has to work on several webbrowsers or even as a native application on different (mobile) operating systems, thiscan certainly be a challenge. We recommend this architecture if your rich client runsin popular web browsers, where users download the latest version of the client application every time they visit your website. Rolling out n\u00aeative clients on several platforms,and maintaining and upgrading the installations, can be a significant burden even inmedium-sized intranets where you control the user\u2019s environment. Without an EJB environment, you have to customize serialization and transmissiono\u00aef detached entity state between the client and the server. Can you serialize and deserialize an Item instance? What happens if you didn\u2019t write your client in Java? We\u2019lllook at this issue in section 19.4. Next, you implement the same use cases again, but with a\u00ae very different strategy.The server will now hold the conversational state of the application, and the client willonly be a dumb input/output device. This is an architecture with a thin client and astateful server.The application you\u2019ll write is still simple. It supports the same two uses cases as before:editing an auction item and placing a bid for an item. No difference is visible to theusers of the application; the EJB console client still looks like figures 18.2 and 18.4.",
    "page385": " With a thin client, it\u2019s the server\u2019s job to transform data for output into a display format understood by the thin client for example, into HTML pages rendered by a webbrowser. The client transmits user input operations directly to the server for example, as simple HTML\u00ae form submissions. The server is responsible for decoding andtransforming the input into higher-level domain model operations. We keep this partsimple for now, though, and use only remote method invocation wi\u00aeth an EJB client. The server must then also hold conversational data, usually stored in some kind ofserver-side session associated with a particular client. Note that a client\u2019s session has alarger scope than a single conversation; a user may perform several conversations during a session. If the user walks away from the client and doesn\u2019t complete a conversation, temporary conversation data must be removed on the server at some point. Theserver typically handles this situation with timeouts; for example, the server may discard a client\u2019s session and all the data it contains after a certain period of inactivity.This sounds like a job for EJB stateful session beans, and, indeed, they\u2019re ideal for thiskind of architecture if you\u2019re in need of a standardized solution. Keeping those fundamental issues in mind, let\u2019\u00aes implement the first use case: editing an auction item.",
    "page386": "The client presents the user again with a list of auction items, and the user picks one.This part of the application is trivial, and you don\u2019t have to hold any conversationalstate on the server. Have a look at the sequence of calls and contexts in figure 18.6.Even if you have a stateful server architecture, there will be many short conversationsin your application that don\u2019t\u00ae require any state to be held on the s\u00aeerver. This is bothnormal and important: holding state on the server consumes resources. If you implemented the getSummarie\u00aes() operation with a stateful session bean, you\u2019d wasteresources. You\u2019d only use the stateful bean for a single operation, and then it wouldconsume memory until the container expired it. Stateful server architecture doesn\u2019tmean you can only use stateful server-side components. Next, the client renders the ItemBidSummary list, which only contains the identifier of each auction item, its description, and the current highest bid. This is exactlywhat the user sees on the screen, as shown in figure 18.2. The user then enters an itemidentifier and starts a conversation that works with this item. You can see the\u00ae road mapfor this conversation in figure 18.7.The service called here isn\u2019t the pooled stateless AuctionService from the last section. This new ItemService is a stateful component; the server will create an instanceand assign it to this client exclusively. You implement this service with a stateful session bean.",
    "page387": "There are many annotations on this class, defining how the container will handle thisstateful component. With a 10-minute timeout, the server removes and destroys aninstance of this component if the client hasn\u2019t called it in the last 10 minutes. Thishandles dangling conversations: for example, when a user walks away from the client. You also disable passivation for this EJB: an EJB container may serialize and storethe stateful component to disk, to\u00ae preserve memory or to transmit it to another nodein a cluster when session-failover is needed. This passivation won\u2019t work because ofone member field: the EntityManager. You\u2019re attaching a persistence context to thisstateful component with the EXTENDED switch, and the EntityManager isn\u2019tjava.io.Serializable.You use @PersistenceContext to declare that this stateful bean needs an EntityManager and that the container should extend the persistence context to span the sameduration as the life cycle of the stateful session bean. This extended mode is an op\u00aetionexclusively for stateful EJBs. Without it, the container will create and close a persistence context whenever a transaction commits. Here, you want the persistence context to stay open beyond any transaction boundaries and remain attached to thestateful session bean instance.Furthermore, you don\u2019t want the persistence context to be flushed automaticallywhen a transaction commits, so you configure it to be UNSYNCHRONIZED. Hibernate willonly flush the persistence context after you manually join it with a transaction. NowHibernate won\u2019t automatically write to the database changes you make to loaded persistent enti\u00aety instances; instead, you queue them until you\u2019re ready\u00ae to write everything.",
    "page388": "At the start of the conversation, the server loads an Item instance and holds it asconversational state in a member field  (figure 18.7). The ItemDA\u00aeO also needs anEntityManager; remember that it has a @PersistenceContext annotation without anyoptions. The rules for persistence context propagation in EJB calls you\u2019ve seen beforestill apply: Hibernate propagates the persistence context along with the transactioncontext. The persistence context rides along into the ItemDAO with the transactionthat was started for the startConversation() method call. When startConversation() returns, the transaction is committed, but the persistence context is neitherflushed nor closed. The ItemServiceImpl instance waits on the server for the nextcall from the client. The next call from the client instructs the server to change the item\u2019s nameOn the server, a transaction is started for the setItemName() method. But because notransactional resources are involved (no DAO calls, no EntityManager calls), nothinghappens but a change to the Item you hold in conversational state:Note that the Item is still be in persistent state the persistenc\u00aee context is still open!But because it isn\u2019t synchronized, it won\u2019t detect the change you made to the Item,because it won\u2019t be flushe\u00aed when the transaction commits. Finally,\u00ae the client ends the conversation, giving the OK to store all changes on theserver On the server, you can flush changes to the database and discard the conversationalstate.",
    "page389": "You\u2019ve now completed the implementation of the first use case. We\u2019ll skip the implementation of the second use case (placing a bid) and refer you to the example codefor details. The code for the second case is almost the same as the first, and youshouldn\u2019t have any problems understanding it. The important aspect you must understand is how persistence context and transaction handling work in EJBs.As in our analysis of the stateless application, the first question is how the unit of workis implemented from the perspective of the user. In particular, you need to ask howatomicity of the conversation works and how you can make all steps in the workflowappear as a single u\u00aenit. At some point, usually when the last request in a conversation occurs, you committhe conversation and write the changes to the database. The conversati\u00aeon is atomic ifyou don\u2019t join the extended EntityManager with a transaction until the last event inthe conversation. No dirty checkin\u00aeg and flushing will occur if you only read data inunsynchronized mode. While you keep the persistence context open, you can keep loading data lazily byacce\u00aessing proxies and unloaded collections; this is obviously convenient. The loadedItem and other data become stale, however, if the user needs a long time to trigger thenext request.",
    "page390": "You may want to refresh() some managed entity instances during theconversation if you need updates from the database, as explained in section 10.2.6.Alternatively, you can refresh to undo an operation during a conversation. For example, if the user changes the Item#name in a dialog but then decides to undo this, you\u00aecan refresh() the persistent Item instance to retrieve the \u201cold\u201d name from the database. This is a nice feature of an extended persistence context and allows the Item tobe always available in managed persistent state.The stateful s\u00aeerver architecture may be more difficult to scale horizontally. If a serverfails, the state of the current conversation and indeed the entire session is lost. Replicating sessions on several servers is a costly operation, because any modification of session data on one server involves network communication to (potentially all) otherservers. With stateful EJBs and a member-extended EntityManager, serialization of thisextended persistence context isn\u2019t possible. If you use stateful EJBs and an extendedpersistence context in a cluster, consider enabling sticky sessions, causing a particularclient\u2019s requests to always route to the same server. This allows you to handle moreload with additional servers easily, but your users must accept losing session state whena server fails. On the other hand, stateful servers can act as a first line of caches with theirextended persistence contexts in user sessions. Once an Item has been loaded for aparticular user conversation, that Item won\u2019t be loaded again from the database in thesame conversation. This can be a great tool to reduce the load on your database serve\u00aers (the most expensive tier \u00aeto scale).",
    "page391": " An extended persistence-context strategy requires more memory on the serverthan holding only detached instances: the persistence context in Hibernate contains asnapshot copy of all managed instances. You may want to manually detach() managedinstances to cont\u00aerol what is held in the persistence context, or disable dirty checkingand snapshots (while still being able to lazy load) as explained in section 10.2.8. There are alternative implementations of thin clients and stateful serve\u00aers, ofcourse. You can use regular request-scoped persistence contexts and manage detached(not persistent) entity instances on the serve\u00aer manually. This is certainly possible withdetaching and merging but can be much more work. One of the main advantages ofthe extended persistence context, transparent lazy loading even across requests, wouldno longer be available either. In the next chapter, we\u2019ll show you such a state\u00aeful serviceimplementation with request-\u00aescoped persistence contexts in CDI and JSF, and you cancompare it with the extended persistence context feature of EJBs you\u2019ve seen in thischapter. Thin client systems typically produce more load on servers than rich clients do.Every time the user interacts with the application, a client event results in a networkrequest. This can even happen for every mouse click in a web application. Only theserver knows the state of the current conversation and has to prepare and render allinformation the user is viewing. A rich client, on the other hand, can load raw dataneeded for a conversation in one request, transform it, and bind it locally to the userinterface as needed. A dialog in a rich client can queue modifications on the clientside and fire a network request only when it has to make changes persistent at the endof a conversation.",
    "page392": "An additional challenge with thin clients is parallel conversations by one user: whathappens if a user is editing two items at the same time for exa\u00aemple, in two webbrowser tabs? This means the user has two parallel conversations with the server. Theserver must separate data in the user session by conversation. Client requests during aconversation must therefore contain some sort of conversation\u00ae identifier so you canselect the correct conversation sta\u00aete from the use\u00aer\u2019s session for each request. This happens automatically with EJB clients and servers but probably isn\u2019t built into your favorite web application framework (unless it\u2019s JSF and CDI, as you\u2019ll see in the nextchapter). One significant benefit of a stateful server is less reli\u00aeance on the client platform; ifthe client is a simple input/output terminal with few moving parts, there is less chancefor things to go wrong. The only place you have to implement data validation andsecurity checks is the server. There are no deployment issues to deal with; you can rollout application upgrades on servers without touching clients. Today, there are few advantage\u00aes to thin client systems, and stateful server installations are declining. This is especially true in the web application sector, where easyscalability is frequently a major concern.You implemented simple conversations units of work, from the perspective ofyour application user. You saw two server and client designs, with stateless and stateful servers, andlearned how Hibernate fits into both these architectures. You can work with either detac\u00aehed entity state or an extended conversationscoped persistence context.",
    "page393": "There are dozens of web application frameworks for Java, so we apologize ifwe don\u2019t cover your favorite combination here. We discuss JPA in the standard JavaEnterprise Edition environment, in particular combined with standards Contextsand Dependency Injection (CDI), JavaServer Faces (JSF), and Java API for RESTfulweb services (JAX-RS). As always, we \u00aeshow patterns you can apply in other proprietary e\u00aenvironments. First we revisit the persistence layer and introduce CDI management for theDAO classes. Then we extend these classes with a generic solution for sorting andpaging data. This solution is useful whenever you have to display data in tables, nomatter what framework you choose. Next, you write a fully functional JSF application on top of the persistence layerand look at the Java EE conversation scope\u00ae, where CDI and JSF work together toprovide a simple stateful mod\u00aeel for server-side components. If you didn\u2019t like the stateful EJBs with the extended persistence context in the last chapter, maybe these conversation examples with detached entity state on the server are what you\u2019re looking for. Finally, if you prefer writing web applications with rich clients, a stateless server,and frameworks such as JAX-RS, GWT, or AngularJS, we show you how to customizeserialization of JPA entity instances into XML and JSON formats. We start with migrating the persistence layer from EJB components to CDI.",
    "page394": "The CDI standard offers a type-safe dependency injection and component life-cyclemanagement system \u00aein a Java EE runtime environment. You saw the Inject annotation in the previous chapter and used i\u00aet to wire the ItemDAO and BidDAO componentstogether with the service EJB classes. The JPA @PersistenceContext annotation you used inside the DAO classes is justanother special injection case: you tell the runtime container to provide and automatically handle an EntityManager instance. This is a container-managed EntityManager.There are some strings attached, though,\u00ae such as the persistence-c\u00aeontext propagationand transaction rules we discussed in the previous chapter. \u00aeSuch rules are convenientwhen all of your service and DAO classes are EJBs, but if you don\u2019t employ EJBs, you maynot want to follow these rules. With an application-managed EntityManager, you can create your own persistence context management, propagation, and injection rules. \u00aeYou now rewrite the DAO classes as simple CDI managed beans, which are just likeEJBs: plain Java classes with extra annotations. You want to Inject an EntityManagerand drop the PersistenceContext annotation, and thus have full control over thepersistence context. Before you can inject your own EntityManager, you must produce it.A producer in CDI parlance is a factory used to customize creation of an instance andtell the runtime container to call a custom routine whenever the application needs aninstance based on the declared scope. For example, the container will create an application-scoped instance only once during the life cycle of\u00ae the application.",
    "page395": "The container creates a request-scoped instance once for every request handled by a serverand a session-scoped instance once for every session a user has with a server. The CDI specific\u00aeation maps the abstract notions of request and session to servletrequests and sessions. Remember that both JSF and JAX-RS build on top of servlets, soCDI works well with those frameworks. In other words, don\u2019t worry much about this: ina Java EE environment, all the integration work has already been done for you. Let\u2019s create a producer of request-scoped EntityManager instances:This CDI annotation declares that only one producer is needed in the entire application: there will only ever be one instance of EntityManagerProducer. The Java EE runtime gives you the persistence unit configured in persistence.xml,whi\u00aech is also an application-scoped component. (If you use CDI standalone and outsidea Java EE environment, you can instead use the static Persistence.createEntityManagerFactory() bootstrap.) Whenever an EntityManager is needed, create() is called. The container reuses thesame EntityManager during a request handled by your server. (If you forget@RequestScoped on the method, the EntityManager will be application-scoped likethe producer class!)E When a request is over and the request co\u00aentext is being destroyed, the CDI containercalls this method to get rid of an EntityManager instance. You created this applicationmanaged persistence context (see section 10.1.2), so it\u2019s your job to close it.",
    "page396": "A common issue with CDI-annotated classes is mistaken imports of annotations. InJava EE 7, there are two annot\u00aeations called @Produces; the other one is in javax.ws.rs (JAX-RS). This isn\u2019t the same as the CDI producer annotation and you can spendhours \u00aelooking for this error in your code if you pick the wrong one. Another duplicateis @RequestScoped, also in javax.faces.bean (JSF). Like all other outdated JSF beanmanagement annotations in the javax.faces.bean package, don\u2019t use them \u00aewhenyou have the more modern CDI available. We hope that future\u00ae Java EE specificationversions will resolve these ambiguities. You now have a producer of application-managed EntityManagers and requestscoped persistence contexts. Next, you must f\u00aeind a way to let the EntityManager knowabout your syst\u00aeem transactions.When your server must handle a servlet request, the container creates the EntityManager automatically when it first needs it for injection. Remember that an EntityManager you manually create will only join a system transaction automatically if thetransaction is already in progress. Otherwise, it will be unsynchronized: you\u2019ll read datain auto-commit mode, and Hibernate won\u2019t flush the \u00aepersistence context. It\u2019s not always obvious when the container will call the EntityManager producer orexactly when dur\u00aeing a request the first EntityManager injection takes place. Ofcourse, if you process a request without a system transaction, the EntityManager youget is always unsynchronized. He\u00aence, you must ensure that the EntityManager knowsyou hav\u00aee a system transaction.",
    "page397": "You should call this method on any DAO before storing data, when you\u2019re sure you\u2019rein a transaction. If you forget, Hibernate will throw a TransactionRequiredExceptionwhen you try to write data, indicating that the EntityManager has been created beforea transaction was started and that nobody told it about the transaction. If you want toexercise your CDI skills, you could try to implement this aspect with a CDI decorator orinterceptor. Let\u2019s implement this GenericDAO interface method and wire the new EntityManager to the DAO classes.The old GenericDAOImpl relies on the @PersistenceContext annotation for injectionof an EntityManager in a field, or someone calling setEntityManager() before theDAO is used. With CDI, you can use the safer constructor-injection technique:Anyone who wants to instantiate a DAO must provide an EntityManager. This declaration of an invariant of the class is a much stro\u00aenger guarantee; hence, although we frequently use field injec\u00aetion in our examples, you should always first think aboutconstructor injection. (We don\u2019t do it in some examples because they would be evenlonger, and this book is already quite big.) In the concrete (entity DAO) subclasses, declare the necessary injection on theconstructor:When your application needs an ItemDAO, the CDI runtime will call your EntityManagerProducer and then call the ItemDAOImpl constructor. The container will reusethe same EntityManager for any injection in any DAO during a particular request. What scope then is ItemDAO? Because you don\u2019t declare a scope for the implementation class, i\u00aet\u2019s dependent.",
    "page398": "An ItemDAO is created whenever someone needs it, and theItemDAO instance is then in the same context and scope as its caller and belongs to thatcalling object. This is a good choice for the per\u00aesistence layer API, because you delegatescoping decisions to th\u00aee upper layer with the services calling the persistence layer. You\u2019re now ready to @Inject an ItemDAO field in a service class. Before you use yourCDI-enabled persistence layer, let\u2019s add some functionality for paging and sorting data.A very common task is loading data from the database with a query and then displaying that data on a web page in a table. Frequently you must also implement dynamicpaging and sorting of the data: Because the query returns much more information than can be displayed on asingle page, you only show a subset of the data. You only render a certain number of rows in a data table and give users the options to go to the next, previous,first, or last page of rows. Users also expect the application to preserve sortingwhen they switch pages. Users want to be able to click a column header in the table and sort the rows ofthe table by the values of this column. Typically, you can sort in either ascen\u00aedingor descending order; this can be switched by subsequent clicks the columnheader.You now implement a generic solution for browsing through pages of data, based onthe metamodel of persistent classes provided by JPA. Page bro\u00aewsing can be implemented in two variations: using the offs\u00aeet or the seektechnique. Let\u2019s look first at the differences and what you want to implement.",
    "page399": "Figu\u00aere 19.1 shows an example of a browsing UI with an offset-based paging mechanism. You see that there\u2019s a handful of auction items and that a small page size ofthree records is used. Here, you\u2019re on the first page; the application renders links tothe other pages dynamically. The current sort order is by ascending item name. Youcan click the c\u00aeolumn header and change the sorting to descending (or ascendi\u00aeng) byname, auction end date, or highest bid amount. Clicking the item name in each tablerow opens the item detail view, where you can make bids f\u00aeor an item. We take on thisuse case later in this chapter.Behind this page are database queries with an off\u00aeset and a limit condition, based onrow numbers. The Java Persistence API for this are Query#setFirstResult() andQuery#setMaxResults(), which we discussed in section 14.2.4. You write a query andthen let Hibernate wrap the offset and limit clauses around it, depending on yourdatabase SQL dialect. Now consider the alternative: paging with a seek method, as shown in figure 19.2.Here you don\u2019t offer users the option to jump to any page by offset; you only allowthem to seek forward, to the\u00ae next page. This may seem restrictive, but you\u2019ve probablyseen or even implemented such a paging routi\u00aene when you needed infinite scrolling.You can, for example, automatically load and display the next page of data when theuser reaches \u00aethe bottom of the table/screen.",
    "page400": "The seek method relies on a special additional restriction in the query retrieving thedata. When the next page h\u00aeas to be loaded, you query for all items with a name\u201cgreater than [Coffee Machine]\u201d. You seek forward not by offset of result rows withsetFirstResult(), but by restricting the result based on the ordered values of somekey. If you\u2019re unfamiliar with seek paging (sometimes called keyset paging), we\u2019re sureyou won\u2019t find this difficult once you see the queries later in this section. Let\u2019s compare the advantages and disadvantages of both techniques. You can ofcourse implement an endless-scrolling feature with offset paging or direct-page navigation with the seek technique; but they each have their strength and weaknesses: The offset method is great when users want to jump to pages directly. For example\u00ae, many search engines offer the option to jump directly to page 42 of a queryresult or directly to the last page. Because you can easily calculate the offset andlimit of a range of rows based on the desired page number, you can implementthis with little effort.\u00ae With the seek method, providing such a page-jumping UIis more difficult; you must know the value to seek. You don\u2019t \u00aeknow what itemname the client displayed just before page 42, so you can\u2019t seek forward toitems with names \u201cgreater than X.\u201d Seeking is best suited for a UI where usersonly move forward or backward page by page through a data list or table, and ifyou can easily remember the last value shown to the user. A great use case for seeking is paging based on anchor values that you don\u2019thave to remember. For example, all customers whose names start with C could.",
    "page401": "be on one page and those starting with D on the next page. Alternatively, eachpage shows auction items that have reached a certain threshold value for theirhighest bid \u00aeamount. The offset method performs much worse if yo\u00aeu fetch higher page numbers. Ifyou jump to page 5,000, the database must count all the rows and prepare 5,000pages of data before it c\u00aean skip 4,999 of them to give you the result. A co\u00aemmonworkaround is to restrict\u00ae how far a user can jump: for example, only allowingdirect jumps to the first 100 pages and forcing the user to refine the queryrestrictions to get a smaller result. The seek method is usually faster than the offset method, even on low page numbers. The database query optimizer can skipdirectly to the start of the desired page and efficiently limit the index range toscan. Records shown on previous pages don\u2019t have to be considered or counted. The offset method may sometimes show incorrect results. Although the resultmay be consistent with what\u2019s in the database, your users may con\u00aesider it incorrect. When applications insert new records or delete existing records while theuser is browsing, anomalies may occur. Imagine the user looking at pag\u00aee 1 andsome\u00aeone adding a new record that would appear on page 1. If the us\u00aeer nowretrieves page 2, some record they may have seen on page 1 is pushed forwardonto page 2. If a record on page 1 was deleted, the user may miss data on page2 because a record was pulled back to page 1.",
    "page402": "The seek method avoids theseanomalies; records don\u2019t mysteriously reappear or vanish.We now show you how to implement both paging techniques by extending the persistence layer. You start with a simple model that \u00aeholds the current paging and sortingsettings of a data table viewWhen you want to coordinate querying and rendering data pages, you need to keepsome details about the size of the pages and what page you\u2019re currently viewing. Following is a simpl\u00aee model class encapsulating this information; it\u2019s an abstra\u00aect superclass that will work for both the offse\u00aet and seek techniques:The model holds the size of each page and the number of records shown per page.The value -1 is special, meaning \u201cno limit; show all records.\u201dKeeping the number of total records is necessary for some calculations: for example,to determi\u00aene whether there is a \u201cnext\u201d page. Paging always requires a deterministic record order. Typically, you sort \u00aeby a particularattribute of your entity classes in ascending or descending order. javax.persistence.metamodel.SingularAttribute is an attribute of either an entity\u00ae or an embeddableclass in JPA; it\u2019s not a collection (you can\u2019t \u201corder by collection\u201d in a query).The allowedAttributes list is set when creating the page model. It restricts the possible sortable attributes to the ones you can handle in your queries.Some methods of the Page class that we h\u00aeaven\u2019t shown are trivial mostly getters andsetters. The abstract createQuery() method, however, is what subclasses must implement: it\u2019s how paging settings are applied to a CriteriaQuery before the query isexecuted.",
    "page403": " For offset-based paging, you need to know which page you\u2019re on. By default, you startw\u00aeith page 1. Test whether the sorting attribute of this page can be resolved against the attributepath and therefore the model used by the query. The method throws an exception ifthe sorting attribute of the page wasn\u2019t available on the model class referenced in thequery. This is a safety mechanism that produ\u00aeces a meaningful error message if youpair the wrong paging settings with the wrong query. Add an ORDER BY clause to the query. Set the offset of the query: the starting result row.\u00ae Cut the result off with the desired page size.We haven\u2019t shown all the methods involved here, such as getRangeStartInteger(),which calculates the number of the first row that must be retrieved for the currentpage based on page size. You can find other simple convenience methods in thesource code. Note that the result order may not be deterministic: if you sort by ascending itemname, and several items have the same name, the database will return them in whatever order the DBMS creators deemed appropriate. You should either sort by a uniquekey attribute or add an additional order criterion with a key attribute. Although manydevelopers get away with ignoring deterministic sorting problems with the offsetmethod, a predictable record order is mandatory for the seek strategy.",
    "page404": "For seek paging, you need to add restrictions to a query. Let\u2019s assume that the previous page showed all items sorted by ascending name up to Coffee Machine, as shownin figure 19.2, and that you want to re\u00aetrieve the next page with an SQL query. Youremembered the last value of the previous page, the Coffee Machine record and itsidentifier value (let\u2019s say 5), so you can w\u00aerite in SQL:The first restriction of the query says, \u201cGive me all items with a name greater than orequal to [Coffee Machine],\u201d which seeks forward to the end of the previous page. Thedatabase may perform this efficient restriction with an index scan. Then you furtherrestrict the result by saying you want no records named Coffee Machine, thus skippingthe last record you\u2019ve already shown on the previous page.But there may be two items named Coffee Machine in the database. A unique keyis necessary to prevent items from falling through the cracks between pages. You mustorder \u00aeby and restrict the result with that unique key. Here you u\u00aese the primary key,thus guaranteeing that the database includes only items not named Coffee Machine or(even if named Coffee Machine) with a greater identifier value than the one youshowed on the previous page.Of course, if the item name (or any other col\u00aeumn you sort by) is unique, you won\u2019tneed an additional unique key. The generic example code assumes that you alwaysprovide an explicit unique key attribute. Also note that the fact that item identifiersare incremented numerical values isn\u2019t important; the important aspect is that this keyallows deterministic sorting.",
    "page405": "This kind of restriction expression works even in JPQL in Hibernate. JPA doesn\u2019t standardized this, though; it\u2019s unavailable with the criteria API and not supported by alldatabase products. We prefer the slightly m\u00aeore verbose variation\u00ae, which works everywhere. If you sort in descending order, you invert the comparisons from greater than toless thanIn addition to the regular sorting attribute, the seek technique requires a paging attribute that\u2019s a guaranteed unique key. This can be any unique at\u00aetribute of your entitymodel, but it\u2019s usually the primary key attribute. For both the sorting attribute and the unique key attribute, you must remember theirvalues from the \u201clast page.\u201d You can then retrieve the next page by seeking those values. Any Comparable value is fine, as required by the restriction API in criteria queries. You must always sort the result by both the sorting attribute and the unique keyattribute. Add any necessary additional restrictions (not shown) to the where clause of thequery, seeking beyond the last known values to the target page.Cut off the result with the desired page size.The full applySeekRestriction() method can be found in the example code; this iscriteria query code we don\u2019t have enough space for here. The final query is equivalentto the SQL example you saw earlier. Let\u2019s test the new paging feature of the persistence layer.",
    "page406": "When you call the ItemDAO#getItemBidSummaries() method now, you must provide aPage instance. A serv\u00aeice or UI layer client on top of the persistence layer executes thefollowing code:With the offset strategy, all you h\u00aead to know to jump to a page was the page number.With the seek strategy, you must remember the last values shown on the previous page:This is clearly more work for a client of the persistence layer. Instead of a simple number, it must be able to remember the last values dynamically, depending on the sortand the unique attribute. We implemented the UIs for earlier screenshots with JSF and CDI, including thenecessary glue code for our paging API. Look at the source code for inspiration onhow to integra\u00aete this technique with your own service and UI layer.\u00ae Even if you don\u2019t use JSF, it should be straightforward to adapt our paging and sorting solutions to other web frameworks. If you plan to use JSF, the next section is foryou: we dive into complex use cases such as placing a bid and editing an auction itemwith a JSF web interface and a stateful service layer.In the previous example, you can click an auction item name in the catalog. Thisbrings you to the auction page for that item, with more detailed information, asshown in figure 19.3. The item auction page has one form, allowing you to place a bid on the item. Thisis the first use case to implement: placing a bid. You can handle this with a simplerequest-scoped service.",
    "page407": "The page is bookmarkable that is, it can and will be called with a query parameter,the identifier value of an Item. The view metadata tells JSF it should bind this parameter value to the back-end property id of the AuctionService. You implement this newservice in a minute.Rendering this information repeatedly calls the AuctionService#getItem() method,something you have to keep in mind when you work on the service implementation.Finally, here\u2019s the form for placing a bid:You need to transmit the identifier value of the item when the form is submitted. Theback-end service is request-\u00aescoped, so it needs to be initialized for every request: thiscalls the AuctionService#setId() method. The entered bid amount is set by JSF with the AuctionService#setNewBidAmount()method when the POSTback of this form is processed. After all values have been bound, the action method AuctionService#placeBid() iscalled.You don\u2019t need to hold state across requests for this use case. A service instance is created when the auction page view is rendered with a GET request, and JSF binds therequest parameter by calling setId(). \u00aeThe service instance is destroyed after rendering is complete. Your server doesn\u2019t hold any state between requests. When the auction form is submitted and processing of that POST request starts, JSF calls set\u00aeId() tobind the hidden form field, and you can again initialize the state of the service. The state you hold for each request is the identifier value of the Item th\u00aee user is working on, the Item after it was loaded, the currently highest bid amount for that item,and the new bid amount entered by the user.",
    "page408": "Remember that in JSF, any property accessor methods bound in an XHTML templatemay be called multiple times. In the AuctionService, data will be loaded when accessor methods are called, and you have to make sure you don\u2019t load from the databasemultiple times by accident:When JSF calls setId() for the first time in a request, you load the Item with that identifier value once. Fail immediately if the entity instance can\u2019t be found. You also loadthe currently highest bid amount to initialize the state of this service fully, before theview can be rendered or the action method called. Note that this is a nontr\u00aeansactional method! Unlike EJBs, methods in a simple CDIbean don\u2019t require a transaction to be active by default. The produced EntityManagerand the persistence context used by the DAOs are unsynchronized, and \u00aeyou read datafrom the database in auto-commit mode. Even if later, while still processing the samerequest, a transactional method is called, the same unsynchronized, already-produced, request-scoped EntityManager is reused.The @Transactional annotation is new in Java EE 7 (from JTA 1.2) and similar to@TransactionAttribute on EJB components. Internally, an interceptor wraps themethod call in a system transaction context, just like an EJB method. Per\u00aeform transactional work and store a new bid in the database, and prevent concurrent bids. You must join the persistence context with the transaction. It doesn\u2019t matterwhich DAO you call: all of them share the same request-scoped EntityManager.",
    "page409": "If another transaction is committed for a higher bid in the time the user was thinkingand looki\u00aeng at the rendered auction page, fail and re-render the auction page with amessage. You must force a version increment of the Item at flush time to prevent concurrent bids.If another transaction runs at the same time as this and loads the same Item version andcurren\u00aet highest bid from the database in setId(), one of the transactions must fail inplaceBid(). This is a simple redirect-after-POST in JSF, so users can safely reload the page after submitting a bid.When the placeBid(\u00ae) method returns, the transaction is committed, and the joinedpersistence context f\u00aelushes changes to the database automatically. In Java EE 7, youcan finally have the most convenient feature of EJBs: the declarative transactiondemarcation, independ\u00aeent from EJBs. Another new feature in Java EE is the CDI conversation scope and its integrationin JSF.You can declare beans and produced instances in Java EE as @ConversationScoped.This special scope is manually contr\u00aeollable with the standard javax.enterprise.context.Conversation API. To understand conversation scope, think about theexamples in the previous chapter. The shortest possible conversation, a unit of work f\u00aerom the perspective of theapplication user, is a single request/response cycle. A user sends a request, and theserver star\u00aets a conversation context to hold the data for that request.",
    "page410": " When theresponse is returned, this short-running transient conversation context ends and isclosed. Transient conversation scope is therefore the same as the request scope; bothcontexts have the same life cycle. Thi\u00aes is how the CDI specifi\u00aecation maps the conversation scope to servlet requests and therefore also JSF and JAX-RS requests. With t\u00aehe Conversation API, you can\u00ae promote a conversation on the server andmake it long-running and no longer transient. If you promote the conversation during a request, the same conversation context is then available to the next request. Youcan use the context to transport data from one request to the next. Usually you\u2019dneed the session context for this, but the data from several parallel conversations (auser opens two browser tabs) would have to be manually isolated in the session. This iswhat the conversation context provides: automatic data isolation in a controllable context, which is more convenient than the regular session context. You implement the conversation context with server-side sessions, of course, andstore the da\u00aeta in the user\u2019s session. To isolate and identify conversations within a se\u00aession, each conversation has an identifier value; you must transmit that value with everyrequest. The parameter name cid has even been standardized for this purpose. Eac\u00aehconversation also has an indi\u00aevidual timeout setting so the server can free resources if auser stops sending requests. This allows the server to clean up expired conversationstate automatically without waiting until the entire user session expires.",
    "page411": "Another nice feature of the long-running (not transient) conversation context isthat the server protects concurrent ac\u00aecess to\u00ae conversation-scoped data automatically:if a user quickly clicks an action button several times, and each request transmits theidentifier value of a particular conversation, the server will terminate all except one ofthe requests with BusyConversationExceptions. We now walk through an example of conversation scope usage in JSF. You implement a use case with a workflow that takes the user several requests to c\u00aeomplete: putting an item up for auctionIf you think about it for a minute, creating and editing an auction item are very\u00ae similar tasks. These wo\u00aerkflows are conversations: units of work from the perspective of theapplication user, and your users will expect them to look similar. Your goal is thereforeto avoid code duplication; you should implement both cases with a single UI and backend service. Your application guides the user through the workflow; figure 19.4 showsa graphical representation as a state chart. Here\u2019s how to read this diagram. The user may sta\u00aert the conversation without anydata, if no item exists. Alternatively, the user may have the identifier value of an existing item, which is probably a simple numeric value. How the user obtained this identifier isn\u2019t important; maybe they executed some k\u00aeind of search in a previousconversation. This is a\u00ae common scenario, and many conversation workflows have several entry points (the solid filled circles in the state chart).\u00ae From the perspective of the user, editing an item is a multistep process: each boxwith rounded corners represents a state of the application.",
    "page412": " In the Edit Item state, theapplication presents the user with a dialog or form where they can change the item\u2019sdescription or the starting price for the auction. Meanwhile, the application waits forthe user to trigger an event; we call this user think-time. When the user triggers the Next eve\u00aent, the application processes it, and the userproceed\u00aes to the next (wait) state, Edit Images. When the user finishes working on theitem images, they end the conversation by submitting the item and images (the solidcircles with an outer circle). Clicking Cancel on any page aborts everything. From theapplication user\u2019s perspective, the entire conversation is an atomic unit: all changesare committed and final wh\u00aeen the\u00ae user clicks Submit. No other transition or outcomeshould result in any permanent changes to the database. You\u2019ll implement this with a wizard-style user interface with two web pages. On thefirst page, the user enters the details of the item to sell or edit, as shown in figure 19.5.Note that rendering this page won\u2019t start a long-running conversation context on these\u00aerver. This would waste resources\u00ae, because you don\u2019t know yet whether the userwants to follow through and work on the item details. You begin the long-runningconversation context a\u00aend hold state across requests on the server when the user clicksthe Next button for the first time. If form validation passes, the server stores the conversational data in the us\u00aeer\u2019s session and advances the user to the Edit Images page(see figure 19.6).",
    "page413": "The unit of work is complete when the user submits the item afte\u00aer uploadingimages for the item. The user may at any time click Cancel to end the conversation orwalk away from the terminal and let the conversation expire. The user may work intwo browser tabs and run several conversations in parallel, so they must be isolatedfrom each other on the server within the same user session. All conversation data for auser is removed if the user session expires (or if, in a cluster of servers with sticky sessions, a server fails). The XHTML templates for this wizard have no markup relevant for the conversation; JSF with CDI automates this. Conversation identifiers are transmitted automatically with regular JSF form submissions in a generated hidden field, if a long-runningconversation context was discovered when the form was rendered. This works evenacross JSF redirects: the cid parameter is appended automatically to the redirect destination URL. The conversation-scoped back-end service bound to those pages the EditItemServic\u00aee is w\u00aehere everything happens and where you control the contexts.The service instance is conversation-scoped. By default, the conversation context istransient and therefore behaves as a request-scoped service. The class must be Serializable, unlike a request-scope implementation. An instanceof EditItemService may be stored in the HTTP session, and that session data may beserialized to disk or sent across the network in a cluster. We took the easy way out inchapter 18 by using a stateful EJB, saying, \u201cIt\u2019s not passivation capable.\u201d Anything inthe CDI conversation scope must be passivation-capable and therefore serializable.",
    "page414": "The injected DAO instances have dependent scope and are serializable. You may thinkthey aren\u2019t, because they have \u00aean EntityManager field, which isn\u2019t serializable. We talkabout this mismatch in a second.The Conversation API is provided by the container. Call it to control the conversationcontext. You need it when the user clicks the Next button for the first time, promotingthe transient conversation to long-running.This is the state of the service: the item the user is editing on the pages of the wizard.You start with a fresh Item entity instance in transient state. If this service is initializedwith an item identifier value, load the Item in setItemId(). This is a transient state of the service. You only need it temporarily when the userclicks Upload on the Edit\u00ae Images page. The Part class of the Servlet API isn\u2019t serializable. It\u2019s not uncommon to have some transient state in a conversational service, butyou must initialize it for every request when it\u2019s needed.setItemId() is called only if the request contains an item identifier value. You therefore have two entry points into this conversation: with or\u00ae without an existing item\u2019sidentifier value. If the user is editing an item, you must load it from the database. You\u2019re still relying ona request-scoped persistence context,\u00ae so as soon as the request is complete, this Iteminstance is in detached state. You can hold detached entity instances in a conversational service\u2019s state and merge it when needed to persist changes (see section 10.3.4).",
    "page415": "Unlike stateful EJBs, you can\u2019t disable passivation of the conversation context. The CDIspecification requires that the class and all non-transient dependencies of a conversation-scoped component be serializable. In fact, you\u2019ll get a deployment error if youmake a mistake and include state that can\u2019t be serialized in a conversation-scopedbean \u00aeor any of its dependencies. To pass this test, we lied earlier by saying that GenericDAO is java.io.Seria\u00aelizable.The EntityManager field in GenericDAOImpl is not serializable! This works because CDIuses contextual references smart placeholders. The EntityManager field of the DAOs isn\u2019t an actual instance of a persistence context at runtime. The field holds a reference to some EntityManager: some currentpersistence context. Remember that CDI produces and injects the dependencythrough the constructor. Because you declared it request-scoped, at runtime a specialproxy is inje\u00aected that only looks like a real EntityManager. This proxy delegates allcalls to an actual EntityManage\u00aer it finds in the current request context. The proxy isserializable and doesn\u2019t hold a reference to an EntityManager after a requ\u00aeest is complete. It can t\u00aehen be easily serialized; and when it\u2019s deserialized, maybe even on a different JVM, it will continue to do its work and obtain a request-scoped EntityManagerwhenever called. Therefore, you\u2019re not serial\u00aeizing the entire persistence context only a proxy that can look up the current request-scoped\u00ae persistence context.",
    "page416": "This may sound strange at first, but it\u00ae\u2019s how CDI works: if a request-scoped bean isinjected into a conversation-, session-, or application-scoped bean, an indirect reference is required. If you try to call the EntityManager proxy through a DA\u00aeO when norequest context is active (say, in a servlet\u2019s init() method), you\u2019ll get a ContextNotActiveException because the proxy can\u2019t obtain a current EntityManager. The CDIspecification also d\u00aeefines that such proxies may be passivated (serialized), even whenthe component they represent may not be. Assume now that the user has filled out the form on the first page of the\u00ae wizardwith the item details and clicks Next. You have to promote the transient conversationcontext.Click\u00aeing Next in the wizard sub\u00aemits the form with the Item details and takes the user tothe Edit Images page. Because the EditItemService is bound to a transient conversation context, processing this request happens in a new, transient conversation context.Reme\u00aember that the transient conversation scope is the same as the request scope. When you process the request with an action method, you call the ConversationAPI to control the current transient conversation context:\u00aeThe action method is called after all the Item details have been set on the EditItemService#item property\u00ae by the JSF engine. You make the transient conversation longrunning with an individual timeout setti\u00aeng. This timeout is obviously shorter than orequal to the timeout of the user\u2019s session; larger values don\u2019t make sense.",
    "page417": "The serverpreserves the state of the service in the user\u2019s session and renders an automaticallygenerat\u00aeed conversation identifier as a hidden field on any action form on the EditImages page. If you need it, you can also obtain the identifier value of the conversation with Conversation#getId(). You can even set your own identifier value in theConversation#begin() method call. The server now waits for the next request with the identifier of the conversation,most likely originating from the Edit Images page\u00ae. If this takes too long, becauseeither the \u00aelong-running conversation or the entire session expired, a NonexistentConversationException is thrown on request.Create the Imag\u00aee entity instance from the submitted multipart form.You must add the transient Image to the transient or detached Item. This conversationwill consume more and more memory on the server as uploaded image data is addedto conve\u00aersational state and therefore the user\u2019s session.One of the most important issues when building a stateful server system is memoryconsumption and how many concurrent user sessions the system can handle. Youmust be careful with conversational data. Always question whether you must hold datayou get from the user or data you load from the database for the entire conversation. When the user clicks Submit Item, the conversation ends, and all transient anddetached entity instances must be stored.",
    "page418": "The system transaction interceptor wraps the method call.You must join the unsynchronized request-scoped persistence context with the systemtransaction if you want to store data.This DAO call makes the transient or detached Item persistent. Because you enabled itwith a cas\u00aecading rule on the @OneToMany, it also stores any new transient or olddetached \u00aeItem#images collection elements. According to the DAO contract, you musttake the returned instance as the cu\u00aerrent state. Manually end the long-running conversation. This is effectively a demotion: the longrunning conversation becomes transient. You destroy the conversation context andthis service instance when the request is complete. All conversational state is removedfrom the user\u2019s session. This is a redirect-after-POST in JSF to the auction item details page, with the new identifier value of the now-persistent Item.This completes our example of stateful services with JSF, CDI, and a JPA persistencelayer. We think JSF and CDI are great in combination with JPA; you get a well-testedand standardized programming model with little overhead, both in terms of re\u00aesourceconsumption and lines of code. We now continue with CDI but, instead of JSF, introduce JAX-RS combined\u00ae with JPAin a stateless server design for any rich clients. One of the challenges you face in thisarchitecture is serializing data.",
    "page419": "When we first talked about writing persistence-capable classes in section 3.2.3, webriefly mentioned that the classes don\u2019t have to implement java.io.Serializable.You can apply this marker interface when needed. One of the cases when this was necessary so far was in chapter 18. Domain modelinstances were transmitted between EJB client and server systems, and they were automatically serialized on one end into some wire format and deserialized on the otherend. This worked flawlessly and without customization because both client and serverare Java virtual machines, and the Hibernate libraries were available on both systems.The client used Remote Method Invocation (RMI) and the standardized Java serialization format (a stream of bytes representing Java objects). If your client isn\u2019t running in a Java\u00ae virtual machine, you probably don\u2019t want toreceive a stream of bytes representing Java objects from the server. A common scenario is a s\u00aetateless server system that handles a rich client, such as a JavaScript application running in a web browser or a mobile device application. To implement this, youtypically create a Web API on the server that speaks HTT\u00aeP and transmit either XML orJSON payloads, which clients must then parse. The clients also send XML or JSON datawhen ch\u00aeanges must be stored on the server, so your server must be able to produceand consume the desired media types.",
    "page420": "You now write an HTTP server with the JAX-RS \u00aeframework, producing and consumingXML documents. Although the examples are in XML, they\u2019re equally applicable toJSON, and the fundamental problems we discuss are the same in both.Let\u2019s start with the JAX-RS serv\u00aeice. One servi\u00aece method delivers an XML document representing an Item entity instance when a client sends a GET HTTP request. Anotherservice method accepts an XML document for updating an Item in a PUT request:When the server receives a request with the request path /item, the method on thisservice handles it. By default, the service instance is request-scoped, but you can applyCDI scoping annotations to change that. An HTTP GET request maps to this method. The container uses the path segm\u00aeent after /item as an argument value for the call,such as /item/123. You map it to a method parameter with @PathParam. This method produces XML media; therefore, someone has to serialize the method\u2019sreturned value into XML. Be careful: this annotation isn\u2019t the same producer annotation as in CDI. It\u2019s in a different package! This method consumes XML media; therefore, someone has to deserialize the XMLdocument and transform i\u00aet into a detached Item instance.You want to store data in this method, so you must start a system transaction and jointhe persistence context with it.The JAX-RS standard covers automatic marshalling for the most important mediatypes and a whole range of Java types. A JAX-RS implement\u00aeation, for example, must beable to produce and consume XML media for an application-supplied Java Architecture for XML Binding (JAXB) class. The domain model entity class Item must therefore become a JAXB class.",
    "page421": "JAXB, much like \u00aeJPA, works with annotations to declare a class\u2019s capabilities. Theseannotations map the properties of a class to elements and attributes in an XML document. The JAXB runtime automatically reads and writes instances from and to XMLdocuments. This shou\u00aeld sound familiar to you by now; JAXB is a great companion forJPA-enabled domain models.An Item instance maps to an <item>\u00ae XML element. This annotation effectively enablesJAXB on the class. When serializing or deserializing an instance, JAXB should call the fields directly andnot the getter or setter methods. The reasoning behind this is the same as for JPA:freedom in meth\u00aeod design.The identifier and auction end date of the item become XML attributes, and all otherproperties are nested XML elements. You don\u2019t have to put any JAXB an\u00aenotations ondescription and initialPrice; they map to elements by default. Singular attributesof the domain model class are easy: they\u2019re either XML attributes or \u00aenested XML elements with some text. What about entity associations and collections?\u00ae You can embed a collection directly in the XML document and thus eagerlyinclude it, as you did with the Item#bids:There is some optimization potential here: if you say, \u201cAlways eagerly include bids\u201dwhen your service returns an Item, then you should load them eagerly. Right now, several queries \u00aeare necessary in JPA to load the Item and\u00ae the default lazy-mappedItem#bids. The JAXB serializer automatically iterates through the collection elementswhen the response is prepared.",
    "page422": "Hibernate initializes the collection data either lazily or eagerly, and JAXB (or anyother serializer you have) serializes each element in turn. The fact that Hibernate usesspecial collections internally, as discussed in section 12.1.2, makes no difference whenyou serialize. It will later be important when you deserialize an XML document, butlet\u2019s ignore this issue for now. Wh\u00aeen you don\u2019t want to include a collection or a property in the XML document,use the @XmlTransient annotation:Collections are easy to handle, regardless of whether they\u2019re collections of primitives,embeddables, or many-valued entity associations. Of course, you must be careful withcircular references, such as each Bid having a (back) reference to an Item. At somepoint, you must make a break and declare a reference transient. The most difficult issue you face when serializing an entity instance loaded byHibernate is internal proxies: the placeholders used for lazy loading of entity associations. In the Item class, this is the seller property, refere\u00aencing a User entity.Such a document would indicate to a client that the item has no seller. This is ofcourse wrong; an uninitialized proxy is not the same as null! You could assign specialmeaning to an empty XML element and say, on the client, \u201cAn empty element means a\u00aeproxy\u201d and \u201cA missing element means null.\u201d Unfortunately, we\u2019ve seen serializationsolutions, even designed for Hibernate, that don\u2019t make this distinction. Some serialization solutions, not designed for Hibernate, may even stumble and fail as soon asthey discover a Hibernate proxy. Usually, you must customize your serialization tool to deal with Hibernate proxiesin some meaningful way. In this application, you want the following XML data for anuninitialized entity proxy.",
    "page423": "This is the same data as the proxy: the entity class and the identifier value representedby the proxy. A client now knows that there is indeed a seller for this item and theidentifier of that user; it can request this data if needed. If you receive th\u00aeis XML document on the server when a user updates an item, you can reconstruct a proxy from theentity\u00ae class name and identifier value. You should write a model class that represents such an entity reference and map itto XML elements and attributes:Next, you must customize marshalling and unmarshalling the Item, so instead of areal User, an EntityReference handles the Item#seller property. In JAXB, you applya custom type adapter on the property:JAXB calls this constructor when it generates an XML document. In that case, youdon\u00ae\u2019t need an EntityManager: the proxy contains all the info\u00aermation you need towrite an EntityReference. JAXB must call this constructor when it reads an XML document. You need an EntityManager to get a Hibernate proxy from an EntityReference.When writing an XML document, take the Hibernate proxy and create a serializablerepresentation. This calls internal Hibernate methods that we haven\u2019t shown here.When reading an XML document, take the serialized representation and create aHibernate pr\u00aeoxy attached to the current persistence context.",
    "page424": "Finally, you need an extension for JAX-RS that will automatically initialize this a\u00aedapterwith the current request-scoped EntityManager when an XML document has to beunmarshalled on the server. You can find this EntityReferenceXMLReader extensionin the example code for this book. There are a few remaining points we need to discuss. First, we haven\u2019t talked aboutunmarshalling collections. Any <bids> element in the XML document will be deserialized when the service is called, and detached instances of Bid will be created from thatdata. You can access them on the detached Item#bids when your service runs. Nothing else will happen or can happen, though: the collection created during unmarshalling by JAXB isn\u2019t one of the \u00aespecial Hibernate collections. Even if you had enabledcascaded merging of the Item#bids collection in your mapping, it would be ignoredby EntityManager#merge(). This is similar to the proxy problem you solved in the previous section. You wouldhave to detect that you must create a special Hibernate collection when a particularproperty is unmarshalled in an XML document. You\u2019d have to call some Hibernateinternal APIs to create that magic collection. We recommend that you consider collections to be read-only; collection mappings in general are a shortcut for embeddingdata in query results when you send data to the client. When the client sends an XMLdocument to the server, it shouldn\u2019t include any <bids> element. On the server, youonly access the collecti\u00aeon on the persistent Item after it\u2019s merged (ignoring the collection during merge).",
    "page425": " Second, yo\u00aeu\u2019re probably wondering where our JSON examples are. We knowyou\u2019re most likely relying on JSON right now in your applications and not on a customXML\u00ae media type. JSON is a convenient format to parse in a JavaScript client. The badnews is that we couldn\u2019t figure out a way to customize JSON marshalling and unmarshalling in JAX-RS without relying on a proprietary framework. Although JAX-RS maybe standardized, how it generates and reads JSON isn\u2019t standardized; some JAX-RSimplementations use Jackson, whereas others use\u00ae Jettison. There is also the new standard Java API for JSON Proc\u00aeessing (JSONP), which some JAX-RS implementations mayrely on in the future. If you want to use JSON with Hibernate, you must write the same extension code wewrote \u00aefor JAXB, but for your favorite JSON marshalling tool. You\u2019ll have to customizeproxy handling, how proxy data is sent to the client, and how p\u00aeroxy data is turned backinto entity references with em.ge\u00aetReference(). You\u2019ll certainly have to rely on someextension API of your framework, just as we did with JAXB, but the same pattern applies.You\u2019ve seen many ways to integrate Hibernate and JPA in a web application environment. You enabled the EntityManager for CDI injection and improved thepersistence layer with CDI and a generic sorting and paging solution for finderqueries.You looked at JPA in a JSF web application: how to write request- and conversation-scoped services with a JPA persistence context.We discussed problems and solutions associated with entity data serialization,and how to resolve those in an environment with stateless clients and a JAX-RSserver.",
    "page426": "You use object/relational mapping to move data into the application tier in orderto use an object-oriented programming language to process that data. This is agood strategy when implementing a multiuser online transaction-processing application wi\u00aeth small to medium size data sets involved in each unit of work. On the other hand, operations that require massive amounts of data aren\u2019t bestsuited for the application tier. You \u00aeshould move the operation closer to where thedata lives, rather than the other way around. In an SQL system, the DML statemen\u00aetsUPDATE and DELETE execute directly in the database and are often sufficient if youhave to implement an operation that involves thousands of rows. Operations thatare more complex may require additional procedures to run inside the database;therefore, you should consider stored procedures as one possible strategy. You canfall back to JDBC and SQL at any time in Hibernate applications. We discussed somethese options earlier, in chapter 17. In this chapter, we show you how to avoid falling back to JDBC and how to execute bulk and batch operations with Hibernateand JPA.A major justification for our claim that applications using an object/relational persistence layer outperform ap\u00aeplications built using direct JDBC is caching. Although weargue passionately that most applications should be designed so that it\u2019s possible toachieve\u00ae acceptable performance without the use of a cache, there\u2019s no doubt that forsome kinds of applications, especially read-mostly applications or applications thatkeep significant metadata in the database, caching can \u00aehave an enormous impact onperformance.",
    "page427": "Furthermore, scaling a highly concurrent application to thousands ofonline transactions per second usually requires some caching to reduce the load onthe database server(s). After discussing bulk and batch operations, we explore Hibernate\u2019s caching system.First we look at standardized bulk statements in JPQL, such as UPDATE and DELETE, andtheir equivalent criteria versions. After that, we repeat some of these operations withSQL native statements. Then, you learn how to insert and update a large number ofentity instances in batches. Finally, we introduce the special org.hibernate.StatelessSession APIThe Java Persistence Query Language is similar to SQL. The main difference betweenthe two is that JPQL uses class names instead of table na\u00aemes and property namesinstead \u00aeof column names. JPQL also understands inheritance that is, whether you\u2019requerying with a superclass or an interface. The JPA criteria query facility supports thesame query const\u00aeructs as JPQL but in addition offers type-safe and easy programmaticstatement creation.\u00ae The next statements we show you support upda\u00aet\u00aeing and deleting data directly inthe database without the need to r\u00aeetrieve them into memory. We also provide a statement that can select data and insert it as new entity instances, directly in the database.JPA offers DML operations that are a little more powerful than plain SQL. Let\u2019s look \u00aeatthe first operation in JPQL: an UPDATE.",
    "page428": "This JPQL statement looks like an SQL statement, but it uses an entity name (classname) and property names. The aliases are optional, so you can also write updateItem set active true. You use the standard query API to bi\u00aend named and positionalparameters. The executeUpdate call returns the number of updated entity instances,which may be different from the number of updated database rows, de\u00aepending on themapping strategy. This UPDATE statement only affects the database; Hibernate doesn\u2019\u00aet update anyItem inst\u00aeance you\u2019ve already retrieved into the (current) persistence context. In theprevious chapters, we\u2019ve repeated that you should think about state management ofentity instances, not how SQL st\u00aeatements are managed. This strategy assumes that theentity instances you\u2019re referring to are available in memory. If you update or deletedata directly in the database, what you\u2019ve already loaded into application memory,into the persistence context, isn\u2019t updated or deleted. A pragmatic solution th\u00aeat avoids this issue is a simple convention: execute anydirect DML operations first in a fresh persistence context. Then, use the EntityManager to load and store entity instances. This convention guarantees that the persistence context is unaffected by any statements executed earlier. Alternatively, you canse\u00aelectively use the\u00ae refresh() operation to reload the state of an entity instance in thepersistence context from the database, if you know it\u2019s been modified outside of thepersistence context.",
    "page429": "Hibernate knows how to execute this update, even if several SQL statements have to begenerated or some data needs to be copied into a temporary table; it updates rows inseveral base \u00aetables (because CreditCard is mapped to several superclass and subclasstables).JPQL UPDATE statements can reference only a single entity class, and criteria bulkop\u00aeerations may have only one root entity;\u00ae you can\u2019t write a si\u00aengle statement to updateItem and CreditCard data simultaneously, for example. \u00aeSubqueries are allowed in theWHERE clause, and any joins are allowed only in these subqueries. You can upda\u00aete values of an embedded type: for example, update User u setu.homeAddress.street You can\u2019t update values of an embeddable type in acollection. This isn\u2019t allowed: update Item i set i.images.title Direct DML operations, by default, don\u2019t affect any version or timestamp values inthe affected entities (as standardized by JPA). But a Hibernate extension lets youincrement the version number of directly modified entity instances:The version of\u00ae each updated Item entity instance will now be directly incremented inthe database, indicating to any other transaction relying on optimistic concurrencycontrol that you modified the data. (Hi\u00aebernate\u00ae doesn\u2019t allow use of the versionedkeyword if your version or timestamp property relies on a custom org.hibernate.usertype.UserVersionType.) With the JPA criteria API, you have to increment the version yourself.",
    "page430": "The same rules for UPDATE statements and CriteriaUpdate apply to DELETE andCriteriaDelete: no joins, single entity class only, optional aliases, or subqueriesallowed in the WHERE clause. Another special JPQL bulk operation lets you create entity i\u00aenstances directly in thedatabase.Let\u2019s assume that some of your customers\u2019 credit cards have been stolen. You write twobulk operations to mark the day they were stolen (well, the day you discovered thetheft) and to remove the compromised credit-card data from your records. Becauseyou work for a responsible company, you have to report the stolen credit cards to theauthorities and affected customers. Therefore, before you delete the records, youextract everything stolen and create a few hundred (or thousand) StolenCreditCardrecords. You write a new mapped entity class just for this purpose:Hibernate maps this class to the STOLENCREDITCARD table. Next, you need a statementthat executes directly in the database, retrieves all compromised credit car\u00aeds, and creates new StolenCreditCard records. This is possible with the Hibernate-only INSERT... SELECT statement:This operation does two things. First, it selects the details of CreditCard records andthe respective owner (a User). Second, it inserts the result directly into the tablemapped by the StolenCreditCard class.\u00ae Note the following: The properties that are the target of an INSERT ... SELECT (in this case, theStolenCreditCard properties you list) have to be for a particular subclass, notan (abstract) superclass. Because StolenCreditCard isn\u2019t part of an inheritancehierarchy, this isn\u2019t an issue.",
    "page431": "The types returned by the projection in the SELECT must match the typesrequired for the arguments of the INSERT. In the example, the identifier property of StolenCreditCard is in the list ofinse\u00aerted properties and supplied through selection; it\u2019s the same as the originalCreditCard identifier value. Alternatively, you can map an identifier generatorfor StolenCreditCard; but this works only for identifier generators that operate directly inside the database, such as sequences or identity fields.If the generated records are of a versioned class (with a version or timestampproperty), a fresh version (zero, or the current timestamp) is also generated.Alternatively, you can select a version (or timestamp) value and add the version(or timestamp) property to the list of inserted properties.The INSERT ... SELECT statement was, at the time of writing, not supported by the JPAor Hibernate criteria APIs.JPQL and criteria bulk operations cover many situations in which you\u2019d usuallyresort to plain SQL. In some cases, you may want to execute SQL bulk operations without falling back to JDBC. In the previous section, you saw JPQL UPDATE and DELE\u00aeTE statements. The primaryadvantage of these statements is that they work with class and property names and thatHibernate knows how to handle inheritance hierarchies and versioning when generating SQL. Because Hibernate parses JPQL, it also knows how to efficiently dirty-checkand flush the p\u00aeersistence con\u00aetext before the query and how to invalidate second-levelcache regions.",
    "page432": "With JPA native bulk statements, you must be aware of one important issue: Hibernatewill not parse your SQL statement to detect the affected tables. This means Hibernatedoesn\u2019t know whether a flush of the persistence context is required before the queryexecutes. In the previous example, Hibernate doesn\u2019t know you\u2019re updating rows inthe ITEM table. Hibernate has to dirty-check and flush any entity instances in the persistence context when you exec\u00aeute the query; it can\u2019t only dirty-check and flush Iteminstances in the persistence context. You must con\u00aesider another issue if you enable the second-level cache (if you don\u2019t,don\u2019t worry): Hibernate has to keep your second-level cache synchronized to avoidreturning stale data, so it will invalidate an\u00aed clear all second-level cache regions whenyou execute a native SQL UPDATE or DELETE statement. This means your second\u00ae-levelcache will be empty after this query!With the addSynchronizedEntityClass() method, you can let Hibernate know whichtables are affected by your SQL statement and Hibernate will clear only the relevantcache regions. Hibernate now also knows that it has to flush only modified Item entityinstance in the persistence context, before the query. Sometimes you can\u2019t exclude the application tier in a mass d\u00aeata operation. Youhave to load data into application memory and work with the EntityManager to perform your updates an\u00aed deletions, which brings us to batch processing.",
    "page433": "If you have to create or update a few hundred or thousand entity instances in onetransaction and unit of work, you may run\u00ae out of memory. Furthermore, you have toconsider the time it takes for the transaction to complete. Mos\u00aet transaction managershave a low transaction timeout, in\u00ae the range of seconds or minutes. The Bitronixtransaction manager used for the examples in this book has a default transactiontimeout of 60 seconds. If your unit of work takes longer to complete, you should firstoverride t\u00aehis timeout for a particular transaction:This is the UserTransaction API. Only future transactions started on this thread willhave the new timeout. You must set the timeout before you begin() the transaction. Next, let\u2019s insert a few thousand Item instances into the database in a batch.Every transient entity instance you pass to EntityManager#persist() is added to thepersistence context cache, as explained in section 10.2.8. To prevent memory exhaustion, you flush() and clear() the persistence context after a certain number ofinsertions, effectively batching the inserts.Create and persist 100,000 Item instances. After 100 operations, flush and clear the persistence context. This executes the SQLINSERT statements for 100 Item instances, and because they\u2019re now in detached stateand no longer referenced, the JVM garbage collection can reclaim that memory.You should set the hibernate.jdbc.batch_size property in the persistence unit tothe same size as your batch, here 100. With this setting, Hibernate will batch theINSERT statements at the JDBC level, with PreparedStatement#addBatch().",
    "page434": "If you enable the shared second-level cache for the Item entity, you should thenbypass the cache for your batch (insertion) procedure; see section 20.2.5. A serious problem with mass insertions is contention on the identifier generator:ev\u00aeery call of EntityManager#persist() must obtain a new identifier value. Typically, the generator is a database sequence, called once for every persisted entityinstance. You have to reduce the number of database round trips for an efficientbatch procedure.Now use the generator with @GeneratedValue in your mapped entity classes. With increment_size set to 100, the sequence produces the \u201cnext\u201d values 100,200, 300, 400, and so on. The pooled-lo optimizer in Hibernate generates intermediate values each time you call persist(), without another round trip to the database.Therefore, if the next value obtained from the sequence is 100, Hibernate will generate the identifier values 101, 102, 103, and so on in the application tier. Once the optimizer\u2019s pool of 100 identifier values is exhausted, the database obtains the nextsequence value,\u00ae and the procedure repeats. This means you only make one round tripto get an identifier value from the database per batch of 100 insertions. Other identifier generator optimizers are available, but the pooled-lo optimizer covers virtually alluse cases and is the easiest to understand and configure. Be aware that an increment size of 100 will leave large gaps in between numericidentif\u00aeiers if an application uses the same sequence but doesn\u2019t apply the same algorithm as Hibernate\u2019s optimizer. This shouldn\u2019t be too much of a concern; instead ofbeing able to generate a new identifier value each millisecond for 300 million years,you might exhaust the number space in 3 million years. You can use the same batching technique to update large number of entityinstances.",
    "page435": "Imagine that you have to manipulate many Item entity instanc\u00aees and that the changesyou need to make aren\u2019t as trivial as setting a flag (which you\u2019ve done with a singleUPDATE JPQL statement previously). Let\u2019s also assume that you can\u2019t create a databasestored procedure, for whatever reason (maybe because your applic\u00aeation has to workon database-management systems that don\u2019t support stored \u00aeprocedures). Your onlychoice is to write the procedure in Java and to retrieve a massive a\u00aemount of dat\u00aea intomemory to run it through the procedure. This requires working in batches and scrolling through a query result with a database cursor, which is a Hibernate-only query feature. Please r\u00aeeview our explanation ofscrolling with cursors in section 14.3.3 and make sure database cursors are p\u00aeroperlysupported by your DBMS and JDBC driver. The following code loads 100 Item entityinstances at a time for processing.You use a JPQL query to load all Item instanc\u00aees from the databa\u00aese. Instead of retrieving the result of the query completely into application memory, you open an onlinedatabase cursor. You control the cursor with the ScrollableResults API and move it along the result.Each call to next() forwards the cursor to the next record. The get(int i) call retrieves a single entity instance into memory: the record the cursor is currently pointing to.To avoid memor\u00aey exhaustion, you flush and clear the persistence context before loading the next 100 records into it.",
    "page436": "For the best performance, you should set the size of the property hibernate.jdbc.batch_size in the persistence unit configuration to the same value as your procedure batch: 100. Hibernate bat\u00aeches at the JDBC level all UPD\u00aeATE statements executedwhile flushing. By default, Hibernate won\u2019t batch at the JDBC level \u00aeif you\u2019ve enabledversioning for an entity class some JDBC drivers have trouble returning the correctupdated row count for batch UPDATE statements (Oracle is known to have this issue).If you\u2019re sure your JDBC driver supports this properly, and your Item entity class hasan @Version annotation, enable JDBC batching by setting the \u00aeproperty hibernate.jdbc.batch_versioned_data to true. If you enable the shared second-levelcache for the Item entity, you should then bypass the cache for your batch (update)procedure; see section 20.2.5. Another option that avoids memory consumption in the persistence context (byeffectively disabling it) is the org.hibernate.StatelessSession interface.The persistence context is an essential feature of the Hibernate engine. Without apersistence context, you can\u2019t manipulate entity state and have Hibernate detect \u00aeyourchanges automatically. Many other things wouldn\u2019t also be possible. Hibernate offers an alternative interface, however, if you prefer to work with yourdatabase by executing statements. The statement-oriented\u00ae interface org.hibernate.StatelessSession,\u00ae feels and works like plain JDBC, except that you get the benefit of mapped persistent classes and Hibernate\u2019s database portability. The mostinteresting methods in this interface are insert(), update(), and delete(), which allmap to the equivalent immediately executed JDBC/SQL operation.",
    "page437": "Open a StatelessSession on the Hibernate SessionFactory, which you can unwrapfrom an EntityManagerFactory.Use a JPQL query to load all Item instances from the database. Instead of retrievingthe result of \u00aethe query completely into application memory, open an online databasecursor.Scroll through th\u00aee result with the cursor, and retr\u00aeieve an Item entity instance. Thisinstance is in detac\u00aehed state; there i\u00aes no persistence context! Because Hibernate doesn\u2019t detect changes automatically without a persistence context\u00ae, you have to execute SQL UPDATE statements manually.Disabling the persistence context and working with the StatelessSession interfacehas some other serious consequences and conceptual limitations (at least, if you compare it to a regular EntityManager and org.hibernate.Session): The StatelessSession doesn\u2019t have a per\u00aesistence context cache and doesn\u2019tinteract with any other second-level or query cache. There is no automatic dirtychecking or SQL execution when a transaction commits. Everything\u00ae you doresults in immediate SQL operations. No modification of an entity insta\u00aence and no operation you call are cascaded toany associated instance. Hibernate ignores any cascading rules in your mappings. You\u2019re working with instances of a sing\u00aele entity class. You have no guaranteed scope of object identity. The same query executedtwice in the same StatelessSession produces two different in-memorydetached instances. This can lead to data-aliasing effects if you don\u2019t carefullyimplement the equals() and hashCode() methods in your persistent classes.",
    "page438": "H\u00aeibernate ignores any modifications to a collection that you mapped as anentity association (one-to-many, many-to-many). Only collections of basic orembeddable types are considered. Therefore, you shouldn\u2019t map entity associations with collections but only many-to-one or one-to-one and handle the relationship through that side only. Write a query to obtain data you\u2019d otherwiseretrieve by itera\u00aeting through a mapped collection. Hibernate doesn\u2019t invoke JP\u00aeA event listeners and event callback methods foroperations executed with StatelessSession. StatelessSession bypasses anyenabled org.hibernate.Interceptor, and you can\u2019t intercept it through theHibernate core event system.Good use cases for a StatelessSession are rare; you may prefer it if manual batchingwith a regular En\u00aetityManager becomes cumbersome. In the next section, we introduce the Hibernate shared caching system. Cachingdata on the application tier is a complementary optimization that you can utilize inany sophisticated multiuser application.In this section, we show you how to enable, tune, and m\u00aeanage the shared data cachesin Hibernate. The shared data cache is not the persistence context cache, whichHibernate never shares between application threads. For reasons explained in section10.1.2, this isn\u2019t optional. We call the persistence context a first-level cache. The shareddata cache the second-level cache is optional, and although JPA sta\u00aendardizes someconfiguration settings and mapping metadata for shared caching, every vendor hasdifferent solut\u00aeions for optimization. Let\u2019s start with some background informationand explore the architecture of Hibernate\u2019s shared cache.",
    "page439": "A cache keeps a representation of curre\u00aent database state close to the application,either in\u00ae memory or on disk of the application server machine. A cache is a local copyof the data and sits between your application and the d\u00aeatabase. Simplified, to Hibernate a cache looks like a map of key/value pairs. Hibernate can store data in the cacheby providing a key and a value, and it can look up a value in the cache with a key. Hibernate has several types of shared caches available. You may use a cache toavoid a database hit whenever the following take place: The application performs an entity instance lookup by identifier (primary key);this may get a hit in the entity data cache. Initializing an entity proxy on demandis the same operation and, internally, may hit the entity data cache instead ofthe database. The cache key is the identifier value of the entity instance, andthe cache value is the data of the entity instance (its property va\u00aelues).\u00ae Theactual data is stored in a special disassembled format, and Hibernate assemblesan entity instance again when it reads from the entity data cache.The persistence engine initializes a collection lazily; a collection cache may holdthe elements of the collection. The cache key is the collection role: for example, \u201cItem[1234]#bids\u201d wo\u00aeuld be the bids collection of an Item instance withidentifier 1234. The cache value in this case w\u00aeould be a set of Bid identifier values, the elements o\u00aef the collection. (Note that this collection cache does nothold the Bid entity data, only the data\u2019s id\u00aeentifi\u00aeer values!).",
    "page440": "The application performs an entity instance lookup by a unique key attribute.This is a special natural identifier cache for entity classes with unique properties:for example, User#username. The cache key is the unique property, such as theusername, and the cached value is the User\u2019s entity instance identifier. You execute a JPQL, criteria, or SQL query, and the result of the actual SQLquery is already stored in the query result cache. The cache key is t\u00aehe renderedSQL statement including all its parameter values, and the cache value is somerepresentation of the SQL result set, which may include entity identifier values.It\u2019s critically important to understand that the entity data cache is the only type ofcache that holds actual entity data values. The other three cache types only hold entityidentifier information. Therefore, it doesn\u2019t make sense to enable the natural identifier cache, for example, without also enabling the entity data cache. A lookup in thenatural identifier cache will, when a match is fou\u00aend, always involve a lookup in theentity data cache. We\u2019ll further analyze this behavior below with some code examples. As we hin\u00aeted earlier, Hibernate has a two-level cache architecture.You can see the various elements of Hibernate\u2019s ca\u00aeching system in figure 20.\u00ae1. Thefirst-level cache is the persistence context cache, which we discussed in section 10.1.2.Hibernate does not shar\u00aee this cache between threads; each application thread has itsown copy of the data in this cache. Hence, there are no issues with transact\u00aeion isolation and concurrency when accessing this cache.",
    "page441": "The second-level cache system in Hibernate may be process-scoped in the JVM ormay be a cache system that can work in a cluster of JVMs. Multiple application threadsmay access the shared second-level caches concurrently. The cache concur\u00aerency strategydefines the transaction \u00aeisolation details for entity data, collecti\u00aeon elements, and\u00aenatural identifier caches. Whenever an entry is stored or loaded in these caches,Hibernate will coordinate access with the configured strategy. Picking the right cacheconcurrency strategy for entity classes and their collections can be cha\u00aellenging, andwe\u2019ll guide you through the process with several examples later on. The query result cache also has its own, internal strategy for handling concurrentaccess and keeping the cached results fresh and coordinated with the database. Weshow you how the query cache works and for which queries it makes sense to enableresult caching. The cache provider implements the physical cac\u00aehes as a pluggable system. For now,Hibernate forces you to choose\u00ae a single cache provider for the entire persistence unit.The cache provider is responsible for handling physical cache regions the bucketswhere the data is held on the application tier (in memory, in indexed files, or evenreplicated in a cluster). The cache provider controls expiration policies, such as whento remove data from a region by timeout, or keeping only the most-recently used datawhen the cache is full. The cache provider implementation may be able to comm\u00aeunicate with other instances in a cluster of JVMs, to synchronize data in each instance\u2019sbuckets. Hibernate itself doesn\u2019t\u00ae handle any clustering of caches; this is fully delegated to the cache provider engine.",
    "page442": "In this section, you set up caching on a single JVM with the Ehcache provider, a simple bu\u00aet very powerful caching engine (originally developed for Hibernate specificallyas the easy Hibernate cache). We only cover some of Ehcach\u00aee\u2019s basic settings; consult itsmanual for more information. Frequently, the first question many developers have about the H\u00aeibernate cachin\u00aegsystem is, \u201cWill the cache know when data is modified in the database?\u201d Let\u2019s try toanswer this question before you get hands-on with cache configuration and usage.If an application does not have exclusive access to the database, shared caching shouldonly be used for data that changes rarely and for which a small window of inconsistency is acceptable after an update. When another application updates the database,your cache may contain stale data until it expires. The other application may be adatabase-triggered stored procedure or even an ON DELETE or ON UPDATE foreign keyoption. There is no way for Hibernate\u2019s cache system to know when another application or trigger updates the data in the database; the database can\u2019t send your application a message. (You could implement such a messaging system with database triggers\u00aeand JMS, but doing so isn\u2019t exactly trivial.) Therefore, using caching depends on thetype of data and the freshness of the data required by your\u00ae business case. Let\u2019s assume for a moment that your application has exclusive access to the database. Even then, you must ask the same questions as a shared cache makes dataretrieved from the database in one transaction visible to another transaction. Whattransaction isolation guarantees \u00aeshould the shared cache provide? The \u00aeshared cachewill affect the isolatio\u00aen level of your transactions, whether you read only committeddata or if reads are repeatable.",
    "page443": "For some data, it may be acceptable that updates by oneapplication thread aren\u2019t immediately visible by other application threads, providingan acceptable window of inconsistency. This would allow a much more efficient andaggressive caching strategy. Start this\u00ae design process wi\u00aeth a diagram of your domain model, and look at theentity classes. Good candidates for caching are classes that represent Data that changes rarely Noncritical data (for example, content-management data)Data that\u2019s local to the application and not modified by other applicationsBad candidates include Data that is updated often Financial data, where decisions must be based on the latest update Data that is shared with and/or written by other applicationsThese aren\u2019t the only rules we usually apply. Many applications have a number ofclasses with the following properties: A small number of instances (thousands, not millions) that all fit into memory Each instance referenced by many instances of another class or classes Instances that are rarely (or never) updatedWe sometimes call this kind of data reference data. Examples of reference data are Zipcodes, locations, static text messages, and so on. Reference data is an excellent candidate for shared caching, and any application that uses referen\u00aece data heavily will benefit greatly from caching that data. You allow the data to be refreshed when the cachetimeout period expires, and some small window of inconsistency is acceptable after anupdate. In fact, some reference data (such as country codes) may have an extremelylarge window of inconsistency or may be cached eternally if the data is read-only. You must exercise careful judgment for each class and collection for which youwant to enable caching. You have to decide which concurrency strategy to use.",
    "page444": "A cache concurrency strategy is a mediator: it\u2019s responsible for storing i\u00aetems of data inthe cache and retrieving them from the ca\u00aeche. This important role defines the transaction isolation semantics for that particular item. You\u2019ll have to decide, for each persistent class and collection, which cache concurrency strateg\u00aey to use if you want toenable the shared cache. The four built-in Hibernate concurrency strategies represen\u00aet decreasing levels ofstrictness in terms of transaction isolation: TRANSACTIONAL Available only in environments \u00aewith a system transaction manager, this strategy guarantees full transactional isolation up to repeatable read, ifsupported by the cache provider. With this strategy, Hibernate assumes that thecache provider is aware of and participating in system transactions. Hibernatedoesn\u2019t perform any kind of locking or version \u00aechecking; it relies solely on thecache provider\u2019s ability to isolate data in concurrent transactions. Use this strategy for read-mostly data where it\u2019s critical to prevent stale data in concurrenttransactions, in the rare case of an update. This strategy also works in a cluster ifthe cache provider engine supports synchronous distributed caching. READ_WRITE Maintains read committed isolation where Hibernate can use atime-stamping mechanism; hence, this strategy only works in a non-clusteredenvironment. Hibernate may also use a proprietary locking API offered by thecache provider. Enable this strategy for read-mostly da\u00aeta where it\u2019s critical toprevent stal\u00aee data in concurrent transactions, in the rare case of an update. Youshouldn\u2019t enable this strategy if data is concurrently modified (by other applications) in the database.",
    "page445": "NONSTRICT_READ_WRITE Makes no guarantee of consistency between thecache and the database. A transaction may read stale data from the cache. Usethis strategy if data hardl\u00aey ever changes (say, not every 10 seconds) and a window of inconsistency isn\u2019t of critical concern. You configure the duration of theinconsistency window with the expiration policies of your cache provider. Thisstrategy is usable in a cluster, even with asynchronous distributed caching. Itmay be app\u00aeropriate if other applications change data in th\u00aee sam\u00aee database. READ_ONLY Suitable for data that never changes. You get an exception if youtrigger an update. Use it for reference data only.With decreasing strictness come increasing performance and\u00ae scalability. A clusteredasynchronous cache with NONSTRICT_READ_WRITE can handle many more transactionsthan a synchronous c\u00aeluster with TRANSACTIONAL. You have to evaluate carefully theperformance of a clustered cache with full transaction isolation before using it in production. In many cases, you may be better off not enabling the shared cache for a particular class, if stale data isn\u2019t an option! You should benchmark your application with the shared cache disabled. Enable itfor good candidate classes, one at a time, while continuously testing the scalability ofyour system and evaluating concurrency strategies. You must have automated testsavailable to judge the impact of changes to your cache setup. We recommend that youwrite these tests first, for the performance and scalability hotspots of your application,before you enable the shared cache. With all this theory under your belt, it\u2019s time to see how caching works in practice.First, you configure the shared cac\u00aehe.",
    "page446": "The shared cache mode controls how entity classes of this persistence unit becomecacheable. Usually you prefer to enable caching selectively for only some entityclasses. Options: DISABLE_SELECTIVE, ALL, and NONE. Hibernate\u2019s second-level cache system h\u00aeas to be enabled explicitly; it isn\u2019t enabled bydefault. You can separately enable the query result cache; it\u2019s disabled by default aswell. Pick a provider for the s\u00aeecond-level cache system. For Ehcache, add the org.hibernate:hibernate-ehcache Maven artifact dependency to your classpath. Then, choose howHibernate uses Ehcache with this region factory setting; here you tell Hibernate to manage a single Ehcache instance internally as the second-level cache provider. Hibernate passes this property to Ehcache when the provider is started, setting thelocation of the Ehcache configuration file. All physical cache settings for cacheregions are in this file. This controls how Hibernate disassembles and assembles entity state when data isstored and loaded from the second-level cache. The structured cache entry format isless efficient but necessary in a clustered environment. For a nonclustered secondlevel cache like the singleton Ehcache on this JVM, you can disable this setting and usea more efficient format. When you experiment with the second-level cache, you usually want to see what\u2019s happening behind the scenes. Hibernate has a statistics collector and an API to accessthese statistics. For performa\u00aence reasons, it\u2019s disabled by default (and should be disabled in production).The second-level cache system is now ready, and Hibernate will start Ehcache whenyou build an EntityManagerFactory for this persistence unit. Hibernate won\u2019t cacheanything by default, though; you have to enable caching selectively for entity classesand their collections.",
    "page447": "We now look at entity classes and collections of the Ca\u00aeveatEmptor domain model andenable caching with the right concurrency strategy. In parallel, you\u2019ll configure thenecessary physical cache regions in the Ehcache configuration file. First the User entity: this data rarely changes, but, of course, a user may changetheir user name or address from time to time. This isn\u2019t critical data in a financialsense; few people make buying decisions based on a user\u2019s name or address. A smallwindow of inconsistency is acceptable when a user changes name or address information. Let\u2019s say there is no problem if, \u00aefor a maximum of one minute, the old information is still visible in some transactions. This means you can enable caching with theNONSTRICT_READ_WRITE strategy:The @Cacheable annotation enables the shared cache for this entity class, but a Hibernate annotation is necessary to pick the concurrency strategy. Hibernate stores andloads User entity data in the second-level cache, in a cache region named your.package.name.User. You can override the name with the region attribute of the @Cache annotation. (Alternatively, you can set a global region name prefix with the hibernate.cache.region_prefix property in the persis\u00aetence unit.) You also enable the natural identifier cache for the User entity with @org.hibernate.annotations.NaturalIdCache. The natural identifier properties are marked with@org.hibernate.annotations.NaturalId, and you have to tell Hibernate whether theproperty is mutable. This enables you to look up User instances by username withouthitting the database. Next, configure the cache regions for both the entity data \u00aeand the natural identifier caches in Ehcache.",
    "page448": "You can store a maximum 500 entries in both caches, and Ehcache won\u2019t keep themeternally. Ehcache will remove an element if it hasn\u2019t\u00ae been accessed for 30 secondsand will remove even actively accessed entries after 1 minute. This guarantees thatyour window of inconsistency from cache reads is never more than 1 minute. In otherwords, the cache region(s) will hold up to the 500 most-recently used user accounts,none older than 1 minute, and shrink automatically. Let\u2019s move on to the Item entity class. This data changes frequently, although youstill have many more reads than writes. If the name or description of an item ischanged, concurrent transactions should see this update immediately. Users makefinancial decisions, whether to\u00ae buy an item, based on the description of an item.Therefore, READ_WRITE is an appropriate strategy:Hibernate will coordinate reads and writes when Item changes are made, ensuringthat you can always read committed data from the shared cache. If ano\u00aether application is modifying Item data directly in the database, all bets are off! You configure thecache region in Ehcache to expire th\u00aee most-recently used Item data after one hour, toavoid filling up the cache bucket with stale data:Conside\u00aer the bids collection of the Item entity class: A particu\u00aelar Bid in the Item#bidscollection is immutable, but the collection itself is mutable, and concurrent units ofwork need to see any addition or removal of a collection element immediately.",
    "page449": "It\u2019s critical to r\u00aeemember that the collection cache will not contain the actual Bid data.The collection cache only holds a set of Bid identifier values. Therefore, you mustenable caching for the Bid entity as well. Otherwise, Hibernate may hit the cachewhen you start iterating through Item#bids, but then, due to cache misses, load eachBid separat\u00aeely from the database. This is a case where enabling the cache will result inmore load on your database server! We\u2019ve said that Bids are immutable, so you can cache this entity data as READ_ONLY:Hibernate\u2019s tran\u00aesparent caching behavior can be difficult to analyze. The API \u00aefor loading and storing data is still the EntityManager, with Hibernate automatically writingand reading data in the cache. Of course, you can see actual database access by logging Hibernate\u2019s SQL statements, but you should familiarize yourself with the org.hibernate.stat.Statistics API to obtain more information about a unit of workand see what\u2019s going on behind the scenes. Let\u2019s run through some examples to seehow this works. You enabled the statistics collector earlier in the persistence unit configuration, insection 20.\u00ae2.2. You access the statistics of the persistenc\u00aee\u00ae unit on the org.hibernate.SessionFactory:Here, you also get statistics for the data cache region for Item entities, and you can seethat there are several entries already in the cache. This is a warm cache; Hibernatestored data in the cache when \u00aethe application saved Item entity instances. However,the entities haven\u2019t b\u00aeeen read from the cache, and the hit count is zero\u00ae.",
    "page450": "The statistics tell you that there are three Item#bids collections in the cache (one foreach Item). No successful cache lookups have occurred so far. The entity cache of Bid has five records, and you haven\u2019t accessed it either. Initializing the collection reads the data from both caches. The cache found one collection as well as the data for its three Bid elements.The special natural identifier cache for Users is not completely transparent. You needto call a method on the org.hibernate.Session to perform a lookup by naturalidentifier:The natural identifier cache region for Users has one element.The org.hibernate.Sess\u00aeion API performs natural identifier lookup; this is the onlyAPI for accessing the natural identifier cache. You had a cache hit for the natural identifier lookup. The cache returned the identifier value \u201cjohndoe\u201d. You also had a cache hit for the entity data of that User.The statistics API offers much more information than we\u2019ve shown in these simpleexamples; we encourage you to explore this API further. Hibernate collects information about all its operations, and these statistics are useful for finding hotspots such asthe queries taking the longest\u00ae time and the entities and collections most accessedAs mentioned at the beginning of this section, Hibernate transparently writes andreads t\u00aehe cached data. For some procedures, you need more control over cacheusage, and you may want to bypass the caches explicitly. This is where cache modescome into play.",
    "page451": "JPA standardizes control of the shared cache with several cache modes. The followingEntityManager#find() operation, for example, doesn\u2019t attempt a cache lookup andhits the database directly:The default CacheRetrieveMode is USE; here, you override it for one operation withBYPASS. A more common usage of cache modes is the CacheStoreMode. By default, Hibernate puts entity data in the cache when you call EntityManager#persist(). It alsoputs data in the cache when you load an entity instance from the database. But if youstore or load a large number of entity instances, you may not want to fill up th\u00aee available cache. This\u00ae is especially important for batch procedures, as we showed earlier inthis chapter. You can disable storage of data in the shared entity cache for the entire unit ofwork by setting a CacheStoreMode on the EntityManager:Let\u2019s look at the spe\u00aecial cache mode CacheStoreMode.R\u00aeEFRESH. When you load anentity instance from the database with the default CacheS\u00aetoreMode.USE, Hibernatefirst asks the cache whether it already has the data of the loaded entity instance. Then,if the cache already contains the data, Hibernate doesn\u2019t put the loaded data into thecache. This avoids a cache write, assuming that cache reads are cheaper. With theREFRESH mode, Hibernate always puts load\u00aeed data into the cache without first querying t\u00aehe cache In a cluster with synchronous distributed caching, writing to all cache nodes is usually a very expensive operation. In fact, with a distributed cache, you should set theconfiguration property hibernate.cache.use_minimal_puts to true. This optimizessecond-level cache operation to minimize writes, at the cost of more frequent reads.",
    "page452": "If, however, there is no difference for your cache provider and architecture betweenreads and writes, you may want to disable the additional read with CacheStoreMode.REFRESH. (Note that some cache \u00aeproviders in Hibernate may set use_minimal_puts: for example, with Ehcache this setting is enabled by default.) Cache modes, as you\u2019ve seen, can be set on the find() operation and for theentire EntityManager. You\u00ae can also set cache modes on the refresh() operation andon individual Querys as hints, as discussed in section 14.5. The per-operation and perquery settings override the cache mode of the EntityMana\u00aeger. The cache mode o\u00aenly influences how Hibernate works with the caches internally.Sometimes you want to control the cache system programmatically: for example, toremove data from the cache.This is a simple API, and it only allows you to acce\u00aess cache regions of entity data. Youneed the org.hibernate.Cache API to access the other cache regions, such as the collection and natural identifier cache regions:You\u2019ll rarely need these control mechanisms. Also, note that eviction of\u00ae the secondlevel cache is nontransactional: that is, Hibernate doesn\u2019t lock the cache regions during ev\u00aeiction. Let\u2019s move on to the last part of the Hibernate caching system: the query result cache.The query result cache is by default disabled, and every JPA, criteria, or native SQLquery you write always hits the database first. In this section, we show you why Hibernate disables the query cache by def\u00aeault and then how to enable it for particular queries when needed.",
    "page453": "You have to enable caching for a particular query. Without the org.hibernate.cachable hint, the result won\u2019t be stored in the query result cache. Hibernate executes the SQL query and retrieves the result set into memory. Using the statistics API, you can find out more details. This is the first time you executethis query, so you get a cache miss, not a hit. Hibernate puts the query and its resultinto the cache. If you run the same query again, the res\u00aeult will be from the cache. The entity instance data retrieved in the result set is stored in the entity cache region,not in the query result cache.The org.hibernate.cachable hint is set on the Query API, so it also works for criteriaand native SQL queries. Internally, the cache key is the SQL H\u00aeibernate uses to access thedatabase, with arguments rendered into the string where you had parameter markers. The query result cache doesn\u2019t contain the entire result set of the SQL query. Inthe last example, the SQL result set contained rows from the ITEM table. Hibernateignores most of the information in this result set; only the ID valu\u00aee of each ITEMrecor\u00aed is stored in the query r\u00aeesult cache. The property values of each Item are storedin the entity cache region. Now, when you exec\u00aeute the same query again, with the same argument values forits parameters, Hibernate first accesses the query result cache. It retriev\u00aees the identifier values of the ITEM records from the cache region for query results.",
    "page454": "Then, Hi\u00aebernate looks up and assembles each Item entity instance by identifier from the entitycache region. If you query for entities and decide to enable caching, make sure youalso enable\u00ae regular data caching for these entities. If you don\u2019t, you may end up withmore database hits after enabling the query result cache!If you cache the result of a \u00aequery that doesn\u2019t return entity instances but returnsonly scalar or embeddable values (for example, select i.name from Item i or selectu.homeAddress from User), the val\u00aeues are held in the query result cache regiondirectly. The query result cache uses two physical cache regions:The first cache region is where the \u00aequery results are stored. You should let the cacheprovider expire the most-recently used result s\u00aeets over time, such \u00aethat the cache usesthe available space for recently ex\u00aeecuted queries. The second region, org.hibernate.cache.spi.UpdateTimestampsCache, is special: Hibernate uses this region to decide whether a cached query result set is stale.When you re-execute a query with caching enabled, Hibernate looks in the timestampcache region for the timestamp of the most recent insert, update, or delete made tothe queried table(s). If the timestamp found is later than the timestamp of the cachedquery results\u00ae, Hibernate discards the cached results and issues a new database query.This effectively guarantees that Hibernate won\u2019t use the cached query result if anytable that may be involved in the query contains updated data; hence, the cachedresult may be stale. You should disable expiration of the update timestamp cache sothat the cache provider never removes an element from this cache. The maximumnumber of elements in this cache region depends on the number of tables in y\u00aeourmapped model.",
    "page455": "The majority of queries don\u2019t benefit from result caching. This may come as a surprise. After all, it sounds like avoiding a database hit is always a good thing. There aretwo good reasons this doesn\u2019t always work for arbitrary queries, compared to entityretrieval by identifier or collection initialization. First, you must ask how often you\u2019re going\u00ae to execute the same query repeatedly,with the same arguments. Granted, your application may execute a few queries repeatedly with exactly the same arguments bound to parameters and the same automaticallygenerated SQL statement. We consider this a rare case, but when you\u2019re certain you\u2019reexecuting a query repeatedly, it becomes a good candidate for result set caching. Second, for applications that perform many queries and few inserts, deletes, orupdates, caching query results can improve performance and scalability. On the otherhand, if the application performs many writes, Hibernate won\u2019t use the query resultcache efficiently. Hibernate expires a cached query result set when there is any insert,update, or delete of any row of a table that appears in the cached query result. Thismeans cached results may have a short lifetime, and even if you execute a queryrepeatedly, Hibernate won\u2019t use cached results due to concurrent modifications ofrows in the t\u00aeables referenced by the query. For many queries, the benefit of the query result cache is nonexistent or, at least,doesn\u2019t have the impact you\u2019d expect. But if your query restriction is on a unique natural identifier, such a\u00aes select u from User u where u.username ?, you should consider natural identifier caching and lookup as shown earlier in this chapter.",
    "page456": "So, sure, Maven is an alternative to Ant, but Apache Ant continues to be a great, widely-used tool. Ithas been the reigning champion of Java builds for years, and you can integrate Ant build scripts withyour project\u2019s Maven bu\u00aeild very easily. This is a common usage pattern for a Maven project. On theother hand, as more and more open source projects move to Maven as a project management platform,working developers are starting to realize that Maven not only simplifies the task of build management, itis helping to encourage a common interface between developers and software projects. Maven is more ofa platform than a tool, while you cou\u00aeld consider Maven an alternative to Ant, you are comparing applesto oranges. \"Maven\" includes more than just a build tool.This is the central point that makes all of the Maven vs. Ant, Maven vs. Buildr,\u00ae Maven vs. Gradlearguments irrelevant. Maven isn\u2019t totally defined by the mechanics of your build sy\u00aestem. It isn\u2019t aboutscripting the various tasks in your build as much as it is about encouraging a set of standards, a commoninterface, a life-cycle, a standard repository format, a standard directory layout, etc. It certainly isn\u2019tabout what format the POM happens to be in (XML vs. YAML vs. Ruby). Maven is much larger thanthat, and Maven refers to much more than the tool itself. When this book talks of Maven, it is referring tothe constellation of software, systems, and standards that support it. Buildr, Ivy, Gradle, all of these toolsinteract with the repository format that Maven helped create, and you could just as easily use a repositorymanager like Nexus to support a build written entirely in Ant.",
    "page457": "While Maven is an alternative to many of these tools, the comm\u00aeunity needs to evolve beyond seeingtechnology as a zero-sum game between unfriendly competitors in a competition for users and developers.This might be how large corporations relate to one another, but it has very little rele\u00aevance to the way thatopen source communities work. The headline \"Who\u2019s winning? Ant or Maven?\" isn\u2019t very constructive.If you force us to answer this question, we\u2019re definitely going to say that Maven is a superior alternativeto Ant as a foundational technology for a build; at th\u00aee same time, Maven\u2019s boundaries are constantlyshifting and the Maven community is constantly trying to seek out new ways to become \u00aemore ecumenical,more inter-operable,\u00ae more cooperative. The core tenets of Maven are declarative builds, dependencymanagement, repository managers, universal reuse through plugins, but the specific incarnation of theseideas at any given moment is less important than the sense that the open source community is coll\u00aeaboratingto reduce the inefficiency of \"enterprise-scale builds\".T\u00aehe authors of this book \u00aehave no interest in creating a feud between Apache Ant and Apache Maven,but we are also cognizant of the fact that most organizations have to make a decision between \u00aethe twostandard solutions: Apache Ant and Apache Mav\u00aeen. In this section, we compare and contrast the tools.Ant excels at build process, it is a build system modeled after make with targets and dependencies. Eachtarget consists of a set of instructions which are coded in XML. There is a copy task and a javac taskas well as a jar task. When you use Ant, you supply Ant with specific instructions for compiling andpackaging your output. Look at the following example of a simple build.xml file.",
    "page458": "In this simple Ant example, you can see how you have to tell Ant exactly what to do. There is a compile goal which includes the javac task that compiles the source in the src/main/java directory to thetarget/classes directory. You have to tell Ant exactly where your source is, where you want the resultingbytecode to be stored, and how to package this all into a JAR file. While there are some recent developments that help make Ant less procedural, a developer\u2019s experience with Ant is in coding a procedurallanguage written in XML.Contrast the previous Ant example with a Maven example. In Maven, to create a JAR file from some Javasource, all you need to do is create a simpl\u00aee pom.xml, place your source code in $\u00ae{basedir}/src/main/javaand then run mvn install from the command line. The example Maven pom.xml that achieves thesame resu\u00aelts as the simple Ant f\u00aeile listed in A Simple Ant build.xml file is shown in A Sample Mavenpom.xml.\u00aeThat\u2019s all you need in your pom.xml. Running mvn install from the command line will processresources, compile source, execute unit tests, create a JAR, and install the JAR in a local repository forreuse in other projects. Without modification, you can run mvn site and then find an index.html file intarget/site that contains links to JavaDoc and a few reports abo\u00aeut your source code.",
    "page459": "Admittedly, this is the simplest possible example project containing nothing more than some source codeand producing a simple JAR. It is a project which closely follows Maven conventions and doesn\u2019t requireany dependencies or customization. If we wanted to start customizing the behavior, our pom.xml is goingto grow in size, and in the largest of projects you can see collections of very complex Maven POMs whichcontain a great deal of plugin customization and dependency declarations. But, even when your project\u2019sPOM files become more substan\u00aetial, they hold an entirely different kind of information from the build fileof a similarly sized project using Ant. Maven POMs contain declarations: \"This is a JAR project\", and\"The source code is in src/main/java\". \u00aeAnt build files contain explicit ins\u00aetructions: \"This is project\", \"Thesource is in \u00aesrc/main/java\", \"Run javac against this directory\", \"Put the results in target/classes\", \"Createa JAR from the . . . .\", etc. Where Ant had to be explicit about the process, there was something \"built-in\"to Maven that just knew where the source code was and how it should be processed.Apache Ant Ant doesn\u2019t have formal conventions like a common project directory structure or default behavior. You have to tell Ant exactly where to find the source and where to put the output. Informalconventions have emerged over time, but they haven\u2019t been codified into the product.",
    "page460": "Ant is procedural. You have to tell Ant exactly what to do and when to do it. You have to tell it tocompile, then copy, then compress.Ant doesn\u2019t have a lifecycle. You have to define goals and goal dependencies. You have to attach asequence of tasks to each goal manually.Apache Maven Maven has conventions. It knows where your source code is because you followed the convention.Maven\u2019s Compiler plugin put the bytecode in target/classes, and it produces a JAR file in target. Maven is declarative. All you had to do was create a pom.xml \u00aefile a\u00aend put your source in the defaultdirectory. Maven took care of the rest.Maven has a lifecycle which was invoked when you executed mvn install. This command toldMaven to execute a series of sequential lifecycle phases until it reached the install lifecycle phase. Asa side-effect of this journey through the lifecycle, Maven executed a number of default plugin goalswhich did things like compile and create a JAR.Ma\u00aeven has built-in intelligence about common project tasks in the form of Maven plugins. If you wantedto write and execute unit tests, all you would need to do is write the tests, place them in ${basedir}/src/test/java,add a test-scoped dependency on either TestNG or JUnit, and run mvn test. If you wanted to deploy aweb application and not a JAR, all you would need to do is change your project type to war and put yourdocroot in ${basedir}/src/main/webapp.",
    "page461": "Sure, you can do all of this with Ant, but you will be writingthe instructions from scratch. In Ant, you would first have to figure out where the JUnit JAR file shouldbe. Then you would have to create a classpath that includes the JUnit JAR file. Then you would tellAnt where it should look for test source code, write a goal that compiles the test source to byte\u00aecode, andexecute the unit tests with JUnit.Without supporting technologies like antlibs and Ivy (even with these supporting technologies), Ant hasthe feeling of a c`ustom procedural build. An efficient set of Maven POMs in a project which adheresto Maven\u2019s assumed conventions has surprisingly little XML compared to the Ant alternative. Anotherbenefit of Maven is the reliance \u00aeon widely-shared Maven plugins. Everyone uses the Maven Surefireplugin \u00aefor unit testing, and if someone adds support for a new unit testing framework, you can gain newcapabilities in your own build by just incrementing the version of a particular Maven plugin in yourproject\u2019s POM.The decision to use Maven or Ant isn\u2019t a binary one, and Ant still has a \u00aeplace in a complex build. Ifyour current build contains some highly customized process, or if you\u2019ve written some Ant scripts tocomplete a specific process in a specific way that cannot be adapted to the Maven standards, you can stilluse these scripts with Maven. Ant is made available as a core Maven plugin. Custom Maven plugins canbe implemented in Ant, and Maven p\u00aerojects can be configured to execute Ant scripts within the Mavenproject lifecycle.",
    "page462": "The latest version of Maven currently requires the usage of Java 7 or higher. While older Maven versionscan run on older Java versions, this book assumes that you are \u00aerunning at least Java 7. Go with the mostrecent stable Java Development Kit (JDK) available for your operating system.Maven works with all certified JavaTM compatible developm\u00aeent kits, and a few non-certified implementations of Java. The examples in this book were written and teste\u00aed against the\u00ae official Java DevelopmentKit releases downloaded from the Oracle web\u00ae site.When downloading Maven, you can download the latest available version the latest available version ofMaven 3 in various branches. The latest version of Maven 3 when this book was last updated was Maven3.3.3. If you are not familiar with the Apache Software License, you should familiarize yourself with t\u00aeheterms of the license before you start using the product. More information on the Apache Software Licensecan be found in Section 2.8.There are wide differe\u00aences between op\u00aeerating systems such as Mac OS X and Microsoft Windows, andthere are subtle differences between different versions of Windows. Luckily, the process of installingMaven on all of these operating systems is relatively painless and straightforward. The \u00aefollowing sectionsoutline the recommended best-practice for installing Maven on a variety of operating systems.",
    "page463": "You can download a binary release of Maven from http://maven.apache.org/download.html. Downloadthe current release of \u00aeMaven in a format that is convenient for you to work with. Pick an appropriate placefor it to live, and expand the archive there. If you expanded the archive into the directory /opt/apachemaven-\u00ae3.2.5, you may want to crea\u00aete a symbolic link to make it easier to work with and to avoid the needto change any environment configuration when you upgrade to a newer version:Once Maven is installed, you need to do a couple of things to make it work correctly. You need to add itsbin d\u00aeirectory in the distribution (in this example, /opt/maven/bin) to your command path.Installing Maven on Windows is very similar to installing Maven on Mac OSX, the main differences beingthe installation location and the setting of an environment variabl\u00aee. This book assumes a Maven installation directory of c:\\Program Files\\apache-maven-3.2.5, but it won\u2019t make a difference ifyou install Maven in another directory as long as you configure the proper environment variables. Onceyou\u2019ve unpa\u00aecked M\u00aeaven to the installation directory, you will need to set the PATH environment variable.You can use the following commands:Setting these environment variables on the command-line will allow you to run Maven in your currentsession, but unless you add them to the System environment variables through the control panel, you\u2019llhave to execute these two lines every time you log into your system. You should modify both of thesevariables through the Control Panel in Microsof\u00aet Windows.",
    "page464": "If you\u00ae see this output, you know that Maven is available and ready to be used. If you do not see thisoutput, and your operating system cannot find the mvn command, make sure that your PATH environmentvariable and M\u00ae2_HOME environment variable have been properly set.Maven\u2019s download measures in at roughly 1.5 MiB, it has attained such a slim download size becausethe core of Maven has been designed to retrieve plugins and dependencies from a remote repository ondemand. When you start using Maven, it will start to download plugins t\u00aeo a local repository described inSection 2.5.1. In case you are curious, let\u2019s take a quick look at what is in Maven\u2019s installation directory.LICENSE.txt contains the software license for Apache Maven. This license is described in s\u00aeome detaillater in the section Section 2.8. NOTICE.txt contains some notices a\u00aend attributions required by librariesthat Maven depends on. README.txt contains some installation instructions. bin/ contains the mvn scriptthat executes Maven. boot/ contains a JAR file (classwords-1.1.jar) that is responsible for creating theC\u00aelass Loader in which Maven executes. conf/ contains a global settings.xml that can be used to customizethe behavior of your Maven installation. If you need to customize Maven, it is customary to override anysettings in a settings.xml file stored in ~/.m2. lib/ contains a single JAR file (maven-core-3.0.3-uber.jar)that contains the core of Maven.",
    "page465": "If you\u2019ve installed Maven on a Mac OSX or Unix machine according to the details in Section 2.3.1, itshould be easy to upgrade to newer versions of Maven when they become available. Simply install thenewer version of Maven (/opt/maven-3.future) next to the existing version of Maven (/opt/maven-3.2.5).Then switch the symbolic link /opt/maven from /opt/maven-3.2.5 to /opt/maven-3.future. Since, you\u2019vealready set your M2_HOME variable to point to /opt/maven, you won\u2019t need to cha\u00aenge any environmentvariables.If you have installed Maven on a Windows machine, simply unpack \u00aeMaven to c:\\Program Files\\maven3.future and update your M2_HOME variable.Mos\u00aet of the installation instructions involve unpacking of the Maven distribution archive in a direc\u00aetoryand setting\u00ae of various environment variables. If you need to remov\u00aee Maven from your computer, all youneed to do is delete your Maven installation directory and remove the environment variables. You willalso want to delete the ~/.m2 directory as it contains your local repository.While t\u00aehis book aims to be a comprehensive\u00ae reference, there are going to be topics we will miss andspecial situations and tips \u00aewhich are not covered. While\u00ae the core of Maven is very simple, the real workin Maven happens in the plugins, and there are too many plugins available to cover them all in one book.You are going to encounter problems and features which have not been covered in this book; in thesecases, we suggest searching for answers at the following locations.",
    "page466": "This will be the first place to look, the Maven web site contains a wealth of information and doc-umentation. Every plugin has a few pages of documentation and there are a series of \"quick start\"documents which will be helpful in addition to the content of this book. While the Maven sitecontains a wealth of information, it can also be a frustrating, confusing, and overwhelming. Thereis a custom Google search box on the m\u00aeain Maven page that will search known Maven sites forinformation. This provides better results than a g\u00aeeneric Google search.The Maven User mailing list is the p\u00aelace for users to ask questions. Before you ask a questionon the user mailing list, you will want to search \u00aefor any previous discussion that might relateto your que\u00aestion. It is bad form to ask a question that has already been asked without firstchecking to see \u00aeif an answer already exists in the archives. There are a n\u00aeumber of useful mailing list archive browsers, we\u2019ve found Nabble to the be the most useful. You can browse theUser mailing list archives here: http://www.nabble.com/Maven---Users-f178.html. You canjoin the user mailing list by following the instructions available here http://maven.apache.org/-mail-lists.html.Sonatype maintains an online copy of this book and other tutorials related to Apache Maven.",
    "page467": "Apache Maven is released under the Apache Software License, Version 2.0. If you want to read thislicense, you can read ${M2_HOME}/LICENSE.txt or read this license on the Open Source Initiative\u2019sweb site here: http://www.opensource.org/licenses/apache2.0.php.There\u2019s a good c\u00aehance that, if you are reading this book, you are not a lawyer. If you are wonderingwhat the Apache License, Version 2.0 means, the Apache Software Foundation has assembled a veryhelpful Frequently Asked Questions (FAQ) page about the license available here: http://www.apache.org/-foundation/licence-FAQ.html. Here\u2019s is the answer to the question \"I am not a lawyer. What does it allmean?\"freely download and use Apache so\u00aeftware, in whole or in part, for perso\u00aenal, company internal, orcommercial purposes;use Apache software in packages or distributions that you create.redistribute any piece of Apache-originated software withou\u00aet proper attribution\u00ae; use any marks owned by The Apache Software Foundation in any way that might st\u00aeate \u00aeor imply thatthe Foundation endorses your distribution; use any marks owned by The Apache Software Foundation in any way that might state or imply thatyou created the Apache software in questioni\u00aenclude a copy of the license in any redistribution you may make that includes Apache software;provide clear attribution to The Apache Software Foundation for any distributions \u00aethat include Apachesoftware.in\u00aeclude the source of the Apache software itself, or of any modifications you may have made to it, inany redistribution you may assemble that includes it; submit changes that you make to the software back to the Apache Software Foundation (though suchfeedback is encouraged).",
    "page468": "Maven projects, dependencies, builds, artifacts: all of these are objects to be modeled and described.These objects \u00aeare described by an XML file ca\u00aelled a Project Object Model. The POM tells Maven whatsort of project it is dealing with and how to modify default behavior to generate output from source. In\u00ae thesame way a Java web application has a web.xml that describes, configures, and customizes the application,a Maven project is defined by the presence of a pom.xml. It is a descriptive declaration of a project forMaven; it is the figurative \u201cmap\u201d that Maven needs to understand what it is looking at when it builds yourproject.Y\u00aeou could also think of the pom.xml as analogous to a Makefile or an Ant build.xml. When you are usingGNU make to build something like MySQL, you\u2019ll usually have a file named Makefile that containsexplicit instructions for building a binary from source. When you are using Apache Ant, you l\u00aeikely have afile named build.xml that contains explicit instructions for cleaning, compilin\u00aeg, packaging, and dep\u00aeloyingan application. make, Ant, and Maven are similar in that they rely on the presence of a commonly namedfile such as Makefile, build.xml, or pom.xml, but that \u00aeis where the similarities end. If you look at a Mavenpom.xml, the majority of the POM is going to deal with descriptions: Where is t\u00aehe source cod\u00aee? Where arethe resources? What is the packaging? If you look at an Ant build.xml file, you\u2019ll see something entirelydifferent.",
    "page469": "You\u2019ll see explicit instructions for tasks such as compiling a set of Java classes. The MavenPOM is dec\u00aelarative, and although you can certainly choose to include some procedural customizationsvia the Maven\u00ae Ant plugin, for the most part you will not need to get into the gritty procedural details ofyour project\u2019s build.The POM is also not specific to building Jav\u00aea projects. While most of the examples in this book aregeared towards Java applications, there is nothing Java-specific in the definition of a Maven Project ObjectModel. While Maven\u2019s \u00aedefault plugins are targeted at building JAR artifacts from a set of source, tests, andresources, the\u00aere is nothing preventing you from defining a POM for \u00aea project that contains C# sources andproduces some proprietary Microsoft binary using Microsoft tools. Similarly, there is nothing stoppingyou from defining a POM for a technical book. In fact, the source for this book and this book\u2019s examplesis captured in a multi-module Maven project which uses one of the many Maven Docbook plugins toapply the standard Docbook XSL to a series of chapter XML files. Others have created Maven plugins tobuild Adobe Flex code\u00ae into SWCs and SWFs, and yet others have used Maven to build projects writtenin C.We\u2019ve established that the POM describes and declares, it is unlike Ant or Make in that it doesn\u2019t provideexplicit instructions, and we\u2019ve noted that POM concepts are not specific to Java. Div\u00aeing into morespecifics, take a look at Figure 3.1 for a survey of the contents of a POM.",
    "page470": "This includes a project\u2019s name, the URL for a project, the sponsoring organization, and a list\u00ae ofdevelopers and contributors along with the license for a project.In this section, we customize the behavior of the default Maven build. We\u00ae can change the locationof source and tests, we can add new plugins, we can attach plugin goals to the lifecycle, and we cancustomize the site generation parameters.The build environment consists of profiles that can be activated for use in different environments.For example, during development you may want to deploy to a development server, whereas inp\u00aeroduction you want to deploy to a production server. The build environment customizes the buildsettings for specific environments and is often supplemented by a custom settings.xml in ~/.m2.This settings file is discussed in Chapter 5 and in the section Section 15.2A project rarely stands alone; it depends on other projects, inherits POM \u00aesettings from parentprojects, defines its own coordinates, and may include submodules.Before we dive \u00aeinto some examples of POMs, let\u2019s take a quick look at the Super POM. All Maven projectPOMs extend the Super POM, which defines a set of defaults shared by all projects. This Super POM isa part of the Maven installation. Depending on the Maven version it can be found in the maven-x.y.z-uber.jar or maven-model-builder-xy.z.jar file in ${M2_HOME}/lib. If you look in thisJAR file, you will find a file named pom-4.0.0.xml under the org.apache.maven.model package. Itis also published on the Maven reference site that is available for each version of Maven separately ande.g. for Maven 3.1.1 it can be found with the Maven Model B\u00aeuilder documentation. A \u00aeSuper POM forMaven is shown in The Super POM.",
    "page471": "The Super POM defines some standard configuration variables that are inherited by all projects. Thosevalues are captured in the annotated sections: The default Super POM defines a single remote Maven repository with an ID of centra\u00ael. This isthe Central Repository that all Maven clients are configured to read from by default. This settingcan be overridden by a custom settings.xml file. Note that the default Super POM has disabledsnapshot artifacts on the Central Repository. If you need to use a snapshot repository, you willneed to customize repository settings in you\u00aer pom.xml or in your settings.xml. Settings and profilesare covered in Chapter 5 and in Section 15.2The Central Repos\u00aeitory also contains Maven plugins. The default plugin repository is the centralMaven repository. Snapshots are disabled, and the update policy is set to \u201cnever,\u201d which meansthat Maven will never automatically update a plugin if a new version is released. The build element sets the default values for directories in the Maven Standard Directory layout. Starting in Maven 2.0.9, default versions of core plugins have been provided in the Super POM.This was do\u00aene to provide so\u00aeme stability for users that are not specifying versions in their POMs.In newer versions some of this has been migrated out of the file. However you can still see theversions that will be used in your project using mvn help:effective-pom.",
    "page472": "All Maven POMs inherit defaults from the Super POM (introduced earlier in the section Section 3.2.1).If you\u00ae are just writing a simple project that produces a JAR from \u00aesome source in\u00ae src/main/java, want torun your JUnit tests in src/test/java, and want to build a project site using mvn site, you don\u2019t have tocustomize anything. All you would need, in this case, is the simplest possible POM shown in The SimplestPOM. This POM defines a groupId, artifactId, and version: the three required coordinates forevery projectSuch a simple POM would be more than adequa\u00aete for \u00aea simple project e.g., a Java library that producesa JAR file. It isn\u2019t related to any other projects, it has no dependencies, and\u00ae i\u00aet lacks basic informationsuch as a name and a URL. If you were to create this file and then create the subdirectory src/main/javawith some source code, running mvn package would produce a JAR in target/simple-project-1.jar.Executing the effective-pom goal should print out an XML document capturing the merge betweenthe Super POM and the POM from The Simplest POM.Maven is something of a chameleon; you can pick an\u00aed choose the features you want to take advantage of.Some open source projects may value the ability to list developers and contributors, generate clean projectdocumentation, and manage releases automatically u\u00aesing the Maven Release plugin. On the other hand,someone working in a corporate environment on a small team might not be interested in the distributionma\u00aenagement capabilities of Maven nor the ability to list developers.",
    "page473": "The remainder of th\u00aeis chapter isgoing to discuss features of the POM in isolation. Instead of bombarding you with a 10-page listing of aset of related POMs, we\u2019re going to focus on creating a good reference for specific sections of the POM.In this\u00ae chapter, we discuss relationships between POMs, but we don\u2019t illustrate such a project here.The POM i\u00aes always in a file named pom.xml in the base directory of a Maven project. This XML documentcan start with the XML declaration, or you can choose to omit it. All va\u00aelues in a POM are captured asXML elements.A project\u2019s version numbe\u00aer is used to group and order releases. Maven versions contain the followingparts: major version, minor version, incremental version, and qualifier. In a version, these parts correspond to the following format:For example, the version \"1.3.5\" has a major version of 1, a minor version of 3, and an incremental versionof\u00ae 5. The version \"5\" has a major version of 5 and no minor or incremental version. The qualifier existsto capture milestone builds: alpha and beta releases, and the qualifier is separated from the major, minor,and incremental versions by a hyphen. For example\u00ae, the version \"1.3-beta-01\" has a major version of 1, aminor version of 3, no incremental version and a qualifier of \"beta-01\".Keeping your version numbers aligned with this standard will become very important when you want tostart using version ranges in your POMs. Version ranges, intro\u00aeduced in Section 3.4.3, allow you to specifya dependency on a range of versions, and they are only supported because Maven has the ability to sortversions based on the version release number format introduced in this section.",
    "page474": "If your version re\u00aelease number matches the format <major>.<minor>.<incremental>-<qualifier> then your versions will be compared properly; \"1.2.3\" will be evaluate\u00aed as a more recent build than\"1.0.2\", and the comparison will be made usin\u00aeg the numeric values of the major, minor, and incrementalversions. If your version release number does not fit the standard introduced in this section, then yourversions will be compared as strings; \"1.0.1b\" will be compared to \"1.2.0b\" using a String comparison.One gotcha for release version numbers is the ordering of the qualifiers. Take the version release numbers\u201c1.2.3-alpha-2\u201d and \u201c1.2.3-alpha-10,\u201d where\u00ae the \u201calpha-2\u201d build corresponds to the 2nd alpha build, andthe \u201calpha-10\u201d build corresponds to the 10th al\u00aepha build. Even though \u201calpha-10\u201d should be consideredmore recent than \u201calph\u00aea-2,\u201d Maven is going to sort \u201calpha-10\u201d before \u201calpha-2\u201d due to a known issue inthe way Maven handles version numbers.Maven\u00ae is supposed to treat the number after the\u00ae qualifier as a build number. In other words, the qualifiershould be \"alpha\", and the build number should be 2. Even though Maven has been designed to separatethe build number from the qualifier, this parsing is currently broken. As a result, \"alpha-2\" and \"alpha10\" are co\u00aempared using a String comparison, and \"alpha-10\" comes before \"alpha-2\" alphabetically. Toget ar\u00aeound this limitation, you will need to left-pad your qualified build numbers. If you use \"alpha-02\"and \"alpha-10\" this problem will go away, and it will continue to work once Maven properly parses theversion build number.",
    "page475": "Maven versions can contain a string literal to signify that a project is currently under active development.If a version contains the string \u201c-SNAPSHOT,\u201d then Maven will expand this token to a date and timevalue converted to UTC (Coordinated \u00aeUniversal Time) when you install or\u00ae release this component. Forexample, if your project has a version of \u201c1.0-SNAPSHOT\u201d and you deploy this project\u2019s artifacts to aMaven repository, Maven would expand this version to \u201c1.0-20080207-230803-1\u201d if you were to deploya release at 11:08 PM on February 7th, 2008 UTC. In other words, when you deploy a sna\u00aepshot, you arenot making a release of a software component; you are releasing a snapshot of a component at a specifictime.Why would you use this? SNAPSHOT versions are used for projects under \u00aeactive development. If yourproject depends on a software component that is under active development, you can depend on a SNAPSHOT release, and Maven will periodically attempt to download the latest snapshot from a repositorywhen you run a build. Similarly, if the next release of your system is going to\u00ae have a version \"1.4\", yourproject would have a version \"1.4-SNAPSHOT\" until it was formally released.As a \u00aedefault setting, Maven will not check for SNAPSHOT releases on remote repositories. To dependon SNAPSHOT releases, users must explicitly enable th\u00aee ability to download snapshots using a repository or pluginRepository element in the POM.",
    "page476": "When releasing a project, you should resolve all dependencies on SNAPSHOT versions to dependencies on releas\u00aeed versions. If a project depends on a SNAPSHOT, it is not stable as the dependencies maycha\u00aenge over time. Artifacts published to non-snapshot Maven repositories such as http://repo1.maven.org/-maven2 cannot depend on SNAPSHOT version\u00aes, as Maven\u2019s Super POM has snapshot\u2019s disabled f\u00aeromthe Central repository. SNAPSHOT versions are for development onlyThe syntax for using a property in Maven is to surround the property name with two curly braces andprecede it with a dollar symbol. For example, consider the following POMWhen Maven reads a POM, it replaces references to propert\u00aeies when it loads the POM XML. Mavenproperties occur frequently in advanced Maven usage, and are similar to properties in other systems suchas An\u00aet or Velocity. They are simply variables delimited by ${...}. Maven provides three implicit variableswhich can be used to access environment variables, POM information, and Maven Settings:The env variable exposes environment variables exposed by your operating system or shell. Forexample, a reference to ${env.PATH} in a Maven POM would be replaced by the ${PATH} environment variable (or %PATH% in Windows).The project variable exposes the POM. You can use a dot-notated (.) path to reference the valueo\u00aef a POM element. For example, in this section we used the groupId and artifactId to setthe finalName element in the build configuration. The syn\u00aetax for this property reference was:${project.groupId}-${project.artifactId}.\u00ae",
    "page477": "The settings vari\u00aeable exposes Maven settings i\u00aenformation. You can use a dot-notated (.) pathto reference the value of an element in a settings.xml file. For example, ${settings.offline} wouldreference the value of the offline element in ~/.m2/settings.xml.In addition to the three implicit variables, you can reference system properties and any custom propertiesset in the Maven POM or in a build profile:All properties accessible via getProperties() on java.lang.System are exposed asPOM properties. Some examples of system properties are: ${user.name}, ${user.home}, ${java.home},and ${os.name}. A full list of system properties can be found in the Javadoc for the System classArbitrary properties can be set with a properties element in a pom.xml or settings.xml, orproperties can be loaded from external files. If you set a property\u00ae named fooBar in your pom.xml,that same property is referenced with ${fooBar}. Custom properties come in handy when you arebuilding a system that filters resources and targets different deployment platforms. Here is thesyntax for setting ${foo}bar in a POM:Maven can manage both internal and e\u00aexternal dependencies. An external dependency for a Java projectmight be a library such as Plexus, the Sprin\u00aeg Framework, or Log4J. An internal dependency is illustratedby a web application proj\u00aeect depending on another project that contains service classes, model objects, orpersistence l\u00aeogic. Project Dependencies shows some examples of pro\u00aeject dependencies.",
    "page478": "The first dependency is a compile dependency on the XFire SOAP library from Codehaus. You woulduse this type of dependency if your project depended on this library for compilation, testing, and\u00ae duringexecution. The second dependency is a test-scoped dependency on JUnit. You would use a testscoped dependency when you need to reference this library only during testing. The last dependency inProject Dependencies is a dependency on the Servlet 2.4 API. The last dependency is\u00ae scoped as a provideddependency. You would use a provided scope when the application you are developing needs a library forcompilation and testing, but this library is supplied by a\u00ae container at runtime.briefly introduced three of the five dependency scopes: compile, test, and provided. Scope controls which dependencies are available in which classpath, and which dependenciesare included with an application. Let\u2019s explore each scope in detail.compile is the default scope; all dependencies are compile-scoped if a scope is not supplied.compile dependencies are available in all classpaths, and they are packaged.provided dependencies are used when you expect the JDK or a container to provide them. Fo\u00aerexample, if you were developing a web application, you would need the Servlet API available onthe compile classpath to compile a servlet, but you wouldn\u2019t want to include the Servlet API in thepackaged WAR; the Servlet API J\u00aeAR is supplied by your application server or servle\u00aet container.provided dependencies are available on the compilation classpath (not runtime). They are nottransitive, nor are they packaged.",
    "page479": "runtime dependencies are required to execute and test the system, but they are not required forcompilation. For example, you may need a JDBC API JAR at compile time and the JDBC driverimplementation only at runtime.test-scoped dependencies are not required during the normal operation of an application, andthey are available only during test compilation and execution phasesThe system scope is similar to provided except that you have to provide an explicit path to theJAR on the local file system. This is intended to allow compilation against native objects that maybe part of the system libraries. The artifact is assumed to always be available and is n\u00aeot looked upin a repository. If you declare the scope to be system, you must also provide the systemPathelement. Note that this scope is not recommended (you should always try to reference dependenciesin a \u00aepublic or custom Maven repository).Assume that you are working on a library that provides caching behavior. Instead of writing a cachingsystem from scratch, you want to use some of the existing libraries that provide \u00aecaching on the filesystem and distributed caches. Also assume that you want to give the end user an option to cache onthe file system or to use an in-memory distributed cache. To cache on the file system, you\u2019ll want touse a freely available library called EHCache (http://ehcache.sourceforge.net/), and to cache in a distributed in-memory cache, you want to use another freely available caching library named SwarmCache (http://swarmcache.sourceforge.net/ ).",
    "page480": "You\u2019ll code \u00aean interface and create a library that can be configuredto use either EHCache or SwarmCache, but you want to avoid adding a dependency on both cachinglibrari\u00aees to any projec\u00aet that depends on your library.In other words, you need both libraries to compile this library project, but you don\u2019t want both librariesto show up as transitive runtime dependencies for the project that uses your library. You can accomplishthis by using optional dependencies as shown in Declaring Optional Dependencies.Since you\u2019ve declared these dependencies as optional in my-project, if you\u2019ve defined a project thatdepends on my-project which needs t\u00aehose dependencies, you\u2019ll have to include them explicitly in theproject that depends on my-project. For example, if you were writing an application which\u00ae dependedon my-project and wanted to use the EHCache implementation, you would need to add the followingdependency element to your projectIn an ideal world, you wouldn\u2019t have to use optional dependencies. Instead of having one large projectwith a series of optional dependencies, you would separate the EHCache-specific code to a my-project-ehcache submodule and the SwarmCache-specific code to a my-project-swarmcache submodule. This way, instead of requiring projects that reference my-project to specifically add a depend\u00aeency, projects can just reference a particular implem\u00aeentation project and benefit from the transitivedependency.",
    "page481": "Instead of a specific version for each dependency, you can alternatively specify a range of versions thatwould satisfy a given dependency. For example, you can specify that your project depends on version 3.8or greater of JUnit, or anything between versions 4.5 and 4.10 of JUnit. You do this by surrounding oneor more version numbers with the following characters:For example, if you wished to access any JUnit version greater than or equal to 3.8 but less than 4.0,your dependency would be as shown in Specifying a Dependency Range: JUnit 3.8 - JUnit 4.0A version before or after the comma is not required, and means  /- infinity. For example, \"[4.0,)\" meansany version greater than or equal to 4.0. \"(,2.0)\" is any version less than 2.0. \"[1.2]\" means only versio\u00aen1.2, and nothing else.project-a depends on project-b, which in turn depends on project-c, then project-c isconsidered a transitive dependency of project-a. If\u00ae project-c depended\u00ae on project-d, thenproject-d would also be considered a transitive dependency of project-a. Part of Maven\u2019s appealis that it can manage transitive dependencies and shield the developer from having to keep track of allof the dependencies required to c\u00aeompile and run an application. You can just depend on something likethe Spring Framework and not have to worry about tracking down every last dependency of the SpringFramework.",
    "page482": "Maven accomplishes this by building a graph of dependencies and dealing with any conflicts and overlapsthat might occur. For example, if Maven sees that t\u00aewo projects\u00ae depend on the same groupId and artifactId, it will sort out which dependency to use automatically, always favoring the more recent version ofa dependency. Although this sounds convenient, there are some edge cases where transitive dependenciescan cause some configuration issues. For these scenarios, you can use a depen\u00aedency exclusion.Each of the scopes outlined earlier in the section Section 3.4.1 affects not just the scope of the dependencyin the declaring project, but also how it acts as a transitive dependency. The easiest way to convey thisinformation is through a table, as in Table 3.1. Scopes in the top row represent the scope of a transitivedependency. Scopes in the leftmost column represent the scope of a direct dependency. The intersectionof the row and column is the scope that is assigned to a transitive dependency. A blank cell in this tablemeans that the transitive dependency will be omitted.To illustrate the relationship of transitive dependency scope to direct dependency scope, consider thefollowing example. If project-a contains a test scoped dependency on project-b which contains acompile scoped dependency on project-c. project-c would be a test-scoped transitive dependencyof project-a.You can think of this as a transitive boundary which acts as a filter on dependency scope. Transitivedependencies which are provided and test scope usually do not affect a project. Transitive dependencies which are compile and runtime scoped usually affect a project\u00ae regardless of the scope of a directdependency.",
    "page483": "Transitive dependencies which are compile scoped will have the same scope of the directdependency . Transitive dependencies which are runtime scoped will generally have the same scope ofthe direct dependency except wh\u00aeen the direct dependency has a scope of compile. When a transitive dependency is runtime scoped and the direct dependency is compile scoped, the transitive dep\u00aeendency willhave an effective sc\u00aeope of runtime.There will be times when you need\u00ae to exclude a transitive dependency, such as when you are dependingon a project that depends on another project, but you would like to either exclude the dependency altoge\u00aether or replace the transitive dependency with another dependency that provides the same functionality.Excluding a Transitive Dependency shows an example of a dependency element that adds a dependencyon projec\u00aet-a, but excludes the transitive dependency project-b.Often, you will want to replace a tran\u00aesitive dependency with another implem\u00aeentation. For example, ifyou are depending on a library that depends on the Sun JTA API, you may want to replace the declaredtransitive dependency. Hibernate is one example. Hibernate depends on the Sun JTA API JAR, which isnot available in the central Maven repository because it cannot be freely redistributed. Fortunately, theApache Geronimo project has created an independent implementation of this library that can be freelyredistributed. To replace a transitive dependency with another dependency, you would exclude the transitive dependency and declare a dependency on the project you wanted instead. Excluding and Replacing aTransitive Dependency shows an example of a such replacement.",
    "page484": "In Excluding and Replacing a Transitive Dependency, there is nothing marking the dependency on geronimojta_\u00ae1.1_spec as a replacement, it just happens to be a library which provides the same API as the originalJTA dependency. Here are some other reasons you might want to exclude or replace transitive dependencies:The groupId or artifactId of the artifact has changed, where the current project requires analternately named version from a dependency\u2019s version - resulting in 2 copies of the same project inthe classpath. Normally Maven would capture this conflict and use a single version of the project,but when groupId or arti\u00aefactId are different, Maven will consider this to be two differentlibraries. An artifact is not used in your project and the transitive dependency has not been marked as an optional dependency. In this case, you might want to exclude a dependency because it isn\u2019t somethingyour system needs and you are trying to cut down on the number of libraries distributed wi\u00aeth anapplication. An artifact which is provid\u00aee\u00aed by your runtime container thus should not be included with your build.An example of this is if a dependency depends on something like the Servlet API and you want tomake sure that the dependency is not included in a web application\u2019s WEB-INF/lib directory. To exclude a dependency which might be an API with multiple implementations. This is the situation illustrated by Excluding and Replacing a Transitive Dependency; there is a Sun API whichrequires click-wrap licensing and a time-c\u00aeonsuming manual ins\u00aetall into a custom repository (Sun\u2019s\u00aeJTA JAR) versus a freely distributed version of the same API available in the central Maven repository (Geronimo\u00ae\u2019s JTA implementation).=== Dependency Management.",
    "page485": "Once you\u2019ve adopted Maven at your super complex enterprise and you have two hundred and twentyinter\u00ae-related Maven projects, you are going to start wondering if there is a better way to get a handle ondependency versions. If every single project that uses a dependency like the MySQL Java connector needsto independently list the version number of the dependency, yo\u00aeu are going to run into problems when youneed to\u00ae upgrade to a new version. Because the version numbers are distributed throughout your projecttree, you are going to have to manually edit each of the pom.xml files that reference a depe\u00aendency to makesure that you are changing the version number \u00aeeverywhere. Even with find, xargs, and awk, you arestill running the risk of missing a single POM.Luckily, Maven provides a way for you to consolidate dependency version numbers in the dependencyManagement element. You\u2019ll usually see the dependencyManagement element in a top-level parentPOM for an organization or project. Using the dependencyManagement element in a pom.xml allowsyou to reference a dependency in a child project without having to explicitly list the version. Maven willwalk up the parent-child hierarchy until it finds a project with a dependencyManagement element, itwill then use the version specifi\u00aeed in this dependencyManagement element.For example, if you have a large set of projects which make use of the MySQL Java connector version 5.1.2, you could define the following dependencyManagement element in your multi-\u00aemoduleproject\u2019s top-level POM.",
    "page486": "You should notice that the child project did not have to explicitly list the version of the mysql-connector-java dependency. Because this dependency was defined in the top-level POM\u2019s dependencyManagement element, the version number is going to propagate to the child project\u2019s dependency on mysqlconnector-java. Note that if this child project did define a version, it would override the versionlisted in the top-level POM\u2019s depen\u00aedencyManagement section. That is, the dependencyManagement version is only used wh\u00aeen the child does not declare a version directly.Dependency management in a top-level POM is different from just defining a dependency on a widelyshared parent POM. For starters, all dependencies are inherited. If mysql-connector-java werelisted\u00ae as a dependency of the top-level parent project, every single project in the hierarchy would havea reference to this dependency. Instead of adding in unnecessary dependencies, using dependencyManagement allows you to consolidate and centralize the management of dependency versions withoutadding dependencies which are inherited by all children. In other words, the dependencyManagement element is equivalent to an environment variable which allows you to declare a dependency anywherebelow a project without specifying a version number.One of the compelling reasons to use Maven is that it makes the process of tracking down dependencies(and dependencies\u00ae of dependencies) very easy. When a project depends on an artifact produced by anotherproject we say that th\u00aeis\u00ae artifact is a dependency. In the case of a Java project, this can be as simple as aproject depending on an external dependency like Log4J or JUnit. While dependencies can model externaldependencies, they can also manage the dependencies between a set of related projects.",
    "page487": "If projecta depends on project-b, Maven is smart enough to know that project-b must be built beforeproject-a.Relationships are not only about dependencies and figuring out what one project needs to be able \u00aeto buildan artifact. Maven can model the relationship of a project to a parent, and the relationship of a project tosubmodules. This section gives an over\u00aeview of the various relationships betw\u00aeeen projects and how suchrelationships are configured.Coordinates define a unique location for a project. Projects are r\u00aeelated to one another using MavenCoordinates. project-a doesn\u2019t just depend on project-b; a project with a groupId, artifactId, and version depends on another project with a groupId, artifactId, and version. Toreview, a Maven Coordinate is made u\u00aep of three components:A groupId groups a set of related artifacts. Group identifiers generally resemble a Java packagename. For example, the groupId org.apache.maven is the base groupId for all artifacts produced by the Apache Maven project. Group identifiers are translated into paths in the Maven Repository; for example, the org.apache.maven groupId can be found in /maven2/org/apache/maven onrepo1.maven.org.The artifactId is the proj\u00aeect\u2019s main identifier. When you generate an artifact, this artifact isgoing to be named with the artifactId. When you refer to a project, you are going to refe\u00aerto it using the artifactId. The artifactId, groupId combination must be unique. Inother words, you can\u2019t have two separate projects with the same artifactId and groupId;artifactId s are unique within a particular groupId.",
    "page488": "When an artifact is released, it is released with a version number. This version number is a numericidentifier such as \"1.0\", \"1.1.1\", or \"1.1.2-alpha-01\". You can also use what is known as a snapshotversion. A snapshot version is a version for a co\u00aemponent which is un\u00aeder development, snapshot version numbers always end in SNAPSHOT; for example, \"1.0-SNAPSHOT\", \"1.1.1-SNAPSHOT\",and \"1-SNAPSHOT\". Section 3.3.1.1 introduces versions and version ranges.You would use a classifier if you were releasing the same code but needed to produce two separateartifacts for technical reasons. For example, if you wanted to build two separate artifacts of a JAR,one compiled with the Java 1.4 compiler and another compiled with the Java 6 compiler, you mightuse the classifier to produce two separate JAR artifacts under the same groupId:artifactId:versioncombination. If your project uses native extensions, you might use the classifier to produce anartifact for each target platform. Classifiers are commonly used to package up an artifact\u2019\u00aes sources,JavaDocs or binary assemblies.When we talk of dependencies in this book, we often use the following shorthand notation to describe adependency: groupId:artifactId:version. To re\u00aefer to the 2.5 release of the Spring Framework,we would refer to it as org.springframework:spring:2.5. When you ask Maven to print out alist of dependencies with the Maven Dependency plugin, you will also see that Maven tends to print outlog messages with this shorthand dependency notation.",
    "page489": "There are going to be times when you want a project to inherit values from a parent POM. You mightbe building a large system, and you don\u2019t want to have to repeat the same dependency elements overand over again. You can avoid repeating yourself if your projects make use of inheritance via the parentelement. When a project specifies a parent, it inherits the information in the parent project\u2019s POM. It canthen override and add to the values specified in this parent POM.All Maven POMs inherit values from a parent POM. If a POM does not specify a direct parent usingthe parent element, that POM will inherit values from the Super POM. Project Inheritance shows theparent element of project-a which inherits the POM defined by the a-parent project.Runn\u00aeing mvn help:effective-pom in project-a would show a POM that is the result of merging the Super POM with the POM defined by a-parent and the \u00aePOM defined in project-a. Theimplicit and explicit inheritance relationships for project-a are shown in Figure 3.3.When a project specifies a parent project, Maven uses that parent POM as a starting point before it readsthe current project\u2019s POM. It inherits everything, including the groupId and version number. You\u2019llnotice that project-a does not specify either, both groupId and version are inherited from aparent. With a parent element, all a POM really needs to define is an artifactId. This isn\u2019tmandatory, project-a could have a different groupId and version, but by not providing values,Maven will use the values specified in the parent POM. If you start using Maven to manage and buildlarge multi-module projects, you will often be creating many projects which share a common groupId\u00aeand version.",
    "page490": "When Maven inherits dependencies, it will add dependencies of child projects to the dependencies definedin parent projects. You can use this feature of Maven to specify widely used dependencies across allprojects which inherit from a top-level POM. For example, if your system makes\u00ae universal use of theLog4J logging framework, you can li\u00aest this dependency in your top-level POM. Any projects whichinherit POM information from this project will automatically have Log4J as a dependency. Similarly, ifyou\u00ae need to make sure that every project is using the same version of a Maven plugin, you can list thisMaven plugin version explicitly in a top-level parent POM\u2019s pluginManagement section.Maven assumes that the parent POM is available f\u00aerom the local repository, or available in the parentdirectory (../pom.xml) of the current project. If neither location is valid this default behavior may beoverridden via the relativePath element. For example, some organizations prefer a flat projectstructure where a parent project\u2019s pom.\u00aexml isn\u2019t in the parent directory of a child project. It might be in asibling directory to the project. I\u00aef your child project were in a directory ./project-a and th\u00aee parent projectwere in a directory named ./a-parent, you could specify the relative\u00ae location of parent-a\u2019s POM withthe following configuration:Maven can\u00ae be used to manage everything from simple, single-project systems to builds that involve hundreds of inter-related submodules.",
    "page491": "Part of the learning process with Maven isn\u2019t just figuring out the\u00aesyntax for configuring Maven, it is learning the \"Maven Way\" the current set of best practices for organizing and building projects using Maven. This section attempts to distill some of this knowledge to helpyou adopt best practices from the start without having to wade through years of discussions on the Mavenmailing listsIf you have a set of dependencies which are logically grouped together. You can create a p\u00aeroject withpom packaging that groups dependencies together. For example, let\u2019s assume that your application usesHibernate, a popular Object-Relational mapping framework. Every project which uses Hibernate mightalso have a dependency on the Spring Framework and a MySQL JDBC driver. Instead of having to includethese dependencies in every project that\u00ae uses Hibernate, Spring, and MySQL you could create a specialPOM that does nothing more than declare a set of common dependencies. You could create a projectcalled persistence-deps (short for Persistence Dependencies), and have every project that needs todo persistence depend on this convenience project:If you create this project in a directory named persistence-deps, all you need to do is create thispom.xml and run mvn install. Since the packagin\u00aeg type is pom, this POM is installed in your localrepository. You can now add this project as a dependency and all of its dependencies will be added astransitive dependencies to your project. When you declare a dependency on this persistence-deps project,don\u2019t forget to specify the dependency type as pom.",
    "page492": "If you later decide to switch to a di\u00aefferent JDBC driver (for example, JTDS), just replace the dependenciesin the persistence-deps project to use net.source\u00aeforge.jtds:jtds instead of mysql:mysql-java-connector and update the version number. All projects depending on persistence-deps will use JTDS if they decide to update to the newer version. Consolidating related dependencies is a good way to cut down on the length of pom.xml files that start having to depend on a largenumber of dependencies. If you\u00ae need to share a large number of dependencies between projects, youcould also just establish parent-child relationships between projects and refactor all common dependencies to the parent project, but the disadvantage of the parent-child approach is that a project can have onlyone parent. Sometimes it makes more sense to group similar dependencies together and reference a pom\u00aedependency. This way, your project can reference as many of these consolidated dependency POMs as itneeds.There is a difference between inheriting from a parent \u00aeproject and being managed by a multimoduleproject. A parent project is one that passes its values to its children. A multimodule project simplymanages a group of other subprojects or modules. The multimodule relationship is defined from thetopmost level downwards. When setting u\u00aep a multimodule project, you are simply telling a project that itsbuild should include the specified modules. Multimodule builds are to be used to group modules togetherin a single build. The parent-child\u00ae relationship is defined from the lea\u00aef node upwards\u00ae.",
    "page493": "The parent-childrelationship deals more with the definition of a particular project. When you associate a child with itsparent, you are telling Maven that a project\u2019s POM \u00aeis derived from another.To illustrate the decision process that goes into choosing a design that uses inheritance vs. multi-moduleor both approaches consider the following two examples: the Maven project used to generate this bookand a hypothetical project that contains a number of logically grouped modules.When we build this Maven book you are reading, we run mvn package in a multi-module projectnamed maven-book. This multi-module project includes two submodules: book-examples andbook-chapters. Neither of these projects share the same parent, they are related only in that theyare modules in the maven-book project. book-examples builds the ZIP and TGZ archives youdownloaded to get this book\u2019s example. When we run the book-examples build from book-examples/directory with mvn package, it has no knowledge that it is a part of the larger maven-book proje\u00aect.book-examples doesn\u2019t really care about maven-book, all it knows in life is that its parent is thetop-most sonatype POM and that it creates an archive of examples. In this case, the maven-bookproject\u00ae exists only as a convenience and as an aggregator of modules.Each of the three projects: maven-book, book-examples, and book-chapters all list a shared\"corporate\" parent   sonatype.",
    "page494": "\u00aeThis is a common practice in organizations which have adoptedMaven, instead of having every project extend the Super POM by default, some organizations definea\u00ae top-level corporate POM that serves as the default parent when a project doesn\u2019t have any good reasonto depend on another. In this book example, there is no compelling reason to have book-examplesand book-chapters share the same parent POM, they are entirely different projects which have a different set of dependencies, a different build configuration, and use drastically different plugins to createthe content you are now reading. The sonatype POM gives the organization a chance to customize thedefault behavior of Maven and supply some organization-specific information to configure deploymentsettings and bui\u00aeld profiles.Let\u2019s take a look at an example that provides a more accurate picture of a real-world project where inheritance and multi-module relationships exist side by side. Figure 3.5 shows a collection of projects thatresemble a typical set of pro\u00aejects in an enterprise applicati\u00aeon. There is a top-level POM for the corporation with an artifactId of sonatype. There is a multi-module project named big-system whichreferences sub-modules server-side and client-side.What\u2019s going on here? Let\u2019s try to deconstruct this confusing set of arrows. Fir\u00aest, let\u2019s take a look atbig-system. The big-system might be the project that\u00ae you would run mvn package on to buildand test the entire system. big-system references submodules client-side and server-sid\u00aee.Each of these projects effectively rolls up all of the code that runs on either the server \u00aeor on the client.",
    "page495": "Let\u2019s focus on the server-side proj\u00aeect. Under the server-side project we have a project calledserver-lib and a multi-module project named web-apps. Under web-apps we have two Java webapplications: client-web and admin\u00ae-web.Let\u2019s start with the parent/child relationships from client-web and admin-web to web-apps. Sinceboth of the web applications are implemented in the same web application framework (let\u2019s say Wicket),both projects would share the same set of core dependencies. The dependencies on the Servlet API, theJSP API, and Wicket would all be captured in the web-apps project. Both client-web and admin-web also need to depend on server-lib, this dependency would be defined as a dependency betweenweb-apps and server-lib. Because client-web and admin-web share so much configura\u00aetion by inheriting from web-apps, both client-web and admin-web will have very small POMscontaining little more \u00aethan identifiers, a parent declaration, and a final build name.Next we focus on the parent/child relationship from web-apps and server-lib to server-side.In this case, let\u2019s just assume that there is a separate working group of developers which work on theserver-side code and another group of developers that work on the client-side code. The list of developers would be configured in the server-side POM and inherited by all of the child projects underneath it: web-apps, server-lib, client-web, and admin-web.We could also \u00aeimagine thatthe server-side project might have different build and deployment sett\u00aeings which are unique to thedevelopment for the server side.",
    "page496": "The server-side project might define a build profile that only makessense for all of the server-\u00aeside projects. This build profile might contain the database host and credentials, or the server-side project\u2019s POM might configure a specific version of the Maven Jettyplugin which should be universal across all projects that inherit the server-side POM.In this example, the main reason to use parent/child relationships is shared dependencies and commonconfiguration for a group of projects which are logically related. All of the projects below b\u00aeig-systemare related to one another as submodules, but not all submodules are configured to point back to parentproject that included it as a submodule. Everything is a\u00ae submodule for reasons of convenience, to buildthe entire system just go to the big-system project directory and run mvn package. Look moreclosely at the figure and you\u2019ll see that there is no parent/child relationship between server-side andbig-system. Why is this? POM inheritance is very powerful, but it can be overused. When it makessense to share dependencies and build configuration, a parent/child relationship should be used. Whenit doesn\u2019t make sense is when there are d\u00aeistinct differences between two projects. Take, for example,the server-side and client-side projects. It is possible to create a system where clientside and server-side inherited a common POM from big-system, but as soon as a significantdivergence between the two child projects develops, you then have to figure out creative ways to factorout common build configuration to big-system without affecting all of the children. Even th\u00aeoughclient-side and server-side\u00ae might both depend on Log4J, they also might have distinct pluginconfigurations.",
    "page497": "There\u2019s a certain point defined more by style and experience where you decide that minimal duplicationof configuration is a small price to pay for allowing projects like client-side and server-side toremain completely indepe\u00aend\u00aeent. Designing a huge set of thirty plus projects which all inherit five levelsof POM configuration isn\u2019t always the best idea. In such a setup, you might not have to duplicate yourLog4J depen\u00aedency more than once, but you\u2019ll also end up having to wade through five levels of POMjust figure out how Maven calculated your effective POM. All of this complexity to avoid duplicati\u00aengfive lines of dependency declaration. In Maven, there is a \"Maven Way\", but there are also many waysto accomplish the same thing. It all boils down to preference and style. For the most part, you won\u2019t gowrong if all of your submodules turn out to define back-references to the same project as a parent, butyour use of Maven may evolve over time.Maven models projects as nouns which are described by a POM. The POM c\u00aeaptures the identity of apr\u00aeoject: What does a project contain? What type of packaging a project needs? Does the project havea parent? What are the dependencies? We\u2019ve explored the idea of describing a project in the previouschapters, but we haven\u2019t introduced the mechanism that allows Maven to act upon these objects. In Maventhe \"verbs\" are goals packaged in M\u00aeaven plugins which are tied to a phases in a build lifecycle. A Mavenlifecycle consists of a sequence of named phases: prepare-resources, compile, package, and install amongother.",
    "page498": "There is phase that captures compilation and a phase that captures pac\u00aekaging. There are pre- andpost- phases which can be used to register goals which must run prior to compilation, or tasks w\u00aehich mustbe run after a particular phase. When \u00aeyou tell Maven to build a project, you are telling Maven to stepthrough a defined sequence of phases and execute any goals which may have been registered with eachphase.A build lifecycle is an organized sequen\u00aece of phases that exist to give order to a set of goals. Those goalsare chosen and bound by the packaging type of the project being acted upon. There are three standardlifecycles in Maven: clean, default (sometimes called build) and site. In this chapter, you are going tolearn how Maven ties goals to lifecycle phases and how the lifecycle can be customized. You will alsolearn about the default lif\u00aeecycle phases.The interesting phase in the cle\u00aean lifecycle is the clean phase. The Clean plugin\u2019s clean goal (clean:clean) is bound to the clean phase in the clean lifecycle. The clean:clean goal deletes theoutput of a build by deleting the build directory. If you haven\u2019t customized the location of the builddirectory it will be the ${basedir}/target directory as defined by the Super POM. When you execute theclean:clean goal you do not do so by executing the goal directly with mvn clean:clean, youdo so by executing the clean phase of the clean lifecycle. Executing the clean phase gives Maven anopportunity to execute any other goals which may be bound to the pre-clean phase.",
    "page499": "For example, suppose you wanted to trigger an antrun:run goal task to echo a notification on preclean, or to make an archive of a project\u2019s build directory before it is deleted. Simply running theclean:clean goal will not execute the lifecycle at all, but specifying the clean phase will use theclean lifecycle and advance through the t\u00aehree lifecycle \u00aephases until it reaches the clean phase. Triggering a Goal on pre-clean shows an example of buil\u00aed configuration which binds the antrun:run goalto the pre-clean phase to echo an alert that the project \u00aeartifact is about to be deleted. In this example,the antrun:run goal is being used to execute some arbitrary Ant commands to check for an existingproject artifact. If the project\u2019s artifact is about to be\u00ae \u00aedeleted it will print this to the screenIn addition to configuring Maven to run a goal during the pre-clean phase, you can also customizethe Clean plugin to delete files in addition to the build output directory. You can configure the plugin toremove specific files in a fileSet. The examp\u00aele below configures clean to remove all .class files in adirectory named target-other/ using standard Ant file wildcards: * and \\**.Maven does more than build software artifacts from project, it c\u00aean also generate project documentationand reports about the project, or \u00aea collection of projects. Project documentation and site generation havea dedicated lifecycle which contains four phases.",
    "page500": "The packaging type does\u00ae not usually alter this lifecycle since packaging types are concerned primarilywith artifact creation, not with the type of site generated. The Site plugin k\u00aeicks off the execution of Doxiadocument generation and other report generation plugins. You can generate a site from a Maven projectby running the following command:Th\u00aee specific goals bound to each phase default to a set of goals specific \u00aeto a project\u2019s packaging. A projectwith packaging jar has a different set of default goals from a project with a packaging of war. Thepackaging element affects the steps required to build a project. For an example of how the packagingaffects the build, con\u00aesider two projects: one with pom packaging and t\u00ae\u00aehe other with jar packaging.The project with pom packaging will run the site:attach-descriptor goal during the packagephase, and the project with jar packaging will run the jar:jar goal\u00ae instead.The following sections describe the lifecycle for all built-in packaging types in Maven. Use these sectionsto find out wh\u00aeat default goals are mapped to default lifecycle phasesJAR is the default packaging type, the most common, and thus the most commonly encountered lifecycleconfiguration. The default goals for the JAR lifecycle .POM is the simplest packaging type. The artifact that it generates is it\u00aeself only, rather than a JAR, SAR,or EAR. There is no code to test or compile, and there are no resources the process. The default goals forprojects with POM packaging .",
    "page501": "All s\u00aeoftware problems can be termed as bugs. A software bug usually occurs when the software does not do what it is intended to do or does something that it is not intended to do. Flaws in specifications, design, code or other reasons can cause these bugs. Identifying and fixing bugs in the early stages of th\u00aee software is very important as the cost of fixing bugs grows over time. So, the goal of a software tester is to find bugs and f\u00aeind them as early as possible and make sure they are fixed. Testing is context-based and risk-driven. It requires a methodical and disciplined approach to finding bugs. A good software tester needs to build credibility and possess the attitude to be explorative, troubleshooting, relentless, creative, diplomatic and persuasive. As against the perception that testing starts only after the completion of coding phase, it actually begins even before the first line of code c\u00aean be written. In the life cycle of the conventional software product, test\u00aeing begins at the stage when the specifications are written, i.e. from testing the product specifications or product spec. Finding bugs at this stage can save huge amounts of time and mone\u00aey. Once the specifications are well understood, you are required to design and execute the test cases. Selecting the appropriate technique that reduces the number of tests that cover a feature is one of the most important things that you need to take into consideration while designing these test cases. Test cases need to be designed to cover all aspects of the software, i.e. security, database, functionality (critical and general) and the user interface. Bugs originate when the test cases are executed. As a tester you might have to perform testing under different circumstances, i.e. the application could be in the initial stages or undergoing rapid changes, you have less than enough time\u00ae to test, the product might be developed using a life cycle model that does not support much of formal testing or retesting. Further, testing using different operating systems, browsers and the configurations are to be taken care of. Reporting a bug may be the most important and sometimes the\u00ae most difficult task that you as a software tester will perform. By using various tools and clearly communicating to the developer, you can ensure that the bugs you find are fixed. Using automated tools to execute tests, run scripts and tracking bugs impro\u00aeves efficiency and effectiveness of your tests. Also, keeping pace with the latest developments in the field will augment your career as a software test engineer.",
    "page502": "Software is a series of instructions for the computer that perform a particular task,called a program; the two major categories of software are system software andapplication software. System software is made up of control programs. Applicationsoftware is any program that processes data for the user (spreadsheet, wordprocessor, payroll, etc.).A software product should only be released after it has gone through a properprocess of development, testing and bug fixing. Testing looks at areas such asperformance, stability and error handling by setting up test scenarios undercontrolled conditions and assessing the results. This is why exactly any s\u00aeoftware hasto be tested. It is important to note that software is mainly tested to see that it meetsthe customers\u2019 needs and that it conforms to the standards. It is a usual norm thatsoftware is considered of good quality if it meets the user requirements.Quality can briefly be defined as \u201ca degree of excellence\u201d. High quality softwareusually conforms to the user requirements. A customer\u2019s idea of quality may cover abreadth of features - conformance to specifications, good performance onplatform(s)/configurations, completely meets operational\u00ae requirements (even if notspecified!), compatibility to all the end-user equipment, no negative impact onexisting end-user base at introduction time.Quality software saves good amount of time and money. Because software will havefewer defects, this saves time during testing and maintenance phases. Greaterreliability contributes to an immeasurable increase in customer satisfactio\u00aen as well aslower maintenance costs. Because maintenance represents a large portion of allsoftware costs, the overall cost of the project will most likely be lower than similarprojects.Following are two cases that demonstrate the importance of software quality:Apart from exposing faults (\u201cbugs\u201d) in a software product confirming that theprogram meets the program specification, as a test engineer you need to create testcases, procedures, scripts and generate data. You execute test procedures andscripts, analyze standards and evaluate results of system/integration/regressiontesting. You also... Speed up development process by identifying bugs at an early stage (e.g.specif\u00aeications stage)Reduce the organization's risk of legal liability Maximize the value of the software Assure successful launch of the product, save money, time and reputation ofthe company by discovering bugs and design flaws at an early stage beforefailures occur in production, or in the field Promote continual improvement",
    "page503": "As software engineering is now being considered as a technical engineeringprofession, it is important that the software test engineer\u2019s posses certain traits witha relentless attitude to make them stand out. Here are a few.Know the technology. Knowledge of the technology in which the application isdeveloped is an added advantage to any tester. It helps design better and powerfultest cases basing on the weakness or flaws of the technology. Good testers knowwhat it supports and what it doesn\u2019t, so concentrating on these lines will help thembreak the application quickly. Perfectionist and a realist. Being a perfectionist will help testers spot the problemand being a realist helps know at the end of the day which problems are reallyimportant problems. You will know which ones require a fix and which ones don\u2019t.Tactful, diplomatic and persuasive. Good software testers are tactful and knowhow to break the news to the developers. They are diplomatic while c\u00aeonvincing thedevelopers of the bugs and persuade them when necessary and have their bug(s)fixed. It is important to be critical of the issue and not let the person who developedthe application be taken aback of the findings. An explorer. A bit of creativity and an attitude to take risk helps the testersventure into unknown situations and find bugs that otherwi\u00aese will be looked over.Troubleshoot. Troubleshooting and figuring out why something doesn\u2019t workhelps testers be confident and clear in communicating the defects to the developers. Posses people skills and tenacity. Testers can face a lot of resistance fromprogrammers. Being socially smart and diplomatic doesn't mean being indecisive. Thebest testers are both-socially adept and tenacious where it matters.Organized. Best testers very well realize that they too can make mistakes anddon\u2019t take chances. They are very well organized and have checklists, use files, factsand figures to support their findings that can be used as an evidence and doublecheck their findings.Objective and accurate. They are very objective and know what they report and soconvey impartial and meaningful information that keeps politics and emotions out ofmessage. Reporting inaccurate information is losing a little credibility. Good testersmake sure their find\u00aeings are accurate and reproducible. Defects are valuable. Good testers learn from them. Each defect is an opportunityto learn and improve. A defect found early substantially costs less when compared tothe one found at a later stage. Defects can cause serious problems if not managedproperly. Learning from defects helps \u2013 prevention of future problems, trackimprovements, improve prediction and estimation.",
    "page504": "Testing can\u2019t show that bugs don\u2019t exist. An important reason for testing is toprevent defects. You can perform your tests, find and report bugs, but at no point canyou guarantee that there are no bugs. It is impossible to test a program completely. Unfortunately this is not possibleeven with the simplest program because \u2013 the number of inputs is very large, numberof outputs is very large, number of paths throu\u00aegh the software is very large, and thespecification is subjective to frequent changes. You can\u2019t guarantee quality. As a software tester, you cannot test \u00aeeverything andare not responsible for the q\u00aeuality of the product. The main way that a tester can failis to fail to report accurately a defect you have observed. It is important to rememberthat we seldom have little control over quality. Target environment and intended end user. Anticipating and testing theapplication in the environment user is expected to use is one of the major factors thatshould be considered. Also, considering if the application is a single user system ormulti user system is important for demonstrating the ability for immediate readinesswhen necessary. The error case of Disney\u2019s Lion King illustrates this. Disney Companyreleased its first multimedia CD-ROM game for children, The Lion King AnimatedStorybook. It was highly promoted and the sales were huge. Soon there were reportsthat buyers were unable to get the software to work. It worked on a few systems \u2013likely\u00ae the ones that the Disney programmers used to create the game \u2013 but \u00aenot on themost common systems that the general public used.No application is 100% bug free. It is more reasonable to recognize there arepriorities, which may leave some less critical problems unsolved or unidentified.Simple case is the Intel Pentium bug. Enter the following equation into your PC\u2019scalculator: (4195835 / 3145727) * 3145727 \u2013 4195835. If the answer is zero, yourcomputer is just fine. If you get anything else, you have an old Intel Pentium CPU witha floating-point division bug.Be the customer. Try to use the system as a lay user. To get a glimpse of this, get aperson who has no idea of the application to use it for a while and\u00ae you will be amazedto see the number of problems the person seem to come a\u00aecross. \u00aeAs you can see,there is no procedure involved. Doing this could actually cause the system toencounter an array of unexpected tests \u2013 repetition, stress, load, race etc.",
    "page505": " Build your credibility. Credibility is like quality that includes reliability, knowledge,consistency, reputation, trust, attitude and attention to detail. It is not instant butshould be built over time and gives voice to the testers in the organization. Your keysto build credibility \u2013 identify your strengths and weaknesses, build good relations,demonstrate competency, and be willing to admit mistakes, re-assess and adjust. Test what you observe. It is very important that you test what you can observeand have access to. Writing creative test cases can help only when you have the opportunity to observe the results. So, assume nothing.Not all bugs you find will be fixed. Deciding which bugs will be fixed and whichwon\u2019t is a risk-based dec\u00aeision. Several reasons why your bug might not be fixed iswhen there is no enough time, the bug is dismissed for a new feature, fixing it mightbe very risky or it may not be worth it because it occurs infrequently or has a workaround where the user can prevent or avoid the bug. Making a wrong decision can bedisastrous.Review competitive products. Gaining a good insi\u00aeght into various products of thesame kind and getting to know their functionality and general behavior will help youdesign different test cases and to understand the strengths and weaknesses of yourapplication. This will also enable you to add value and suggest new features andenhancements to your product.Follow standards and processes. As a tester, your need to conform to thestandards and guidelines set by the organization. These standards pertain toreporting hierarchy, coding, documentation, testing, reporting bugs, using automatedtools etc. The software life cycle typically includes the following: requirements analysis, design,coding, testing, installation and maintenance. In between, there can be a requirementto provide Operations and support activities for the product.Requirements Analysis. Software organizations provide solutions to customerrequirements by developing appropriate software that best suits theirspecifications. Thus, the life of software starts with \u00aeorigin of requirements. Veryoften, these requirements are vague, emergent and always subject to change.Analysis is performed to - To conduct in depth analysis of the proposed project, toevaluate for technical feasibility, to discover how to partition the system, to identifywhich areas of the requirements need to be elaborated from the customer, toidentify the impact of changes to the requirements, to identify which requirementsshould be allocated to which components.Design and Specifications. The outcome of requirements analysis is therequirements specification. Using this, the overall design for the intend\u00aeed softwareis developed.Activities in this phase - Perform Architectural Design for the software, DesignDatabase (If applicable), Design User Interfaces, Select or Develop Algorithms (IfApplicable), Perform Detailed Design.",
    "page506": "Coding. The development process tends to run iteratively through these phasesrather than linearly; several models (spiral, waterfall etc.) have been proposed todescribe this process.Activities in this phase - Create Test Data, Create Source, Generate Object Code,Create Operating Documentation,\u00ae Plan Integration, Perform IntegrationTesting. The process of using the developed system with the intent to find errors.Defects/flaws/bugs found at this stage will be sent back to the developer for a fixand have to be re-tested. This phase is iterative as long as the bugs are fixed to meetthe requirements.Activities in this phase - Plan Verification and Validation, Execute Verification andvalidation Tasks, Collect and Analyze Metric Data, Plan Testing, Develop TestRequirements, Execute TestsInstallation. The so developed and tested software will finally need to be installed atthe client place. Careful planning has to be done to avoid problems to the user afterinstallation is done Activities in this phase - Plan Insta\u00aellation, Distribution of Software, Installation ofSoftware, Accept Software in Operational Environment.Operation and Support. Support activities are usually performed by theorganization that developed the software. Both the parties usually decide on theseactivities before the system is developed.Activities in this phase - Operate the System, Pr\u00aeovide Technical Assistance andConsulting, Maintain Support Request Log.Maintenance. The process does not stop once it is completely implemented andinstalled at user place; this phase undertakes development of new features,enhancements etc.Activities in this ph\u00aease - Reapplying Software Life Cycle.The way you approach a particular application for testing greatly depends on the lifecycle model it follows. This is because, each life cycle model places emphasis ondifferent aspects of the software i.e. certain models provide good scope and time fortesting whereas some others don\u2019t. So, the number of test cases developed, featurescovered, time spent on each issue depends on the life cycle model the applicationfollows.No matter what the life cycle model is, every application undergoes t\u00aehe same phasesdescribed above as its life cycle.Software Testing Lif\u00aee Cycle consist of six (generic) phases: 1) Planning, 2) Analysis, 3)Design, 4) Construction, 5) Testing Cycles, 6) Final Testing and Implementation and 7)Post Implementation. Each phase in the life cycle is described with the respectiveactivities.Planning. Planning High Level Test plan, QA plan (quality goals), identify \u2013 reportingprocedures, problem classification, acceptance criteria, databases for testing,measurement criteria (defect quantities/severity level and defect origin), projectmetrics and finally begin the schedule for project testing. Also, plan to maintain all test cases (manual or automated) in a database.",
    "page507": "Analysis. Involves activities that - develop functional validation based on BusinessRequirements (writing test cases basing on these details), develop test case format(time estimates and priority assignments), develop test cycles (matrices andtimelines), identify test cases to be automated (if applicable), define area of stressand performance testing, plan the test cycles required for the project and regressiontesting, define procedures for data maintenance (backup, restore, validation), reviewdocumentation.Design. Activities in the design phase - Revise test plan based on changes, revise testcycle matrices and timelines, verify that test plan and cases are in a database orrequisite, continue to write test cases and add new ones based on changes, developRisk Assessment Criteria, formalize details for Stress and Performance testing, finalizetest cycles (number of test case per cycle based on time estimates per\u00ae test case andpriority), finalize the Test Plan, (estimate resources to support development in unittesting).Constru\u00aection (Unit Testing Phase). Complete all plans, complete Test Cycle matricesand timeline\u00aes, complete all test cases (manual), begin Stress and Performance testing,test the automated testing system and fix bugs, (support developmen\u00aet in \u00aeunittesting), run QA acceptance test suite to certify software is ready to turn over to QA.Test Cycle(s) / Bug Fixes (Re-Testing/System Testing Phase). Run the test cases (frontand back end), bug reporting, verification, and revise/add test cases as required.Final Testing and Implementation (Code Freeze Phase). Execution of all front end testcases - manual and automated, execution of all back end test cases - manual andautomated, execute all Stress and Performance tests, provide on-going defecttracking metrics, provide on-going complexity and design metrics, update estimatesfor test cases and test plans, document test cycles, regression testing, and updateaccordingly.Post Implementation. Post implementation evaluation meeting can be conducted toreview entire project. Activities in this phase - Prepare final Defect Report andassociated metrics, identify strategies to prevent similar problems in future project,automation team - 1) Review test cases to evaluate other cases to be automated forregression testing, 2) Clean up automated test cases and variables, and 3) Reviewprocess of integrating results from automated testing in with results from\u00ae manualtesting.A software bug may be defined as a coding error that causes an unexpected defect,fault, flaw, or imperfection in a computer program. In other words, if a program doesnot perform as intended, it is most likely a bug.There are bugs in software due to unclear or constantly changing requirements,software complexity, programming errors, timelines, errors in bug tracking,c\u00aeommunication gap, documentation errors, deviation from standards etc. Unclear software requirements are due to miscommunication as to what thesoftware should or shouldn\u2019t do. In many occasions, the customer may not becompletely clear as to how the product should ultimately function. This is especiallytrue when the software is a developed for a completely new product. Such casesusually lead to a lot of misinterpretations from any or both sides.",
    "page508": "Constantly changing software requirements cause a lot of confusion and pressureboth on the development and testing teams. Often, a new feature added or existingfeature removed can be linked to the other modules or components in the software.Overlooking such issues causes bugs. Also, fixing a bug in one part/component of the software might arise another in adifferent or same component. Lack of foresight in anticipating such issues can causeserious problems and increase in bug count. This is one of the major issues because ofwhich bugs occur since developers are very often subject to pressure related totimelines; frequently changing requirements, increase in the number of bugs etc. Designing and re-designing, UI interfaces, integration of modules, d\u00aeatabasemanagement all these add to the complexity of the software and the system as awhole. Fundamental problems with software design and architecture can cause problemsin programming. Developed software is prone to error as programmers can makemistakes\u00ae too. As a tester you can check for, data reference/declaration errors, controlflow errors, parameter errors, input/output errors etc. Rescheduling of resources, re-doing or discarding already completed work,changes i\u00aen hardware/software requirements can affect the software too. Assigning anew developer to the project in midway can cause bugs. This is possible if propercoding standards have not been followed, improper code documentation, ineffectiveknowledge transfer etc. Discarding \u00aea portion of the existing code might just leave itstrail behind in other parts of the software; overlooking or not eliminating such codecan cause bugs. Serious bugs can especially occur with larger projects, as it getstougher to identify the problem area. Programmers usually tend to rush as the deadline approaches closer. This is thetime when most of the bugs occur. It is possible that you will be able to spot bugs ofall types and severity.Complexity in keeping track of all the bugs can again cause bugs by itself. This getsharder when a bug has a very complex life cycle i.e. when the number of times it hasbeen closed, re-opened, not accepted, ignored etc.  goes on increasing.Bug Life Cycle st\u00aearts with an unintentional software bug/behavior and ends when theassigned developer fixes the bug. A bug when found should be communicated andassigned to a developer that can fix it. Once fixed, the problem area should be retested. Also, confirmation should be made to verify if the fix did not create problemselsewhere. In most of the cases, the life cycle gets very complicated and difficult totrack making it imperative to have a bug/defect tracking system in place.See Chapter 7 \u2013 Defect TrackingFollowing are the different phases of a Bug Life Cycle:Open: A bug is in Open state when a tester identifies a problem areaAccepted: The bug is then assigned to a developer\u00ae for a fix. The developer thenaccepts if valid.",
    "page509": "Not Accepted/Won\u2019t fix: If the developer considers the bug as low level or does notaccept it as a bug, thus pushing i\u00aet into Not Accepted/Won\u2019t fix state.Such bugs will be assigned to the \u00aeproject manager who will decide if the bug needs afix. If it needs, then assigns it back to the developer, and if it doesn\u2019t, then assigns itback to the tester who will have to close the bug.Pending: A bug accepted by the developer may not be fixed immediately. In suchcases, it can be put under Pending state.Fixed: Prog\u00aerammer will fix the bug and resolves it as Fixed.Close: The fixed bug will be assigned to the tester who will put it in the Close state.Re-Open: Fixed bugs can be re-opened by the testers in case the fix producesproblems elsewhere\u00ae.Costs are logarithmic; they increase in size tenfold as the time increases. A bug founda\u00aend fixed during the early stages \u2013 requirements or product spec stage can be fixed bya brief interaction with the concer\u00aened and might cost next to nothing.During coding, a swiftly spotted mistake may take only very less effort to fix. Duringintegration testing, it costs the paperwork of a bug report and a formally documentedfix, as well as the delay and expense of a re-test.During system testing it costs even more time and\u00ae may delay delivery. Finally, duringoperations it may cause anything from a nuisance to a system failure, possibly withcatastrophic consequences in a safety-critical system such as an aircraft or anemergency service.It is difficult to determine when exactly to stop testing. Here are a few commonfactors that help you decide when you can stop or reduce testing: Deadlines (release deadlines, testing deadlines, etc.) Test cases completed with certain percentage passed Test budget depleted Coverage of code/functionality/requirements reaches a specified point Bug rate falls below a certain level Beta or alpha testing period ends \u00aeThere are basically three levels of testing i.e. Unit Testing, Integration Testing andSystem Testing. Various types of testing come under these levels.Unit TestingTo verify a single program or a section of a single programIntegration TestingTo verify interaction between system componentsPrerequisite:\u00ae unit testing completed on all components that compose a system.System TestingTo verify and validate behaviors of the entire system against the original systemobjectives.",
    "page510": "Software testing is a process that identifies the correctness, completeness, andquality of software.Following is a list of various types of software testing and their definitions in arandom order: Formal Testing: Performed by test engineers Informal Testing: Performed by the developers Manual Testing: That part of software testing that requires human input, analysis,or evaluation. Automated Testing: Software testing that utilizes a variety of tools to automatethe testing process. Automated testing still requires a skilled quality assuranceprofessional with knowledge of the automation tools and the software being testedto set up the test cases. Black box Testing: Testing software without any knowledge of the back-end of thesystem, structure or language of the module being tested. Bla\u00aeck box test cases arewritten from a definitive source document, such as a sp\u00aeecification or requirementsdocument. White box Testing: Testing in which the software tester has knowledge of theback-end, structure and language of the software, or at least its purpose. Unit Testing: Unit testing is the process of testing a particular complied program,i.e., a window, a report, an interface, etc. independently as a stand-alonecomponent/program. The types and degrees of unit tests can vary among modifiedand newly created programs.\u00ae Unit testing is mostly performed by the programmerswho are also responsible for the creation of the necessary unit test data. Incremental Testing: Incremental testing is partial testing of an incompleteproduct. The goal of incremental testing is to provide an early feedback to softwaredevelopers. System Testing: System testing is a form of black box testing. The purpose ofsystem testing is to validate an application's accuracy and completeness inperforming the functions as designed.Integration Testing: Testing two or mor\u00aee modules or functions together with theintent of finding interface defects between the modules/functions. System Integration Testing: Testing of software components that have beendistributed across multiple platforms (e.g., client, web server, application server, anddatabase server) to produce failures caused by system integration defects (i.e. defectsinvolving distribution and back- Functional Testing: Verifying that a module functions as stated in the specificationand establishing confidence that a program does what it is supposed to do. End-to-end Testing: Similar to system testing - testing a complete application in asituation that mimics real world use, such as interacting with a database, usingnetwork communication, or interacting with other hardware, application, or system. Sanity Testing: Sanity testing is performed whenever cursory testing is sufficient toprove the application is functioning according to specifications. This level of testing isa subset of regression testing. It normally includes testing basic GUI functionality todemonstrate connectivity to the database, application servers, printers, etc. Regression Testing: Testing with the intent of determining if bug fixes have beensuccessful and have not created any new problems.",
    "page511": "Acceptance Testing: Testing the system with the intent of confirming readiness ofthe product and customer acceptance. Also known as User Acceptance Testing. Adhoc Testing: Testing without a formal test plan or outside of a\u00ae test plan. Withsome projects this type of testing is carried out as an addition to formal testing.Sometimes, if testing occurs very late in the development cycle, this will be the onlykind of testi\u00aeng that can be performed \u2013 usually done by skilled testers. Sometimes adhoc testing is referr\u00aeed to as exploratory testing. Configuration Testing: Testing to determine how well the product works with abroad range of hardware/peripheral equipment configurations as well as on differentoperating systems and software. Load Testing: Testing with the intent of determining how well the product handlescompetition for system resources. The competition may come in the form of networktraffic, CPU utilization or memory allocation. Stress Testing: Testing done to evaluate the behavior when the system is pushedbeyond the breaking point. The goal is to expose the weak links and to determine ifthe system manages to recover gracefully. Performance Testing: Testing with the intent of determining how efficiently aproduct handles a variety of events. Automated test tools geared specifically to testand fine-tune performance are used most often for this type of testing. Usability Testing: Usability testing is testing for 'user-friendliness'. A way toevaluate and measure how users interact with a software product or site. Tasks aregiven to users and observations are made. Installation Testing: Testing with the intent of determining if the product iscompatible with a variety of platforms and how easily it installs. Recovery/Error Testing: Testing how well a system recovers from crashes,hardware failures, or other catastrophic problems.\u2022 Security Testing: Testing of database and network software in order to keepcompany data and resources secure from mistaken/accidental users, hackers, andother malevolent attackers. Penetration Testing: Penetration testing is testing how well the system isprotected against unauthorized internal or external access, or willful damage. Thistype of testing usually requires sophisticated testing techniques.Compatibility Testing: Testing used to determine whether other system softwarecomponents such as browsers, utilities, and competing software will conflict with thesoftware being tested. Exploratory Testing: Any testing in which the tester dynamically changes whatthey're doing for test execution, based on information they learn as they're executingtheir tests. Comparison Testing: Testing that compares software weaknesses and strengths tothose of competitors' products. Alpha Testing: Testing after code is mostly complete or contains most of thefunctionality and prior to reaching customers. Sometimes a selected group of usersare involved. More often this testing will be performed in-house or by an outsidetesting firm in close cooperation with the software engineering department.Beta Testing: Testing after the product is code complete. Betas are often widelydistributed or even distributed to the public at large.",
    "page512": "Following are the most common software errors that aid you in software testing. Thishelps you to identify errors systematically and increases the efficiency andproductivity of software testing.User Interface Errors: Missing/Wrong Functions Doesn\u2019t do what the user expects,Missing information, Misleading, Confusing information, Wrong content in Help text,Inappropriate error messages. Performance issues - Poor responsiveness, Can'tredirect output, Inappropriate use of key board Error Handling: Inadequate - protection against corrupted data, tests of userinput, version control; Ignores \u2013 o\u00aeverflow, data comparison, Error recovery \u2013 abortingerrors, recovery from hardware problems. Boundary related errors: Boundaries in loop, space, time, memory, mishandling ofcases outside boundary. Calculation errors: Bad Logic, Bad Arithmetic, Outdated constants, Calculationerrors, incorrect conversion from one data representation to another, Wrongformula, incorrect approximation. Initial and Later states: Failure to - set data item to zero, to initialize a loopcontrol variable, or re-initialize a pointer, to clear a string or flag, Incorrectinitialization. Control flow errors: Wrong returning state assumed, Exception handling basedexits, Stack underflow/overflow, Failure to block or un-block interrupts, Comparisonsometimes yields wrong result, Missing/wrong default, and Data Type errors. Errors in Handling or Interpreting Data: Un-terminated null strings, overwriting afile after an error exit or user abort. Race Conditions: Assumption that one event or task finished before anothe\u00aerbegins, Resource races, Tasks starts before i\u00aets prerequisites are met, Messages crosor don't arrive in the order sent.Load Conditions: Required resources are not available, No available large memoryarea, Low priority tasks not put off, doesn\u2019t erase old files from mass storage, anddoesn\u2019t return unused memory.Hardware: Wrong Device, Device unavailable, Underutilizing device intelligence,Misunderstood status or return code, Wrong operation or instruction codes.\u2022 Source, Version and ID Control: No Title or version ID, Failure to update\u00ae mu\u00aeltiplecopies of data or program files.Testing Errors: Failure to notice/report a problem, Failure to use the mostpromising test case, Corrupted data files, Misinterpreted specification\u00aes ordocumentation, Failur\u00aee to make it clear how to reproduce the problem, Failure tocheck for unresolved problems just before release, Failure to verify fixes, Failure toprovide summary report.",
    "page513": "Test Policy - A document characterizing the organization\u2019s philosophy towardssoftware testing.Test Strategy - A high-level document defining the test phases to be performed andthe testing within those phases for a program. It defines the process to be followed ineach project. This sets the standards for the processes, documents, activities etc. thatshould be followed for each project.For example, if a product is given for testing, you should decide if it is better to useblack-box testing or white-box testing and if you decide to use both, when will youapply each and to \u00aewhich part of the software? All these details need to be specified inthe Test Strategy.Project Test Plan - a document defining the test phases to be performed and thetesting within those phases for a particular project.A Test Strategy should cover more than one project and should address the followingissues: An approach to testing high risk areas first, Planning for testing, How toimprove the process based on previous testing, Environments/data used, Testmanagement - Configuration management, Problem management, What Metrics arefollowed, Will the tests be automated and if so which tools will be used, What are the\u00aeTesting Stages and Testing Methods, Post Testing Review process, Templates.Test planning needs to start as soon as the project requirements are known. The firstdocument that needs to be produced then is the Test Strategy/Testing Approach thatsets the high level approach for testing and covers all the other elements mentionedabove.Once the approach is understood, a detailed test plan can be written. Usually, thistest plan can be written in different styles. Test plans can completely differ fromproject to project in the same organization.PurposeTo describe the scope, approach, resources, and schedule of the testing activities. Toidentify the items being tested, the features to be \u00aetested, the testing tasks to beperformed, the personnel responsible for each task, and the risks associated with thisplan.OUTLINEA test plan shall have the following structure: Test plan identifier. A unique identifier assign to the test plan. Introduction: Summarized the software items and features to be tested andthe need for them to be included. Test items: Identify the test items, their transmittal\u00ae media which impact their Features to be tested Features not to be testedApproachItem pass/fail criteriaSuspension criteria and resumption requirementsTest deliverablesTesting tasksEnvironmental needsResponsibilitiesStaffing and training needsSchedule Risks and contingencies Approvals",
    "page514": "Like any other process in software testing, the major tasks in test planning are to \u2013Develop Test Strategy, Critical Success Factors, Define Test Objectives, IdentifyNeeded Test Resources, Plan Test Environment, Define Test Procedures, IdentifyFunctions To Be Tested, Identify Interfaces With Other Systems or Components, WriteTest Scripts, Define Test Cases, Design Test Data, Build Test Matrix, Determine TestSchedules, Assemble Information, Finalize the Plan.A test case is a detailed procedure that fully tests a feature or an aspect of a feature.While the test plan describes what to test, a test case describes how to perfo\u00aerm aparticular test. You need to develop test cases for each test listed in the test planAs a tester, the best way to determine the compliance of the software torequirements is by designing effective test cases that provide a thorough test of aunit. Various test case design techniques enable the testers to develop effective testcases. Besides, implementing the design techniques, every tester needs to keep inmind general gu\u00aeidelines that will aid in test case design:a. The purpose of each test case is to run the test in the simplest way possible.[Suitable techniques - Specification derived tests, Equivalence partitioning]b. Concentrate initially on positive testing i.e. the test case should show that thesoftware does what it is intended to do. [Suitable techniques - \u00aeSpecification derivedtests, Equivalence partitioning, State-transition testing]c. Existing test cases\u00ae should be enhanced and further test cases should be designedto show that the software does not do anything that it is not specified to do i.e.Negative Testing [Suitable techniques - Error guessing, Boundary value analysis,Internal boundary value testing, State-transition testing]d. Where appropriate, test cases should be designed to address issues such asperformance, safety requirements and security requirements [Suit\u00aeable techniques -Specification derived tests]e. Further test cases can then be added to the unit test specification to achievespecific test coverage objectives. Once coverage tests have been designed, the testprocedure can be developed and the tests executed [Suitable techniques - Branchtesting, Condition testing, Data definition-use testing, State-transition testing]The \u00aemanner in which a test case is depicted varies between organizations. Anyhow,many test case templates are in the form of a table, for example, a 5-column tablewith fields:The test case design techniques are broadly gr\u00aeouped into two categories: Black boxtechniques, White box techniques and other techniques that do not fall under eithercategorySpecification Derived TestsAs the name suggests, test cases are designed by walking through the relevantspecifications. It is a positive test case design technique.Equivalence PartitioningEquivalence partitioning is the process of taking all of the possible test values andplacing them into classes (partitions or groups). Test cases should be designed to testone value from each class. Thereby, it uses fewest test cases to cover the maximuminput requirements.",
    "page515": "For example, if a prog\u00aeram accepts integer values only from 1 to 10. The possible testcases for such a program would be the range of all integers. In such a program, al\u00aelintegers up to 0 and above 10 will cause an error. So, it is reasonable to assume that if11 will fail, all values above it will fail and vice versa.If an input condition is a range of values, let one valid equivalence class is the range (0or 10 in this example). Let the values below and above the range be two respectiveinvalid equivalence values (i.e. -1 and 11). Therefore, the above three partition valuescan be used as test cases for the above \u00aeexample.Boundary Value AnalysisThis is a selection technique where the test data are chosen to lie along theboundaries of the input domain or the output range. This technique is often called asstress testing and incorporates a degree of negative testing in the test design byanticipating \u00aethat errors will occur at or around the partition boundaries.For example, a field is required to accept amounts of money between $0 and $10. Asa tester, you need to check if it means up to and including $10 and $9.99 and if $10 isacceptable. So, the boundary valu\u00aees are $0, $0.01, $9.99 and $10.Now, the following tests can be executed. A negative value should be rejected, 0should be accepted (this is on the boundary), $0.0\u00ae1 and $9.99 should be accepted,null and $10 should be rejected. In this way, it uses the same concept of partitions asequivalence partitioning.State Transition TestingAs t\u00aehe name suggests, test cases are designed to t\u00aeest the transition between thestates by creating the events that cause the transition.Branch TestingIn branch testing, test cases are designed to exercise control flow branches ordecision points in a unit. This is usually aimed at achieving a target level of DecisionCoverage. Branch Coverage, need to test both branches of IF and ELSE. All branchesand compound conditions (e.g. loops and array handling) within the branch should beexercised at least once.Conditi\u00aeon TestingThe object of condition testing is to design test cases to show that the individualcomponents of logical conditions and combinations of the individual components arecorrect. Test cases are designed to test the individual elements of logical expressions,both within branch conditions and within other expressions in a unit.Data Definition \u2013 Use TestingData definition-use testing designs test cases to test pairs of data definitions anduses. Data definition is anywhere that the value of a data item is set. Data use isanywhere that a data item is read or used. The objective is to create test cases thatwill drive execution through paths between specific definitions and uses.",
    "page516": "Internal Boundary Value TestingIn many cases, partitions and their boundaries can be identified from a functionalspecification for a unit, as described under equivalence partitioning and boundaryvalue analysis above. However, a unit may also have internal boundary values thatcan only be identified from a structural specification.Error GuessingIt is a test case design technique where the testers use their experience to guess thepossible errors that might occur and design test cases accordingly to uncover them.Using any or a combination of the above described test case design techniques; youcan develop effective test cases.A use case describes the system\u2019s behavior under various conditions as it responds toa request from one of the users. The user initiates an interaction with the system toaccomplish some goal. Different sequences of behavior, or scenarios, can unfold,depending on the particular requests made and conditions surrounding the requests.The use case collects together those different scenarios.Use cases are popular largely because they tell coherent stories about how thesystem will behave in use. The users of the system get to see just what this newsystem will be and get to react early.As discussed earlier, defect is the variance from a desired product attribute (it can bea wrong, missing or extra data). It can be of two types \u2013 Defect from the\u00ae product or avariance from customer/user expectations. It is a flaw in the software system \u00aeand hasno impact until it affects the user/customer and operational system.With the knowledge of testing so far gained, you can now be able to categorize thedefects you have found. Defects can be categorized into different types basing on thecore issues \u00aethey address. Some defects address security or database issues whileothers may refer to functionality or UI issues.Security Defects: Application security defects generally involve improper handling ofdata sent from the user to the application. These defects are the most severe andgiven highest priority for a fix.Examples:Authentication: Accepting an invalid username/password Authorization: Accessibility to pages though permission not givenData Quality/Database Defects: Deals with improper handling of data in thedataba\u00aese.Examples: Values not deleted/inserted into the database properly Improper/wrong/null values inserted in place of the actual valuesCritical Functionality Defects: The occurrence of these bugs hampers the crucialfunctionality of the application.Examples:ExceptionsFunctionality Defects: These defects affect the functionality of the application.Examples: All Javascript errors Buttons like Save, Delete, Cancel n\u00aeot performing their intended functionsA missing fun\u00aectionality (or) a feature not functioning the way it is intended toContinuous execution of loops",
    "page517": "User Interface Defects: As the name suggests, the bugs deal with problems related toUI are usually considered less severe.Examples:Improper error/warning/UI messages Spelling mistakes Alignment problemsOnce the test cases are developed using the appropriate techniques, they areexecuted which is when the bugs occur. It is very important that these bugs bereported as soon as possible because, the earlier you report a bug, the more timere\u00aemains in the schedule to get it fixed.Simple example is that you report a wrong functionality documented in the Help file afew months before the product release, the chances that it will be fixed are very high.If you report the same bug few hours before the release, the odds are that it won\u2019t befixed. The bug is still the same though you report it few months or few hours beforethe release, but wh\u00aeat matters is the time.It is not just enough to find the bugs; these should also be reported/communicatedclearly and efficiently, not to mention the number of people who will be reading thedefect.Defect tracking tools (also known as bug tracking tools, issue tracking tools orproblem trackers) greatly aid the tes\u00aeters in reporting and tracking the bugsfound in software applications. They provide a means of consolidating a keyelement of project information in one place. Project managers can then seewhich bugs have been fixed, which are outstanding and how long it is taking tofix defects. Senior management can use reports to understand the state of thedevelopment process.You should provide enough detail while reporting the bug keeping in mind the peoplewho w\u00aeill use it \u2013 test lead, developer, project manager, other testers, new testersassigned etc. This means that the\u00ae report you will write should be concise, straight andclear. Following are the details your report should contain:Bug Title Bug identifier (number, ID, etc.) The application name or identifier and version The function, module, feature, object, screen, \u00aeetc. where the bug occurredEnvironment (OS, Browser and its version)-Bug Type or Category/Severity/Priorityo Bu\u00aeg Category\u00ae: Security, Database, Functionality (Critical/General), UIo Bug Severity: Severity with which the bug affects the application \u2013 Very High,High, Medium, Low, Very Lowo Bug Priority: Recommended priority to be given for a fix of this bug \u2013 P0, P1,P2, P3, P4, P5 (P0-Highest, P5-Lowest) Bug status (Open, Pending, Fixed, Closed, Re-Open) Test case name/number/identifierBug descripti\u00aeonSteps to ReproduceActual ResultTester Comments",
    "page518": "Once the reported defect is fixed, the tester needs to re-test to confirm the fix. This isusually done by executing the possible scenarios where the bug can occur. Onceretesting is completed, the fix can be confirmed and the bug can be closed. Thismarks the end of the bug life cycle.The documents outlined in the IEEE Standard of Software Test Documentation coverstest planning, test specification, and test reporting.Test reporting covers four document types: A Test Item \u00aeTransmittal Report identifies the test items being transmitted fortesting from the development to the testing group in the event that a formalbeginning of test execution is desiredDetails to be included in the report - Purpose, Outline, T\u00aeransmittal-Report Identifier,Transmitted Items, Location, Status, and Approvals.. A Test Log is used by the test team to record what occurred during te\u00aest executionDetails to be included in the report - Purpose, Outline, Test-Log Identifier,Description, Activity and Event Entries, Execution Description, Pr\u00aeocedure Results,Environmental Information, Anomalous Events, Incident-Report Identifiers A Test Incident report describes any event that occurs during \u00aethe test executionthat requires further investigationDetails to be included in the report - Purpose, Outline, Test-Incident-Report Identifier,Summary, Impact A test summary report summarizes the testing activities associated with one ormore test-design specificationsDetails to be included in the report - Purpose, Outline, Test-Summary-ReportIdentifier, Summary, Variances, Comprehensiveness Assessment, Summary of Results,Summary of Activities, and Approvals.Automating testing is no different from a programmer using a coding language towrite programs to automate any \u00aemanual process. One of the problems with testinglarge systems is that it can go \u00aebeyond the scope of small test teams. Because only asmall number of testers are available the coverage and depth of testing provided areinadequate for the task at hand.Expan\u00aeding the test team beyond a certain size also becomes problematic withincrease in work over head. Feasible way to avoid this without introducing a loss ofquality is through appropriate use of tools that can expand individual\u2019s capacityenormously while maintaining the focus (depth) of testing upon the critical elements.Consider the following factors that help determine the use of automated testingtools:Examine your current testing proce\u00aess and determine where it needs to beadjusted for using automated test tools.Be prepared to make changes in the current ways you perform testing.Involve people who will be using the tool to help design the automated testingprocess.Create a set of evaluation criteria for functions that you will want to considerwhen using the automated test tool. These criteria may include the following: Test rep\u00aeeatability Criticality/risk of applications Operational simplicity Ease of automation Level of documentation of the function (requirements, etc.)Examine your existing set of test cases and test scripts to see which ones aremost applicable for test automation.Train people in basic test-planning skills. ",
    "page519": "Fully manual testing has the benefit of being relatively cheap and effective. But asquality of the product improves the additional cost for finding further bugs becomesmore expensive. Large scale manual testing also implies large scale testing teams withthe related costs of space overhead and infrastructure. Manual testing is also farmore responsive and flexible than automated testing but is prone to tester errorthrough fatigue.Fully automated testing is very consistent and allows the repetitions of similar testsat very little marginal cost. The setup and purchase costs of such automation are veryhigh however and maintenance can be equally expensive. Automation is alsorelatively inflexible and requires rework in order to adapt to changing requirements.Partial Automation incorporates automation only where the most benefits can beachieved. The advantage is that it targets specifically the tasks for automation andthus achieves the most benefit from them. It also retains a large component ofmanual testing which maintains the test team\u2019s flexibility and offers redundancy bybacking up automation with manual testing. The disadvantage is that it obviouslydoes not provide as extensive benefits as either extreme solution.Take time to define the\u00ae tool requirements in terms of technology, process,applications, people skills, and organization.During tool evaluation, prioritize which test types\u00ae are the most critical to yoursuccess and judge the candidate tools on those criteria. Understand the tools and their trade-offs. You may need to use a multi-tool solutionto get higher levels of test-type coverage. For example, you will need to combinethe capture/play-back tool with a load-test tool to cover your performance testcases. Involve potential users in the definition of tool requirements and evaluation criteria. Build an evaluation scorecard to compare each tool's performance against acommon set of criteria. Rank the criteria in terms of relative importance to theorganization. Buying the Wrong Tool Inadequate Test Team Organization Lack of Management Support Incomplete Coverage of Test Types by the selected tool Inadequate Tool Training Difficulty using the tool Lack of a Basic Test Process or Understanding of What to Test Lack of Configuration Management Processes La\u00aeck of Tool Compatibility and InteroperabilityLack of Tool Availability",
    "page520": " oversight, Software sub\u00aecontractmanagement, Software q\u00aeuality assurance, Software configuration managementLevel 3 \u2013 Defined: Key practice areas - Organization process focus, Organizationprocess definition, Training program, integrated software management, Softwareproduct engineering, intergroup coordination, Peer reviewsLevel 4 \u2013 Manageable: Key practice areas - Quantitative Process Management,Software Quality ManagementLevel 5 \u2013 Optimizing: K\u00aeey practice areas - Defect prevention, Technology changemanagement, Process change managementSix Sigma is a quality management program to achieve \"six sigma\" levels of \u00aequality. Itwas pioneered by Motorola in the mid-1980s and has spread too many othermanufacturing companies, notably General Electric Corporation (GE).Six Sigma is a rigorous and disciplined methodology that uses data and statisti\u00aecalanalysis to measure and improve a company's operational performance by identifyingand eliminating \"defects\" from manufacturing to transactional and from product toservice. Commonly defined as 3.4 defects per million opportunities, Six Sigma can bedefined and understood at three distinct levels: metric, methodology andphilos\u00aeophy.Training Sigma processes are executed by Six Sigma Green Belts and Six Sigma BlackBelts, and\u00ae are overseen by Six Sigma Master Black BeltsISO - International Organ\u00aeization for Standardization is a network of the nationalstandards institutes of 150 countries, on the basis of one member per country, with aCentral Secretariat in Geneva, Switzerland, that coordinates the system. ISO is a nongovernmental organization. ISO has developed over 13, 000 International Standardson a variety of subjects.",
    "page521": "Capability Matu\u00aerity git Model - Developed by the software community in 1986 withleadership from the SEI. The CMM describes the principles and practices underlyingsoftware process maturity. It is intended to help softwar\u00aee organizations improve thematurity of\u00ae their software processes in terms of an evolutionary path from ad hoc,chaotic processes to mature, disciplined software processes. The focus is onidentifying key process areas and the exemplary practices that \u00aemay comprise adisciplined software process.What makes up the CMM? The CMM is organized into five maturity levels: Initial Repeatable Defined ManageableOptimizingExcept for Level 1, ea\u00aech maturi\u00aety level decomposes into several key process ar\u00aeeasthat indicate the areas an organization should focus on to improve its softwareprocess.Level 1 - Initial Level: Disciplined process, Standard, Consistent process, Predictableprocess, Continuously Improving processLevel 2 \u2013 Repeatable: Key practice areas - Requirements management, Softwareproje\u00aect planning, Software project tracking ",
    "page522": "Following are some facts that can help you gain a better insight into the realities ofSoftware Engineering.1. The best programmers are up to 28 times better than the worst programmers.2. New tools/techniques cause an initial LOSS of productivity/quality.3. The answer to a feasibility study is almost always \u201cyes\u00ae\u201d.4. A May 2002 report prepared for the National Institute of Standards andTechnologies (NIST)(1) estimate the annual cost of software defects in the UnitedStates as $59.5 billion.5. Reusable components are three times as hard to build6.\u00ae For every 25% increase in problem complexity, there is a 100% increase \u00aein solutioncomplexity.7. 80% of software work is intellectual. A fair amount of it is creative. Little of it isclerical.8. Requirements errors are the most expensive to fix during production.9. Missing requirements are the hardest requirement errors to correct.10. Error-removal is the most time-consuming phase of the life cycle.11. Software is usually tested at best at the 55-60% (branch) coverage level.12. 100% coverage is still far from enough.13. Rigorous inspections can remove up to 90% of errors before the first test case isrun.15. Maintenance typically consumes 40-80% of software costs. It is probably the mostimportant life cycle phase of software.16. Enhancements represent roughly 60% of maintenance costs.17. There is no single best approach to software error removal.",
    "page523": "Testing plays an important role in achieving and assessing the quality of a softwareproduct [25]. On the one hand, we improve the quality of the products as we repeata test\u2013find defects\u2013fix cycle during development. On the other hand, we assess howgood our system is when we perform system-level tests before releasing a product.Thus, as Friedman and Voas [26] have succinctly described, software t\u00aeesting is averification process for software quality assessment and improvement. Generallyspeaking, the activiti\u00aees for software quality assessment can be divided into twobroad categories, namely, static anal\u00aeysis and dynamic analysis. Static Analysis: As the term \u201cstatic\u201d suggests, it is base\u00aed on the examination of a number of documents, namely requirements documents, softwaremodels, design documents, and source code. Traditional static analysisinclu\u00aedes code review, inspection, walk-through, algorithm analysis, andproof of correctness. It does not involve actual execution of the code underdevelopment. Instead, it examines code and reasons over all possible behaviors that might arise during run time. Compiler optimizations are standardstatic analysis.Dynamic Analysis: Dynamic analysis of a software system involves actualprogram execution in order to expose possible program failures. The behavioral and performance properties of the program are also observed. Programs are executed with both typical and carefully chosen input values.Often, the input set of a program can be impractically large. However, forpractical considerations, a finite subset of the input set can be selected.Therefore, in testing, we observe some representative program behaviorsand reach a c\u00aeonclusion about the quality of the system. Careful selectionof a finite test set is crucial to reaching a reliable conclusion.",
    "page524": "By performing static and dynamic analyses, practitioners want to identify as manyfaults as possible so that those faults are fixed at an early stage of the softwaredevelopment. Static analysis\u00ae and dynamic analysis are complementary in nature,and for better effectiveness, both must be performed repeatedly and alternated.Practitioners and researchers need to remove the boundaries between static anddynamic analysis and create a hybrid analysis that combines the strengths of bothapproachesTwo similar concepts related to software testing frequently used by practitioners areverification and validation. Both concepts are abstract in natur\u00aee, and each can berealized by a set of concrete, executable activi\u00aeties. The two concepts are explainedas follows: Verification: This kind of activity helps us in evaluating a software systemby determining whether the product of a given development phase satisfiesthe requirements established before the start of that phase. One may notethat a product can be an intermediate product, such as requirement specification, design specification, code, user manual, or even the final product.Activities that check the correctness of a development phase are calledverification activities. Validation: Activities of this kind help us in confirming that a productmeets its intended \u00aeuse. Validation activities aim at confirming that a productmeets its customer\u2019s expectations. In other words, validation activities focuson the final product, which is extensively tested from the customer point ofview. Validation establishes whether the product meets overall expectationsof the users.Late execution of validation activities is often risky by leading tohigher development cost. Validation activities may be executed at earlystages of the software development cycle [28]. An example of early execution of validation activities can be found in the eXtreme Programming",
    "page525": "(XP) software development methodology. In the XP methodology, the customer closely interacts with the software development group and conductsacceptance tests during each development iteration [29].The verification process establishes the correspondence of an implementationphase of the software d\u00aeevelopment process with its specification, whereas validationestablishes the correspondence between a system and users\u2019\u00ae expectations. \u00aeOne cancom\u00aepare verification and validation as follows: Verification activities aim at confi\u00aerming that one is building the product correctly, whereas validation activities aim at confirming that one is buildingthe correct product [30]. Verification activities review interim work products, such as requirementsspecification, design, code, and user manual, during a project life cycle toensure their quality. The quality attributes sought by verification activitiesare consistency, completeness, and correctness at each major stage of system development. On the other hand, validation is performed toward theend of system development to determine if the entire system meets thecustomer\u2019s needs and expectations. Verification activities are performed on interim products by applying mostlystatic analysis techniques, such as inspection, walkthrough, and\u00ae reviews,and using standards and checklists. Ver\u00aeification can also include dynamicanalysis, such as actual program execution. On the other hand, validationis performed on the entire system by actually running the system in its realenvironment and using a variety of testsIn the literature on software testing, one can find references to the terms failure,error, fault, and defect. Although their meanings are related\u00ae, there are importantdistinctions between these four concepts. In the following, we present first threeterms as they are understood in the fault-tolerant computing community",
    "page526": "Failure: A failure is said to occur whenever the external behavior of asystem does not conform to that prescribed in the system specification.\u2022 Error: An error is a state of the system. In the absence of any correctiveaction by the system, an error state could lead to a failure which wouldnot be attributed to any event subsequent to the error.\u2022 Fault: A fault is the adjudged cause of an error.A fault may remain undetected for a long time, until some event activates it. Whenan event activates a fault, it first brings the program into an intermediate error state.If computation is allowed to proceed from an error state without any correctiveaction, the program eventually causes a failure. As an aside, in fault-tolerant computing, corrective actions can be taken to take a program out of\u00ae an error state intoa desirable state such that subsequent computation does not eventually lead to afailure. \u00aeThe process of failure manifestation can therefore be succinctly representedas a behavior chain [31] as follows: fault\u2192error\u2192failure. The behavior chaincan iterate for a while, that is, failure of one component can lead to a failure ofanother interacting component.The above definition of failure assumes that the given specification is acceptable to the customer. However, if the specification does not meet the expectationsof the customer, then, of course, even a fault-free implementation fails to satisfy thecustomer. It is a difficult task to give a precise definition of f\u00aeault, error, or failureof software, because of the \u201chuman factor\u201d involved in the overall acceptance of asystem. In an article titled \u201cWhat Is Software Failure\u201d [32], Ram Chillarege commented that in modern software business software failure means \u201cthe customer\u2019sexpectation has not been met and/or the customer is unable to do useful work withproduct,\u201d p. 354.Roderick Rees [33] extended Chillarege\u2019s comments of software failure bypointing out that \u201cfailure is a matter of function only [and is thus] related to purpose,not to wheth\u00aeer an item is physically intact or not\u201d (p. 163\u00ae). To substantiate this,Behrooz Parhami [34] provided three interesting examples to show the relevanceof such a view point in wider context. One of the examples is quoted here",
    "page527": "The above definition of failure assumes that the given specification is acceptable to the customer. However, if the specification does not meet the expectationsof the customer, then, of course, even a fault-free implementati\u00aeon fails to satisfy thecustomer. It is a difficult task to give a precise definition of \u00aefault, error, or failureof software, because of the \u201chuman factor\u201d involved in the overall acceptance of asystem. In an article titled \u201cWhat Is Software Fail\u00aeure\u201d [32], Ram Chillarege commented that in modern software business software failure means \u201cthe customer\u2019sexpectation has not been met and/or the customer is unable to do useful work withproduct.",
    "page528": " dissatisfactions are errors in the organization\u2019s state. The organization\u2019s personnel or departmentsprobably begin to malfunction as result of the errors, in turn causing an overall degradation of performance\u00ae. The end result can be the organization\u2019s failure to achieveits goal.There is a fine difference be\u00aetween defects and faults in the above exampl\u00aee, thatis, execution of a defective policy may lead to a faulty promotion. In a softwarecontext, a software system may be defective due to design \u00aeissues; certain systemstates will expose a defect, resulti\u00aeng in the development of faults defined as incorrect signal values or decisions with\u00aein the system. In industry, the term defect iswidely used, wherea\u00aes among researchers the term fault is more prevalent. For allpract\u00aeical purpose, the two terms are synonymous. In this book, we use the twoterms interchangeably as required.",
    "page529": "Roderick R\u00aeees [33] extended Chillarege\u2019s comments of software failure bypointing out that \u201cfailure is a matter of funct\u00aeion only [and \u00aeis thus] related to purpose,not to whether an item is phys\u00aeically intact or not\u201d (p. 163). To substantiate this,Behroo\u00aez Parhami [34] provided three interesting examples to show the relevanceof such a view point in wi\u00aeder context. One of the examples is quoted here (p. 451):Consider a small organization. Defects in the organization\u2019s staff\u00ae promotion policies cancause improper promotions, viewed as \u00aefaults. The resulting ineptitudes ",
    "page530": "No matter how many times we run the test\u2013find faults\u2013fix cycle during softwareNo matter how many times we run the test\u2013find faults\u2013fix cycle during soft\u00aewaredevelopment, some faults are likely to escape our attention, and these will eventually surface at the customer site. Therefore, a quantitative measure that is usefulin assessing the quality of a software is its reliability [35]. Software reliability isdefined as the probability of failure-free operation of a software system for a specified time in a specified environment. The level of reliability of a system depends onthose in\u00aeput\u00aes that cause failures to be observed by the end users. Software reliabilitycan be estimated via random testing, as suggested by Hamlet [36]. Since the notionof reliability is specific to a \u201cspecified environment,\u201d test data must be drawn fromthe input distribution to closely resemble the future usage of the system. Capturing the future usage pattern of a system in a general sense is described in a formcalled the operational profileThe stakeholders in a test process are the programmers, the test engineers, t\u00aeheproject managers, and the customers. A stakeholder is a person or an organizationwho influences a system\u2019s behaviors or who is impacted by that system [39].Different stakeholders view a test process from different perspectives as explainedbelow",
    "page531": "It does work: While implementing a program unit, the programmer maywant to test whether or not the unit works in normal circumstances. Theprogrammer gets much confidence if the unit works to his or her satisfaction\u00ae. The same idea applies to an entire system as well once a systemhas been integrated, the developers may want to test whether or not thesystem performs the basic functions. Here, for the psychological reason,the objective of testing is to show that the system works, rather than itdoes not work.It does not wor\u00aek: Once the programmer (or the development team) issatisfied that a unit (or the system) works to a certain degree, more testsare conducted with the objective of finding faults in the unit (or the system).Here, the idea is to try\u00ae to make the unit (or the system) fail.Reduce the risk of failure: Most of the complex software systems containfaults, which cause the system to fail from time to time. This concept of\u201cfailing from time to time\u201d gives rise to the notion of failure rate. Asfaults are discovered and fixed while performing more and more tests, thefailure rate of a system generally decreases. Thus, a higher level objectiveof performing tests is to bring down the risk of failing to an acceptablelevel.Reduce the cost of testing: The different kinds of costs associated with atest process includethe cost of designing, maintaining, and executing test cases,the cost of analyzing the result of executing each test case,the cost of documenting the test cases, andthe cost of actually executing the system and documenting it.",
    "page532": "Therefore, the less\u00ae the number of test cases designed, the less will be theassociated cost of testing. However, producing a small number of arbitrarytest cases is not a good way of saving cost. The highest level of objectiveof performing tests is to produce low-risk software with fewer numberof test cases. This idea leads us to the concept of effectiveness of testcases. Test engineers must therefore judiciously select fewer, effective testcasesIn its most basic form, a test case is a simple pair of < input, expected outcome >.If a program under test is expected to compute the square root of nonnegativenumbers, then four examples o\u00aef test cases are as shown in Figure 1.3.In stateless systems, where the outcome depends solely on the current input,test cases are very simple in structure, as shown in Figure 1.3. A program tocompute the square root of nonnegative numbers is an example of a statelesssystem. A compiler for the C programming language is another example of astateless system. A compiler is a stateless system because to compile a program itdoes not need to know about the programs it compiled previously.In state-oriented systems, where the program outcome depends both on thecurrent state of the system and the current input, \u00aea test case may consist of asequence of < input, expected outcome > pairs. A telephone switching system andan automated teller machine (ATM) are examples of state-oriented systems. For anATM machine, a test case for testing the withdraw function is shown in Figure 1.4.Here, we assume that the user has already entered validated inputs, such as the cashcard and the personal identification number (PIN).",
    "page533": "In the test case TS1, \u201ccheck balance\u201d and \u201cwithdraw\u201d in the first, second, andfourth tuples represent the pressing of the appropriate keys on the ATM keypad. It isassumed that the user accou\u00aent has $500.00 on it, and the user wants to withdraw anamount of $200.00. The expect\u00aeed outcome \u201c$200.00\u201d in the third tuple representsthe cash dispensed by the ATM. After the withdrawal operation, the user makessure that the remaining balance is $300\u00ae.00.For state-oriented systems, most of the test cases include some form of decision and timing in providing input to the system. A test case may include loopsand timers, which we do not show at this moment.An outcome of program execution is a complex entity that may include thefollowing:Values produced by the program:Outputs for local observation\u00ae (integer, text, audio, image)Outputs (messages) for remote storage, manipulation, or observationState change:State change of the programState change of the database (due to add, delete, and update operations) A sequence or set of values which must be interpreted together for theoutcome to be validAn important concept in test design is the concept of an oracle. An oracleis any entity program, process, human expert, or body of data that tell\u00aes us theexpected outcome of a particular test or set of tests [40]. A test case is meaningfulonly if it is possible to decide on the acceptability of the result produced by theprogram under test.Ideally, the expected outcome of a test should be computed while designingthe test case. In other words, the test outcome is computed before the program is executed with the selected test input.",
    "page534": "The idea here is that one should be able tocompute the expected outcome from an understanding of the program\u2019s requirements. Precomputation of the expected outcome will eliminate any implementationbias in case the test case is designed by the developer.In exceptional cases, where it is extremely difficult, impossible,\u00ae or evenundesirable to compute a single expected outcome, one should identify expectedoutcomes by examining the actual test outcomes, as explained in the following:Execute the program with the selected input.Observe the actual out\u00aecome of program execution.Verify that the actual outcome is the expected outcome\u00ae. Use the verified actual outcome as the expected outcome in subsequentruns of the test case It is not unusual to find people making claims such as \u201cI have exhaustively testedthe program.\u201d Complete, or exhaustive, testing mea\u00aens there are no undiscoveredfaults at the end of the test phase. All problem\u00aes must be known at the end ofcomplete testing. For most of the systems, complete testing is near impossiblebecause of the following reasons: The domain of possible inputs of a program is too large to be completelyused in testing a system. There are both valid inputs and invalid inputs.The program may have a large number of states. There may be timingconstraints on the inputs, that is, an input may be valid at a certain timeand invalid at other times. An inp\u00aeut value which is valid but is not properlytimed is called an inopportune input. The \u00aeinput domain of a system canbe very large to be completely used in testing a program.",
    "page535": "The design issues may be too complex to completely test. The design mayhave included implicit design decisions and assumptions. For example,a programmer may use a global variable or a static variable to controlprogram execution. It may not be possible to create all possible execution envi\u00aeronments of thesystem. This becomes more significant when the behavior of the softwaresystem depends on the real, outside world, such as weather, tempe\u00aerature,altitude, p\u00aeressure, and so on.We must realize that though the outcome of complete testing, that is, discovering allfaults, is highly desirable, it is a near-impossible task, and it may not be attempted.The next best thing is to select a subset of the input domain to test a program.Referring to Figure 1.5, let D be the input domain of a program P. Suppose thatwe select a subset D1 of D, that is, D1 \u2282 D, to test program P. It is possible thatD1 exercises only a part P1, that is, P1 \u2282 P, of the execution behavior of P, inwhich case faults with the other part, P2, will go undetected.By selecting a subset of the input domain D1, the test engineer attemptsto deduce properties of an entire program P by observing the behavior of a partP1 of the entire behavior\u00ae of P on selected inputs D1. Therefore, selection of thesubset of the input domain must be done in a systematic and careful manner sothat the deduction is as accurate and complete as p\u00aeossible. For example, the ideaof coverage is considered while selecting test cases In order to test a program, a test engineer must perform a sequence of testingactivities. Most of these activities have been shown in Figure 1.6 and are explainedin the following. ",
    "page536": "Identify an objective to be tested: The fir\u00aest activity is t\u00aeo identify anobjective to be tested. The objective defines the intention, or\u00ae purpose, ofdesigning one or more test cases to ensure that the program supports theobjective. A clear purpose must be associated with e\u00aevery test caseSelect inputs: The second activity is to select test inputs. Selection of testinputs can be based on the requirements specification, the source code,or our expectations. Test inputs are selected by keeping the test objectivein mind.Compute the expected outcome: The third activity is to compute theexpected outcome of the program wi\u00aeth the selected inputs. In most cases,this can be done from an overall, high-level understanding of the testobjective and the specification of the program under test.Set up the execution environment of the program: The fourth step \u00aeis toprepare the right execution environment of the program. In this step all theassumptions external to the program must be satisfied. A few examples ofassumptions external to a program are as follows:Initialize the local system, external to the program. This may includemaking a network connection available, making the right databasesystem available, and so on.Initialize any remote, external system (e.g., remote partner process in adistributed application.) For example, to test the client code, we mayneed to start the server at a remote siteExecute t\u00aehe program: In the fifth step, the test engineer executes theprogram with the selected inputs and o\u00aebserves the actual outcome of theprogram. To execute a test case, in\u00aeputs may be provided to the program atdifferent physical locations at different times. The\u00ae concept of test coordination is used in synchronizing different components of a test case",
    "page537": "Analyze the test result: The final test activity is to analyze the result oftest execution. Here, the main task is to compare the actual outcome ofprogram execution with the expected outcome. The complexity of comparison depends on the complexity of the data to be observed. The observeddata type can be as simple as an i\u00aenteger or a string of characters or ascomplex as an image, a video, or an audio clip. At the end of the analysis step, a test verdict is assigned to the program. There are three majorkinds of test verdicts, namely, pass, fail, and inconclusive, as explainedbelow.If the program produces the expected outcome and the purpose of thetest case is satisfied, then a pass verdict is assigned.If the program does not produce the expected outcome, then a fail verdictis assigned.However, in some cases it may not be possible to assign a clear passor fail verdict. For example, if a timeout occurs while executing atest case on a distributed application, we may not be in a position toassign a clear pass or fail verdict. In those cases, an inconclusive testverdict is assigned. An inconclusive test verdict\u00ae means that furthertests are needed to be done to refine the inconclusive verdict into aclear pass or fail verdict A test report must be written after analyzing the test result. Themotivation for writing a test report is to get the fault fixed if the test revealeda fault. A test report contains the following items to be informative:Explain how to reproduce the failure.Analyze the failure to be able to de\u00aescribe it.A pointer to the actual outcome and the test case, complete with theinput, the expected outcome, and the execution environm\u00aeent.",
    "page538": "Testing is performed at different levels involving the complete system or parts ofit throughout the life cycle of a software product. A software system goes throughfour stages of testing before it is actually depl\u00aeoyed. These four stages are knownas unit, integration, system, and acceptance level testing. The first three levels oftesting are performed by a number of different stakeholders in the developmentorganization, where as acceptance testing is performed by the customers. The fourstages of testing have been illustrated in the form of what is called the classical Vmodel In unit testing, programmers test individual program units, such as a procedures, functions, methods, or classes, in isolation. After ensuring that individualunits work to a satisfactory extent, modules are assembled to construct larger subsystems by following integration testing techniques. Integration testing is jointlyperformed by software developers and integration test engineers. The objective ofintegration testing is to construct a reasonably stable system that can withstandthe rigor of system-level testing. System-level testing includes a wide spectrumof testing, such as functionality testing, security testin\u00aeg, robustness testing, loadtesting, stability testing, stress testing, performance testing, and reliability testing.System testing is a critical phase in a software development process because of theneed to meet a tight schedule close to delivery date, to discover most of the faults,and to verify that fixes are working an\u00aed have not resulted in new faults. Systemtesting comprises a number of distinct activities: creating a test plan, designinga test suite, preparing test environments, executing the tests by following a clearstrategy, and monitoring the process of test execution.",
    "page539": "Regression testing is another level of testing that is performed throughout thelife cycle of a system. Regression testing is performed whenever a component ofthe system is modified. The key idea in regression testing is to ascertain that themodification has not introduced any new faults in the portion that was \u00aenot subjectto modification. To be precise, reg\u00aeression testing is not a distinct level of testing.Rather, it is considered as a subphase of unit, integration, and system-level testing,as illustrated in Figure 1.8 [41].In regression testing, new tests are not designed. Instead, tests are selected,prioritized, and executed from the existing pool of test cases to ensure that noth\u00aeingis broke\u00aen in the ne\u00aew version of the software. Regression testing is an expensiveprocess and accounts for a predominant portion of testing effort in the industry. Itis desirable to select a subset of the test cases from the existing pool to reduce thecost. A key question is how many and which test cases should be selecte\u00aed so thatthe selected test cases are more likely to uncover new faults [42\u201344].After the completion of system-level testing, the product is delivered to thecusto\u00aemer. The customer performs their own series of tests, commonly known asacceptance testing. The objective of acceptance testing is to measure the qualityof the product, rather than searching for the defects, which is objective of systemtesting. A key notion in acceptance testing is the customer\u2019s expectations from thesystem. By the time of acceptance testing, the customer should have developedtheir acceptance criteria based on their own expectations from the system. Thereare two kinds of acceptance\u00ae testing as explained in the following: User acceptance testing (UAT)Business acceptance testing (BAT)",
    "page540": "User acceptance testing is conducted by the customer to ensure that the systemsatisfies the contractual acceptance criteria before being signed off as meeting userneeds. On the other hand, BAT is undertaken within the supplier\u2019s developmentorganization. The idea in having a BAT is to ensure that the system will eventuallypass the user acceptance test. It is a rehearsal of UAT at the supplier\u2019s premises.Designing test cases has continued to stay in the foci of the research communityand the practitioners. A software development\u00ae process generates a large body ofinformation, such as requirements specification, design document, and source code.In order to\u00ae generate effective tests at a lower cost, test designers analyze thefollowing sources of information: Requirements and functional specificationsSource codeinput and output domainsOperational profileFault modelRequirements and Functional Specifications The process of software \u00aedevelopment begins by capturing user needs. The nature and amount of user needsidentified at the beginning of system development will vary depending on thespecific life-cycle model to be followed. Let us consider a few examples. In theWaterfall model [45] of software development, a requirements engineer tries tocapture most of the requireme\u00aents. On\u00ae the other hand, in an agile software development model, such as XP [29] or the Scrum [46\u201348], only a few requirementsare identifie\u00aed in the beginning. A test engineer considers all the requirementsthe program is expected to meet whichever life-cycle model is chosen to test aprogram",
    "page541": "The requirements might ha\u00aeve been specified in an informal manner, such asa combination of plaintext, equations, figures, and flowcharts. Though this form ofrequirements specification may be ambiguous, it is easily understood by customers.For example, the Bluetooth specification consists of about 1100 pages of descriptions explaining how various subsystems of a Bluetooth interface is expected towork. The specification is written in plaintext form supplemented with mathematical equations, state diagrams, tables, and figures. For some systems, requirementsmay have been captu\u00aered in the form of use cases, entity\u2013relationship diagrams,and class diagrams. Sometimes the requirements of a system may have been specified in a formal language or notation, such as Z, SDL, Estelle, or finite-statemachine. Both the informal and formal specifications are prime sources of testcasesSource Code Whereas a requirements specification describes the intendedbehavior of a system, the source code describes the actual behavior of the system.High-level assumptions and constraints take concrete form in an implementation.Though a software designer may produce a detailed design, programmers mayintroduce additional details into the system. For example, a step in the detaileddesign can be \u201csort array A.\u201d To sort an array, there are many sorting algorithmswith different characteristics, such as iteration, rec\u00aeursion, and temporarily usinganother array. Therefore, test cases must be designed based on the program [50].Input and Output Domains Some values in the input domain of a programhave special meanings, and hence must be treated separately [5]. To illustrate thispoint, let us consider the factorial function.",
    "page542": "without considering the special case of n  0. The above wrong implementationwill produce the correct result for all positive values of n, but will fail for n = 0.Sometimes even some output values have special meanings, and a programmust be tested to ensure that it produces the special values for all possible causes.In the above example, the output value 1 has special significance: (i) it is theminimum value \u00aecomputed by the factorial function and (ii) it is the only value\u00aeproduced for two different inputs.In the integer domain, the values 0 and 1 exhibit special characteristicsif arithmetic operations are performed. These characteristics are 0 \u00d7 x = 0 and1 \u00d7 x = x for all values of x. Therefore, all the special values in the input andoutput domains of a program must be considered w\u00aehile testing the program.Operational Profile As the term suggests, an operational profile is a quantitative characterization of how a system will be used. It was created to guide testengineers in selecting test cases (inputs) using samples of system usage. The notionof operational profiles, or usage profiles, was developed by Mills et al.The idea isto infer, from the observed test results, the future reliability of the software whenit is in actual use. To do this, test inputs are assigned a probability distribution, orprofile, according to their occurrences in actual operation. The ways test engineersassign probability and select test cases to operate a system may significantly differfrom the ways actual users operate a system. However, for accurate estimationof the reliability of a system it is important to test a system by considering theways it will actually be used in the field.",
    "page543": "Fault Model Previously encountered fau\u00aelts are an excellent source of information in designing new test cases. The known\u00ae fau\u00aelts are classified into differentclasses, such as initialization faults, logic faults, and interfac\u00aee faults, and stored ina repository [55, 56]. Test engineers can use these data in designing tests to ensurethat a pa\u00aerticula\u00aer class of faults is not resident in the program.There are three types of\u00ae fault-based \u00aetesting: error guessing, fault seeding,and mutation analysis. In error guessing, a test engineer applies his experienceto (i) assess the situation and guess where and what kinds of faults might exist,and (ii) design tests to specifically expose those kinds of faults. In fault seeding,known faults are injected into a program, and the test suite is executed to assessthe effectiveness of the test suite. Fault seeding makes an assumption that a testsuite that finds seeded faults is also likely to find other faults. Mutation analysis issimilar to fault seeding, except that mutations to program statements are made inorder to determine the fault detection capability of the test suite. If the test cases arenot capable of revealing such faults, the test engineer may specify additional testcases to reveal the faults. Mutation testing is based on the idea of fault simulation,whereas\u00ae fault seeding is based on\u00ae the idea of fault injection. In the fault injectionapproach, a fault is inserted into a program, and an oracle is available to assert thatthe inserted fault indeed made the program incorrect. On the other hand, in faultsimulation, a program modification is not guaranteed to lead to a faulty program.In fault simulation, one may modify an incorrect program and turn it into a correctprogram.",
    "page544": "A key idea in Section  was that test cases need to be designed by considering information from several sources, such as the specification, source code, andspecial properties of the program\u2019s input and output domains. This is because allthose sources provide complementary information to t\u00aeest designers. Two broad concepts in testing, based on the sources of information for test design, are white-boxand black-box testing. White-box testing techniques are also called structural testing techniques, whereas black-box testing techniques are called functional testingtechniques.In structural testing, one primarily examines source code with a focus on control flow and data flow. Control flow refers to flow of control from one instructionto another. Control passes from one instruction to another instruction in a numberof ways, such as one instruction appearing after another, funct\u00aeion call, messagepassing, and interrupts. Conditional statements alter the normal, sequential flowof control in a program. Data flow refers to the propagation of values from onevariable \u00aeor constant to another variable. Definitions and uses of variables determinethe data flow aspect in a program.In functional testing, one does not have access to the internal details\u00ae of aprogram and the program is tr\u00aeeated as a black box. A test engineer is concernedonly with the part that is accessible outside the program, that is, just the inputand the externally visible outcome. A test engineer ap\u00aeplies input to a program,observes the externally visible outcome of the program, and deter\u00aemines whetheror not the program outcome is the expected outcome. Inputs are selected fromthe program\u2019s requirements specification and properties of the program\u2019s input andoutput domains. A test engineer \u00aeis concerned only with the functionality and thefeatures found in the program\u2019s specification",
    "page545": "At this point it is useful to identify a distinction between the scopes ofstructural testing and functional testing. One applies structural testing techniquesto individual units of a program, whereas functional testi\u00aeng techniques can beapplied to both an entire system and the individual program units. Since individualprogrammers know the details of the source code they write, they themselvesperform structural testing on the individual program units they write. On the otherhand, functional testing is performed at the external i\u00aenterface \u00aelevel of a system,and it is conducted by a separate software quality assurance group.Let us consider a program unit U which is a part of a larger program P.A program unit is just a piece of source code with a well-defined objective andwell-defined input and output domains. Now, if a programmer derives test casesfor testing U from a knowledge of the internal details of U , then the programmeris said to be performing structural te\u00aesting. On the other hand, if the programmerdesigns test cases from the stated objective of the unit U and from his or herknowledge of the special properties of the input and output domains of U , then heor she is said to be performing functional testing on the same unit U .\u00aea of functional testing.Neither structural testing nor functional testing is by itself good enough todetect most of the faults. Even if one selects all possible inputs, a structural testingtechnique cannot detect all faults if there are missing paths in a program. Intuitively,a path is said to be missing if there is no code to handle a possible condition.",
    "page546": "Similarly, without knowledge of the structural details of a program, many faultswill go undetected. Therefore, a combination of both structural and functionaltesting techniques must be use\u00aed in program testing.The purp\u00aeose of system test planning, or simply test pla\u00aennin\u00aeg, is to get ready andorganized for test execution. A test plan provides a framework, scope, details ofresource needed, effort required, schedule o\u00aef activities, and a budget. A frameworkis a set of ideas, facts, or circumstances within which the tests will be conducted.The stated scope outlines the domain, or ext\u00aeent, of the test activities. The scopecovers the managerial aspects of testing, rather than the detailed techniques andspecific test cases.Test des\u00aeign is a critical phase of software testing. During the test designphase, the system requ\u00aeirements are critically studied, system features to\u00ae betested are thoroughly identified, and the objectives of test cases and the detailedbehavior of test cas\u00aees are defined. Test objectives are identified from differentsources, namely, the requirement specification and the functional specification,and one or more test cases are designed for each test objective. Each test case isdesigned as a combination of modular test components called test steps. Thesetest steps can be combined together to create more complex, multistep tests. Atest case is clearly specified so that others can easily borrow, understand, andreuse it ",
    "page547": "Monitoring and measurement are\u00ae two key principles followed in every \u00aescientific andengineering\u00ae endeavor. The same principles are also applicable to the testing phasesof software development. It \u00aeis important to monitor certain metrics which trulyrepresent the progress of testing and reveal the quality level of the system. Basedon those metrics, the management can trigger corrective and preventive actions. Byputting a small but critical set of metrics in place the executive management willbe able to know whether they are on the right track [58]. Test execution metricscan be broadly categorized in\u00aeto two classes as follows: Metrics for monitoring test execution Metrics for monitoring defectsThe firs\u00aet class of metrics concerns the process of executing test cases, whereasthe second class concerns the defects found as a result of test execution. Thesemetrics need to be tracked and anal\u00aeyzed on a periodic basis, say, daily or weekly.In order to effectively control a test project, it is important to gather valid andaccurate information about the project. One such example is to precisely knowwhen t\u00aeo trigger revert criteria for a test cycle and initiate root cause analysis ofthe problems before more tests can be performed. By triggering such a revertcriteria, a test manager can effectively utilize the time of test enginee\u00aers, and possibly money, by suspending a test cycle on a product with too many defects tocarry out a meaningful system test. A management team mus\u00aet identify and monitor metrics while testing is in progress so that important decisions can be made",
    "page548": " It is important to analyze and understand the test metrics, rather than justcoll\u00aeect data and make decisions based on those raw data. Metrics are meaningful only if they enable the management to make decisions which result in lo\u00aewercost of production, reduced delay in delivery, and improved quality of softwaresystems.Quantitative evaluation is important in every scientific and engineering field.Quantitative evaluation is carried out through measurement. Measurement lets oneevaluate parameters of interest in a quantitative manner as follows: Evaluate the effectiveness of a t\u00aeechnique used in performing a task. Onecan evaluate the effectiveness of a test generation technique by countingthe number of defects detected by test cases generated by following thetech\u00aenique and those detected by test cases generated by other means.Evaluate the productivity of the development activities. One can keep trackof productivity by counting the number of test cases designed per day, thenumber of test cases executed per day, and so on. Evaluate the quality of the pr\u00aeoduct. By monitoring the number of defectsdetected per week of testing, one can observe the quality level of thesystem. Evaluate the product testing. For evaluating a product testing process, thefollowing two measurements are critical:incises in test design. The need for more testing occursas test  engineers get new ideas while executing the planned testcases.Test effort effectiveness metric: It is important to evaluate t\u00aehe effectiveness of the testing effort in the development of a product. After aproduct is deployed at the customer\u2019s site, one is interested to knowthe effectiveness of testing that was \u00aeperformed",
    "page549": "A common measureof test effectiveness is the number of defects found by the customersthat were not found by the test engineers prior to the release of theproduct. These defects had escaped our test effort.In general, software testing is a highly labor\u00ae intensive task. This is because test casesare to a great extent manually generated and often man\u00aeually executed. Moreover,the results of test executions are manually a\u00aenalyzed. The duratio\u00aens of those taskscan be shortened by using appropr\u00aeiate tools. A test engineer can use a variety oftools, such as a static code analyzer, a test data g\u00aeenerator, and a network analyzer,if a network-based application or protocol is under test. Those tools are useful \u00aeinincreasing the efficiency and effectiveness of testing.Test automation is essential for any testing and quality assurance division ofan organization to move forward to become more efficient. The benefits of testautomation are as follows: Increased pr\u00aeoductivity of the testers Better coverage of regression testing Reduced durations of the testing phases Reduced cost of software maintenance Increased effectiveness of test casesTest automation provides an opportunity to improve the skills of the testengineers by writing programs, and hence their morale. They will be more focusedon developing automated test cases to avoid being a bottleneck in product deliveryto the market. Consequently, software testing becomes less of a tedious job.Test automation improves the coverage of regression testing because of accumulation of automated test cases over time. Automation allows an organization tocreate a rich library of reusable test cases and facilitates the execution of a consistent set of test cases. Here consistency means our ability to produce repeatedresults for the same set of tests",
    "page550": " It may be very difficult to reproduce test results inmanual testing, because exact cond\u00aeitions at the time and point of failure may notbe precisely known. In automated testing it is easier to set up the initial conditionsof a system, thereby making it easier to reproduce test results. Test automationsimplifies the debugging work by providing a det\u00aeailed, unambiguous log of activities and intermediate test steps. This leads to a more organized, structured, andreproducible testing approach.Automated execution of test cases reduces the elapsed time for testing, and,thus, it leads to a shorter time to market. The same automated test cases can beexecuted in an unsupervised manner at night, thereby efficiently utilizing the different platforms, such as hardware and configuration. In short, automation increasestest execution efficiency. However, at the end of test execution, it is important toanalyze the test results to determine the number of test cases that passed or failed.And, if a test case failed, one analyzes the reasons for its failure.In the long run, test automation is cost-effective. It drastically reduces th\u00aee software maintenance cost. In the sustaining phase of a software system, the regressiontests required after each change to the system are too many. As a result, regressiontesting becomes too time and labor intensive without automation.A repetitive type of testing is very cumbersome and expensive to performmanually, but it can be automated easily using software tools. A simple repetitivetype of applicat\u00aeion can reveal memory leaks in a software. However, the applicationhas to be run for a significantly long duration, say, for weeks, to reveal memoryleaks. Therefore, manual testing may not be justified, whereas with automation i\u00aetis easy to reveal memory leaks.",
    "page551": "For example, stress testing is a prime candidate forautomation. Stress testing requires a worst-case load for an extended period of time,which is very difficult to realize by manual means. Scalability testing is anotherarea that can be automated. Instead of creating a large test bed with hundreds ofequipment, one can develop a simulator to verify the scalability of the system.Test automation is very attractive, but it comes with a price tag. Sufficienttime and res\u00aeources need to be allocated for the development of an automated testsuite. Development of automated test cases need to be managed like a programmingproject. That is, it should be done in an organized manner; otherwise it is highlylikely to fail. An automated test suite may take longer to develop because the testsuite needs to be debugged before it can be used for testing. Sufficient time andresources need to be allocated for maintaining an automated test suite and setting upa test e\u00aenvironment. Moreover, every time the system is modified, the modificationmust be reflected in \u00aethe automated test suite. Therefore, an automated test suiteshould be designed as a modular system, coordinated into reusable libraries, andcross-referenced and traceable back to the feature being tested.It is important to remember that test automation cannot replace manual testing. Human creativity, variability, and observability cannot be mimicked throughautomation. Automation cannot detect some problems that can be easily observedby a human being. Automated testi\u00aeng does not introduce minor variations\u00ae the waya human can. Certain categories of \u00aetests, such as usability, interoperability, robustness, and compatibility, are often not suited\u00ae for automation. It is too difficult toed.",
    "page552": "The objective of test automation is not to reduce the head counts in thetesting department of an organization, but to improve the productivity, quality, andefficiency of test execution. In fact, test automation requires a larger head count inthe testing department in the first year, because the department needs to automatethe test cases and simultaneously continue the execution of manual tests. Even afterthe completion of the development of a test automation framework and test caselibraries, the head count in the testing department does not drop below its originallevel. The test organization needs to retain the original team members in order toimprove the quality by adding\u00ae more test cases to the automated test case repository.Before a test automation project can proceed, the organization must assessand address a number of considerations. The following list of prerequisites mustbe considered for an assessment of whether the organization is ready for testautomation:The test cases to be automated are well defined. Test tools and an infrastructure are in placeThe test automation professionals have prior successful experience inautomation. Adequate budget should have been allocated for the procurement of software tools.Testing is a distributed activity conducted at different levels throughout the lifecycle of a software. These different levels are unit testing, in\u00aetegration testing, system testing, and acceptance testing. It is logical to have different testing groups inan organization for each level of testing. However, it is more logical and is thecase in reality that unit-level tests be developed and executed by the programmersthemselves rather than an i\u00aendependent group of unit test engineers. The programmer who de\u00aevelops a software unit should take the ownership and responsibilityof producing good-quality software to his or her satisfaction.",
    "page553": "System integrationtesting is performed by the system integration test engineers. The integration testengineers involved need \u00aeto know the software modules very well. This means thatall development engineers who co\u00aellectively built all the units being integratedneed to be involved in \u00aeintegration testing. Also, the integration test engineersshould thoroughly know the build mechanism, which is key to integrating largesystems.A team for performing system-level testing is truly separated from the development team, and it u\u00aesually has a separate head count and a separate bud\u00aeget. Themandate of this grou\u00aep is to ensure that the system requirements have been met andthe system is acceptable. Members of the system test group conduct different categories of tests, such as functionality, \u00aerobustness, stress, load, scalability, reliability,and performance. They also execute business acceptance tests identified in the useracceptance test plan to ensure that the system will eventually pass user acceptancetesting at the customer site. However, the real user acceptance testing is executedby the client\u2019s special user group. The user group consists of people from different backgrounds, such as softwa\u00aere quality assurance engineers, business associates,and customer support engineers. It is a common practice to create a temporaryuser acc\u00aeeptance test group consisting of people with different backgrounds, suchas integration test engineers, system test engineers, customer support engineers,and marketing engineers. Once the user acceptance is completed, the group is dismantled. It is recommended to have at least two test groups in an organization:integration test group and system test group.Hiring and retaining test engineers are challenging tasks. Interview is theprimary mechanism for evaluating applicants. Interviewing is a skill that improveswith practice. It is necessary to have a recruiting process in place in order to beeffective in hiring excellent test engineers. In order to retain test engineers, themanagement must recognize the importance of testing efforts at par with development efforts. The mana\u00aegement should treat the test engineers as professionals andas a part of the overall team that delivers quality products",
    "page554": "With the above high-level introduction to quality and software testing, we are nowin a position to outline the remaining chapters. Each chapter in the book coverstechnical, process, and/or managerial topics related to software testing. The topicshave been designed and organized to facilitate the reader to become a so\u00aeftware testspecialist. In Chapter 2 we provide a self-contained introduction to the theory andlimitations of software testing.Chapters 3\u20136 treat unit testing techniques one by one, as quantitativelyas possible. These chapters describe both static and dynamic unit testing. Staticunit testing has been presented within a general framework called code review,rather than individual techniques called inspection and walkthrough. Dynamic unitte\u00aesting, or execution-based unit testing, focuses on control flow, data flow, anddo\u00aemain testing. The JUnit framework, which is used to create and execute dynamicunit tests, is introduced. We discuss some tools for\u00ae effectively performing unittesting.Chapter 7 discusses the concept of integration testing. Specifically, five kindsof integration techniques, namely, top down, bottom up, sandwich, big bang, andincremental, are explained. Next, we discuss the integration of hardware and software components to form a complete system. We introduce a framework to developa plan for system integration testing. The chapter is completed with a brief discussion of integration te\u00aesting of off-the-shelf components.Chapters 8\u201313 discuss various aspects of system-level testing. These sixchapters introduce the reader to the technical details of system testing that is thepractice in industry. These chapters promote both qualitative and quantitative evaluation of a system testing process. The chapters emphasize the need for having anindependent system testing group. A process for m\u00aeonitoring and controlling system testing is clearly explained. Chapter 14 is devoted to acceptance testing, whichincludes acceptance testing criteria, planning for acceptance testing, and acceptancetest execution.",
    "page555": "execution.Chapter 15 contains the fundamental concepts of software reliability and theirapplication to software testing. We discuss the notion of operation profile and itsapplication in system testing. We conclude the chapter with the description of anexample and the time of releasing a system by determ\u00aeining the additional lengthof system testing. The additional testing time is calculated by using the idea ofsoftware reliability.In Chapter 16, we present the structure of test groups and how these groupscan be organized in a software company. Next, we discuss how to hire and retaintest engineers by providing training, instituting a reward system, and establishingan attractive career path for them within the testing organization. We conclude th\u00aeischapter with the description of how to build and manage a test team with a focuson teamwork rather than individual gain.Chapters 17 and 18 explain the concepts of software quality and differentmaturity models. Chapter 17 focuses on quality factors and criteria and describesthe ISO 9126 and ISO 9000:2000 standards. Chapter 18 covers the CMM, whichwas developed by the SEI at Carnegie Mellon University. Two test-related models,namely the \u00aeTPI model and the TMM, are explained at the end of Chapter 18.We define the key words used in the book in a glossary at the end of the book.The reader will find about 10 practice exercises at the end of each chapter. A listof references is included at the end of each chapter for a reader who would like tofind more detailed discussions of some of the topics. Finally, each chapter, exceptthis one, contains a literature review section that, essentially, provides pointers tomore advanced material related to the topics. The more advanced materials arebased on current research and alternate viewpoints.",
    "page556": "Any approach to testing is based on assumptions\u00ae about the way program faultsoccur. Fa\u00aeults are due to two main reasons:Faults occur due to our ina\u00aedequate understanding of all condi\u00aetions withwhich a program must deal.Faults occur due to our failure to realize that certain combinations of conditions require special treatments.Goodenough and Gerhart classify program faults as follows:Logic Fault: This class of faults means a program produces incorrectresults independent of resources required. That is, the program fails becauseof the faults present in the program and not because of a lack of resources.Logic faults can be further split into three categories:Requirements fault: This means our failure to capture the real requirements of the customer.Design fault: This represents our failure to satisfy an understoodrequirement.Construction fault: This r\u00aeepresents our failure to satisfy a design. Suppose that a design \u00aestep says \u201cSort array A.\u201d To sort the array with Nelements, one may choose one of several sorting algorithms. Let {:}be the desired for loop construct to sort the array. If a programmerwrites the for loop in the formfor (i = 0; i <= N; i  ){:}then there is a construction error in the implementation.Performance Fault: This class of faults leads to a failure of the programto produce expected results within specified or desired resource limitations.A thorough test must be able to detect faults arising from any of the abovereasons. Test data selection criteria must reflect information derived from each stageof software development. Since eac\u00aeh type of fault i\u00aes manifested a\u00aes an impropereffect produced by an implementation, it is useful to categorize the sources of faultsin implementation terms a\u00aes folows: ",
    "page557": "Missing Control Flow Paths: Intuitively, a control flow path, or simply apath, is a feasible sequence of instructions in a program. A path may bemissing from a program if we fail to identify a condition and specify apath to handle that condition. An example of a missing\u00ae path is our failureto test for a zero divisor\u00ae before executing a division. If we fail to recognizethat a divisor can take a zero value, then we will not include a piece ofcode to handle the specia\u00ael case. Thus, a certain desirable computation willbe missing from the program.Inappropriate Path Selection: A program executes an inappropriate path ifa condition is expressed incorrectly.  we show a des\u00aeiredbehavior and an implemented behavior. Both the behaviors are identicalexcept in the condition part of the if statement. The if part of the implemented behavior contains an additional condition B. I\u00aet is easy to see that both the desired part and the implemented part behave in the same way\u00aefor all combinations of values of A and B except when A = 1 and B = 0.Inappropriate or Missing Action: There are three instances of this class offault One may calculate a value using a method that does not necessarilygive the correct res\u00aeult. For example, a desired expression is x = x \u00d7 w,whereas it is wrongly written as x = x   w. These two expressionsproduce identical results for several combinations of x and w, such asx = 1.5 and w = 3, for example.Failing to assign a value \u00aeto a variable is an example of a missing action. Calling a function with the wrong argument list is a kind of inappropriateaction.",
    "page558": "The main danger due to an inappropriate or missing action is that the action isincorrect only under certain combinations of conditions. Therefore, one must dothe following to find test data that reliably reveal errors: Identify all the conditions relevant to the correct operation of a program.Select test data to exercise all possible combinations of these conditions.The above idea of selecting test data leads us to define the following terms:Tes\u00aet Data: Test data are actual values from the input domain of a programthat collectively satisfy some test selection criteria.Test Predicate: A test predicate is a description of conditions and combinations of conditions relevant to correct operation of the program:Test predicates describe the aspects of a program that ar\u00aee to be tested.Test data cause these aspects to be tested.Test predicates are the motivating force for test data selection.Components of test predicates arise first and primarily from the specifications for a program.Further conditions and predicates may be added as implementations areconsidered.A set of test predicates must at least satisfy the following conditions to have anychance of being reliable. These conditions are key to meaningful testing:Every individual branching condition in a program must be represented byan equivalent condition in C.Every\u00ae potential termination condition in the program, for example, an overflow, must be represented by a condition i\u00aen C.Every condition relevant to the correct operation of the program that isimplied by the specification and knowledge of the data structure of theprogram must b\u00aee represented as a condition in C.",
    "page559": " The concepts of reliability and validity have been defined with respect tothe entire input domain of a program. A criterion is guaranteed to be bothreliable and valid if and only if it selects the entire domain as a single test.Since such exhaustive testing is impractical, one\u00ae will have much difficultyin assessing the reliability and validity of a criterion.The concepts of reliability and validity have been defined with respect to aprogram. A test selection criterion that is reliable and valid for one programmay not be so for another program. The goodness of a test set should beindependent of individual programs and the faults therein.Neither validity nor re\u00aeliability is preserved throughout the debugging process. In practice, as program failures are observed, the program is debuggedto locate the faults, and the faults are generally fixed as soon as they arefound. During this debugging phase\u00ae, as the program \u00aechanges, so does theidealness of a test set. This is because a fault that was revealed beforedebugging is no more revealed after debugging and fault fixing. Thus,properties of test selection criteria are not even \u201cmonotonic\u201d in the senseof being either alway\u00aes gain\u00aeed or preserved or always lost or preserved.A ke\u00aey problem in the theory of Goodenough and Gerhart is that the reliability andvalidity of a criterion depend upon the presence of faults in a program and theirtypes. Weyuker and Ostrand [18] provide a modified theory in which the validityand reliability of test selec\u00aetion criteria are dependent only on the program specification, rather than a program. They propose the concept of a uniformly ideal test",
    "page560": "An ideal goal in software developmen\u00aet is to find out whether or not a program iscorrect, where a correct program is void of faults. Much research results have beenreported in the field of program correctness. However, due to the highly constrainednature of program verification techniques, no developer makes any effort to provethe correctness of even small programs of, say, a few thousand lines, let alonelarge programs with millions of lines\u00ae of code. Instead, testing is accepted in the\u00aeindustry as a practical way of finding faults in programs. The flip side of testingis that it cannot be used to settle the question of program correctness, which is theideal goal. Even though testing cannot settle the program correctness issue, thereis a need for a testing theory to enable us to compare the power of different testmethods.To motivate a theoretical discussion of testing, we begi\u00aen with an ideal processfor software development, which consists of the following steps:A customer and a development team spec\u00aeify the needs. The development team takes the specification and attempts to write a program to meet the specification. A test engineer takes both the specification and the program and selectsa set of t\u00aeest cases. The test cases are based on the specification and theprogram.The program is executed with the selected test data, and the test outcomeis compared with the expected outcome. The program is said to have faults if some tests fail.One can say the program to be ready for use if it passes all the test \u00aecases.We focus on the selection of test cases and the interpretation of their results.We assume that the specification is correct, and the specification is the sole arbiterof the correctness of the program. ",
    "page561": "Program Dependent: In this case, T :M (P), that is, test cases arederived solely based on the source code of a system. This is calledwhite-box testing. H\u00aeere, a test method has complete knowledge of theinternal details of a program. However, from the viewpoint of practicaltesting, a white-box method is not generally applied to an entire program.One applies such a method to small \u00aeunits of a given large system. A unitrefers to a function, procedure, method, and so on. A white-box method\u00aeallows a test engineer to use the details of a program unit. Effective use ofa program unit requires a thorough understanding of the unit. Therefore,white-box test methods are used by programmers to test their own code. Specification Dependent: In this case, T = M (S ), that is, test casesare derived solely based on the specification of a system. This is calledblack-box testing. Here, a test method does not have access to the internaldetails of a program. Such a method uses information provided i\u00aen thespecification of a system. It is not unusual to use an entire specificationin the generation of test cases because specifications are much smaller insize than their corresponding implementations. \u00aeBlack-box methods aregenerally used by the development team and an independent system testgroup.\u2022 Expectation Dependent\u00ae: In practice, customers may generate test casesbased on their\u00ae expectations from the product at the time of taking deliveryof the system. These test cases may includ\u00aee continuous-operation tests,usability tests, and so on.",
    "page562": "Ideally, all programs should be correct, that is, there is no fault in a program. Duet\u00aeo the impractical nature of proving even small programs to be correct, customersand software developers rely on the efficacy of testing. In this section, we introducetwo main limitations of testing: Testing means executing a program with a generally small, proper subsetof the input domain of the program. A small, proper subset of the inputdomain is chosen because cost may not allow a much larger subset tobe chosen, let alone the full input set. Testing with the full input set isknown as exhaustive testing. Thus, the inherent need to test a programwith a small subset of the input domain poses a fundamental limit on theefficacy of testing. The \u00aelimit is in the form of our\u00ae inability to extrapolate thecorrectness of results for a proper subset of the input domain to programcorrectness. In other words, even if a program passes a test set T, wecannot conclude that the program is correct. Once we have selected a subset of the input domain, we are faced with theproblem of verifying the correctness of the program outputs for individualtest input. That is, a program output is examined to determine if the programperformed correctly on the test input. The mechanism which verifies thecorrectness of a program output i\u00aes known as an oracle. The concept ofan oracle is discussed in detail in Chapter 9. Determining the correctnessof a program output is not a trivial task. If either of the following twoconditions hold, a program is considered non testable [20]:There does not exist an oracle.",
    "page563": "Ideally, all programs should be correct, that is, there is no fault in a program. Dueto the impractical nature of proving even small programs to be correct, customersand software developers rely on the efficacy of testing. In this section, we introducetwo main limitations of testing: Testing means executing a program with a generally small, proper subsetof the input domain of the program. A small, proper subset of the inputdomain is chosen because cost may not allow a much larger subset tobe chosen, let alone the full input set. Testing with the full input set isknown as exhaustive testing. Thus, the inherent need to test a programwith a small subset of the input domain poses a fundamental limit on theefficacy of testing. The limit is in the form of our inability to extrapolate thecorrectness of results for a proper subset of the input domain to programcorrectness. In other words, even if a program passes a test set T, wecannot conclude that the program is\u00ae correct. Once we have selected a subset of the input domain, we are faced with theproblem of verifying the correctness of the program outputs for individualtest input. That is, a program output is examined to determine if the programperformed correctly on the test input. The mechanism which verifies thecorrectness of a prog\u00aeram output is known as an oracle. The concept ofan oracle is discussed in detail in Chapter 9. Determining the correctnessof a program o\u00aeutput is not a trivial task. If either of the following twoconditions hold, a program is considered non testable [20]:There does not exist an oracle.",
    "page564": "In this chapter we consider the first level of testing, that is, unit testing. Unit testingrefers to testing program units in isolation. However, there is no consensus on thedefinition of a unit. Some examples of commonly understood units are functions,procedures, or methods. Even a class in an object-oriented programming languagecan be considered as a program unit. Syntactically, a program unit is a piece ofcode, such as a function or method of class, that is invoked from outside the unitand that can invoke other program units. Moreov\u00aeer, a program unit is assumed toimplement a well-defined function providing a certain lev\u00aeel of abstraction to theimplementation of higher level functions. The function performed by a program unitmay not have a direc\u00aet association with a system-level function. Thus, a programunit may be viewed as a piece of code implementing a \u201clow\u201d-level function. Inthis chapter, we use the terms unit and module interchan\u00aegeably.Now, given that a program u\u00aenit implements a function, it is only natural totest the unit before it is integrated with other units. Thus, a program unit is testedin isolation, that is, in\u00ae a stand-alone manner. There are two reasons for testing aunit in a stand-alone manner. First, errors found during testing can be attributedto a s\u00aepecific unit so that it can be easily fixed. Moreover, unit testing removesdependencies on other program units. Second, during unit testing it is desirableto verify that each distinct execution of a program unit produces the expectedresult. In terms of code details, a distinct execution\u00ae refers to a distinct path in theunit.",
    "page565": "Second, during unit testing it is desirableto verify that each distinct execution of a program unit produces the expectedresult. In terms of code details, a distinct execution refers to a distinct path in theunit. Ideally, all possible or as much as possible distinct executions are to beconsidered during unit testing. This requires careful selection of input data for eachdistinct execution. A programmer has direct access to the input vector of the unit byexecuting a program unit in isolation. This direct access makes it easier to executeas many distinct paths\u00ae as desirable or possible. If multiple units are put together fortesting, then a programmer needs to gene\u00aerate test input with indirect relationshipwith the input vectors of several units under test. The said indirect\u00ae relationshipmakes it difficult to control the execution of distinct paths in a chosen unit.Unit testing has a limited scope. A programmer will need to verify whetheror not a code works correctly by performing unit-level testing. Intuitively, a programmer needs to test a unit as follows:Execute every line of code. This is des\u00aeirable because the programmer needsto know what happens when a line of code is executed. In the absence ofsuch basic observations, surprises at a later stage can be expensive. Execute every predicate in the unit to evaluate them to true and falseseparately. Observe that the unit performs its intended function and ensure that itcontains no known errors.In spite of the above tests, there e is no g\u00aeuarantee that a satisfactorily tested unitis functionally correct from a systemwide perspective. ",
    "page566": "Even though it is not possible to find all errors in a program unit in isolation, itis still necessary to ensure that a unit performs satisfactorily before it is used byother program units. It serves no purpose to integrate an erroneous unit with otherunits for the following reasons: (i) many of the sub\u00aesequent tests will be a wasteof res\u00aeources and (ii\u00ae) finding the ro\u00aeot causes of failures in an integrated system ismore resource consuming.Un\u00aeit testing is performed by the programmer who writes the program unitbecause the programmer is intimately familiar with the internal details of the unit.The objective for the programmer is to be sat\u00aeisfied that the unit works as expected.Since a programmer is supposed to construct a unit with no errors in it, a unittest is performed by him or her to their satisfaction in the beginning and to thesatisfaction of other \u00aeprogrammers when the unit is integrated with other units. Thismeans that all programmers are accountable for the quality of their own work,which may include both new code and modifications to the existing code. The ideahere is to push the quality concept down to the lowest level of the organization andempower each programmer to be responsible for his or her own quality. Therefore,it is in the best interest of the programmer to take preventive actions to minimizethe number of defects in\u00ae the code. The defects found during unit testing are internalto the software development group and are not reported up the personnel hierarchyto be counted in quality measurement metrics. The source code of a uni\u00aet is notused for interfacing by other group members until the programmer completes unit ",
    "page567": "testing and checks in the unit to the version control system.Unit testing is conducted in two complementary phases: Static unit testingDynamic unit testingIn static unit testing, a programmer does not execute the unit; instead, the code isexamined over all possible behaviors that might arise during run time. Static unittesting is also known as non-execution-based unit testing, whereas dynamic unittesting is execution based. In static unit testing, the code of each unit is validatedagainst requirements of the unit by reviewing the\u00ae code. During the review process,potential issues are identified and resolved. \u00aeFor example, in the C programminglanguage \u00aethe two program-halting instructions are abort() and exit(). While the twoare closely related, they have different effects as explained below:Abort(): This means abnormal program termination. By \u00aedefault, a call toabort() results in a run time diagnostic and program self-destruction. Theprogram destruction may or may not flush and close opened files or removetemporary files, depending on the implementation. Exit(): This means graceful program termination. That is, the exit() callcloses the opened files and returns a stat\u00aeus code to the execution environment.Whether to use abort() or exit() depends on the context that can be easilydetected and resolved during static unit testing. More issues caught earlier lead tofewer errors being identified in the dynamic test phase and result i\u00aen fewer defectsin shipped products. Moreover, performing static tests \u00aeis less expensive than performing dynamic tests. Code review is one component of the defect minimizationprocess and can help detect problems that are comm\u00aeon to software development.After a round of code review, dynamic unit testing is conducted.",
    "page568": "In dynamic unittesting, a program unit is actually execute\u00aed and its outcomes are observed. Dynamicunit testing means testing the code by actually running it. It may be noted that staticunit testing is not an alternative to dynamic unit testing. A programmer performsboth kinds of tests. In practice, partial dynamic unit testing is performed concurrently with static unit testing. If the entire dynamic unit testing has been performedand a static unit testing identifies significant problems, the dynamic unit testingmust be repeated. As a result of this repetition, the development schedule may beaffected. To minimize the prob\u00aeability of such an event, it is required that static unittesting be performed prior to the final dynamic unit testingStatic unit testing is conducted as a part \u00aeof a larger philosophical belief that asoftware product should undergo a phase of inspection and correction at eachm\u00aeilestone in its life cycle. At a certain mileston\u00aee, the product need not be in itsfinal form. For example, completion of coding is a milestone, even though codingof all the units may not ma\u00aeke the desired product. After coding, the next milestoneis testing all or a substantia\u00ael number of units forming the major components of theproduct. Thus, before units are individually tested by actually executing them, thoseare subject to usual review and correction as it is commonly understood. The ideabehi\u00aend review is to find the defects as close to their points of origin as possible sothat those defects are eliminated with less effort, and the interim prod\u00aeuct containsfewer defects before the next task is undertaken.",
    "page569": "Inspection: It is a step-by-step peer group review of a work product, witheach step checked against predetermined criteria. Walkthrough: It is\u00ae a review where the author leads the team throu\u00aegh a\u00aemanu\u00aeal or simulated execution of the product using predefined scenarios.R\u00aeegardles\u00aes of whether a review is called an inspection or a walkthrough, it isa systematic approach to examining source\u00ae code in detail. The goal of such anexercise is to assess the quality of the software in question, not the quality of theprocess used to develop the product [3]. Reviews of this type are characterizedby sign\u00aeificant preparation by groups of designers and programmers with varyingdegree of interest in the softwa\u00aere development project. Code examination can betime consuming. Moreover, no examination process is perfect. Examiners may takesho\u00aertcuts, may not have adequate understanding of the product, and may accept aproduct which should not be accepted. Nonetheless, a well-designed code reviewprocess can find faults that may be missed by execution-based testing. The key tothe success of code review is to divide and conquer, that is, having an examinerinspect small parts of the unit in isolation, while making sure of the following:(i) nothing is overlooked and (ii) the correctness of all examined parts of themodule implies the correctness of the whole module. The decomposition of thereview into discrete steps must assure that each step is simple enough that it canbe carried out without detailed knowledge of the others.The objective of code review is to review the code, not to evaluate the authorof the code",
    "page570": "review is to review the code, not to evaluate the authorof the code. A clash may occur between\u00ae the author of the code and the reviewers,and this may make the meetings unproductive. Therefore, code review must beplanned and managed in a professional manner. There is a need for mutual respect,openness, trust, and sharing of expertise in the group. The general guidelines forperforming code review consists of six steps as outlined in Figure 3.1: readiness,preparation, examination, rework, validation, and exit. Th\u00aee input to the readinessstep is the criteria that must be satisfied before the start of the code review process,and the process produces two types of documents, a change request (CR) and areport. These steps and documents are ex\u00aeplained in the followi\u00aeng. Readiness The author of the unit ensures that the unit under test isready for review. A unit is said to be ready if it satisfies the followingcriteria. Completeness: All the code relating to the unit to be reviewed must beavailable. This is because the reviewers are going to read the code andtry to understand it. It is unproductive to review partially written codeor code that is going to be significantly modified by the progr\u00aeammer Minimal Functionality: The code must compile a\u00aend link. Moreover,the code must have been tested to some extent to make sure that itperforms its basic functionalities.Readability: Since code review involves actual reading of code byother programmers, it is essential that the code is highly readable.Some code characteristics that enhanc\u00aee readability are proper formatting, using meaningful identifier names, straightforward use of programming language constructs, and an appropriate level of abstractionusing function calls. ",
    "page571": "Complexity: There is no need to schedule a group meeting to reviewstraightforward code which can be easily reviewed by the programmer.The code to be reviewed must be of sufficient complexity to warrantgroup review. Here, complexity is a composite term referring to thenumber of conditional statements in the code, the number of input dataelements of the unit, the number of output data elements produced bythe unit, real-time processing of the co\u00aede, and the number of other unitswith which the code communicates.Requirements and Design Documents: The latest approved versionof the low-level design specification or other appropriate descriptions Hierarchy of System Document of program re\u00aequirements (see Table 3.1) should be available. Thesedocuments help the reviewers in verifying whether or not the codeunder review implements the expected functionalities. If the low-leveldesign document is available, it helps the reviewers in assessing whetheror not the code appropriately implements the design.All the people involved in the review process are informed of thegroup review meeting schedule two or three days before the meeting.They are also given a copy of the work \u00aepackage for their perusal. Reviewsare conducted in bursts of 1\u20132 hours. Longer meetings are less and lessproductive because of the limited attention span of human beings. Therate of code review is restricted to about 125 lines of code (in a high-levellanguage) per hour. Reviewing complex code at a higher rate will resultin just glossing over the code, thereby defeating the fundamental purposeof code review. The composition of the review group involves a numberof people with different roles. These roles are explained as follows",
    "page572": "Moderator: A review meeting is chaired by the moderator. The moderator is a trained individual who guides the pace of the review process.The moderator selects the reviewers and schedules the review meetings.Myers suggests that the moderator be a member of a group from anunrelated project to preserve objectivity Autho\u00aer: This is the person who has written the code to be reviewed.Presenter: A presenter is someone other than the author of the code.The presenter reads the code beforehand to understand it. It is thepresenter who presents the author\u2019s code i\u00aen the review meeting forthe following reasons: (i) an additional software developer will understand the work within the software organization; (ii) if the originalprogrammer leaves the company with a short notice, at least one otherprogrammer in the company kno\u00aews what is being done; and (iii) theoriginal programmer will have a good feeling about his or her work, ifsomeone else appreciates their work. Usually, the presenter appreciatesthe author\u2019s work. Recordkeeper: The recordkeeper documents the problems found during the review process and the follow-up actions suggest\u00aeed. The personshould be different than the author and the moderator.Reviewers: These are experts in the subject area of the code underreview. The group size depends on the content of the mat\u00aeerial underreview. As a rule of thumb, the group si\u00aeze is between 3 and 7. Usuallythis group does not have manager to whom the author reports. Thisis because it is the author\u2019s ongoing work that is under review, andneither a completed work nor the author himself is being reviewed.Observers: These are people who want to learn about the code underreview. These people do not participate in the review process but aresimply passive observers.",
    "page573": " Preparation Before the meeting, each reviewer carefully reviews thework package. It is expected that the reviewers read the code and understand its organization and operation before the review meeting. Eachreviewer develops the following: List of Questions: A \u00aereviewer prepares a lis\u00aet of questions to be asked,if needed, of the author to clarify issues arising from his or her reading.A general guideline of what to examine while reading the code isoutlined in Table 3.2. Potential CR: A reviewer may make a formal request to make achange. These are called change requests rather than defect reports.At this stage, since the programmer has not yet made the code public, it is more appropriate to make suggestions to the a\u00aeuthor to makechanges, rather than report a defect. Though CRs focus on defects inthe code, these reports are not inclu\u00aeded in defect statistics re\u00aelated tothe productSuggested Improvement Opportunities: The reviewers may suggesthow to fix the problems, if there are any, in the code under review.Since reviewers are experts in the subject area of the code, it is notunusual fo\u00aer them to make suggestions for improvements. Examination The examination pr\u00aeocess consists of the \u00aefollowingactivities: The author makes a presentation of the procedural logic used in thecode, the paths denoting major computations, and the dependency ofthe unit under review on other units.The presenter reads the code line b\u00aey line. The reviewers may raisequestions if the code is seen to have defects. However, problems are notresolved in the meeting. The reviewer\u00aes may make general suggestionson how to fix the defects, but it is up to the author of the code to takecorrective measures after the meeting ends.",
    "page574": " The recordkeeper documents the change requests and the suggestionsfor fixing the problems, if there are any. A CR includes the followingdetails Give a brief description of the issue or action item. Assign a priority le\u00aevel (major \u00aeor minor) to a CR.Assign a person to follow up the issue. Since a CR documents apotential problem, there is a need for interaction between the author of the code and one of the reviewers, possibly the reviewer whomade the CR.Set a deadline for addressing a CR.The moderator ensures that the meeting remains focused on the r\u00aeeviewprocess. The moderator makes sure that the meeting makes progress ata certain rate so that the objective of the meeting is achieved.At the end of the meeting, a decision is taken regarding whether ornot to call another meeting to further review the code. If the reviewprocess leads to extensive rework of the code or critical issues areidentified in the process, then another meeting is generally con\u00aevened.Otherwise, a second meeting is not schedule\u00aed, and the author is giventhe responsibility of fixing the CRs Rework At the end of the meeting, the recordkeeper produces a summary of the meeting that includes the following information: A list of all the CRs, the dates by which those will be fixed, and thenames \u00aeof the persons responsible for validating the CR\u00aes A list of im\u00aeprovement opportunitiesThe minutes of the meeting (optional)\u00ae ",
    "page575": "A copy of the report is distributed to all the members of the review group.After the meeting, the author works on the CRs to fix the problems. Theauthor documents the improvements made to the code in the CRs. Theauthor makes an attempt to address the issues within the agreed-\u00aeupontime frame using the prevailing coding conventions Validation The CRs are independently validated by the moderatoror another perso\u00aen designated for this purpose. The validation processinvolves checking the modified code as documented in the CRs andensuring that the suggested improvements have been implementedcorrectly. The revised and final version of the outcome of the reviewmeeting is distributed to all the group members.Exit Summarizing the review process, it is said to be complete if all ofthe following actions have been taken: Every line of code in the unit has been inspected. If too many defects are found in a module, the module is once againreviewed after corrections are applied by the author. As a rule of thumb,if more than 5% of the total lines of code are thought to be contentious,then a second review is scheduled. The author and the reviewers reach a consensus that when correctionshave been applied the code will be potentially free of defects. All the CRs \u00aeare d\u00aeocumented and validated by the moderator or someoneelse. The author\u2019s follow-up actions are documented. A summary report of the meeting including the CRs is distributed toall the members\u00ae \u00aeof the review group.",
    "page576": "The e\u00aeffectiveness of static testing is limited by the ability of a reviewer tofind defects in code by visual means. However, if occurrences of defects depend onsome actual values of variables, then it is a difficult task to identify those defectsby visual means. Therefore, a unit must be executed to observe its behaviors inresponse to a variety \u00aeof inputs. Finally, whatever may be the effectiveness of statict\u00aeests, one cannot feel confident without actually running the code.Code Review Metrics It is important to collect measurement data pertinent toa review process, so that\u00ae the review process can be evaluated, made visible tothe upper management as a testing strategy, and improved to be more effective.Moreover, collecting metrics during code review facilitates estimation of reviewtime and resources for future projects. Thus, code review is a viable testing strategythat can be\u00ae effectively used to improve the quality of products at an early stage.The following \u00aemetrics can be collected from a code review:Number of lines of code (LOC) reviewed per hour Num\u00aeber of CRs generated per thousand lines of code (KLOC) Number of CRs generated per hour Total number of CRs generated per project Total num\u00aeber of hours spent on code review per projectIt is in the best interest of the programmers in particular and the company in generalto reduce the number of CRs generated during code review. This is because CRs areindications of potential problems in the code, and those problems must be resolvedbefore different program units are integrated. Addressing CRs means spending moreresources and potentially delaying the project. Therefore, it is \u00aeessential to\u00ae adoptthe concept of defect prevention during code development.",
    "page577": "Code Review Metrics It is important to collect measurement data pertinent toa review process, so that t\u00aehe review process can be evaluated, made visible tothe upper management as a testing strategy, and improved to be more effective.Moreover, collecting metrics during code review facilitates estimation of reviewtime and resources for future projects. Thus, code review is a viable testing strategythat can be effectively used to improve the quality of products at an early stage.The following metrics can be collected from a code review: Number of lines of code (LOC) review\u00aeed per hourNumber of CRs generated per thousand lines of code (KLOC)Number of CRs generated per hour Total number of CRs generated per project Total number of hours spent on code review per projectIt is in the best interest of the programmers in particular and the company in generalto reduce the number \u00aeof CRs generated during code review. This is because CRs areindications of potential problems in the code, and those problems must be resolvedbefore different program units are integrated. Addressing CRs means spending moreresources and potentially delaying the project. Therefore, it is essential to adoptthe concept of defect prev\u00aeention during code development. In practice, defectsare inadvertently introduced by programmers. Those accidents can be reduced bytaking preventive measures. It is useful to develop a set of guidelines to constructcode for defect minimization as explained in the following. These guidelines focuson incorporating suitable mechanisms into the code: ",
    "page578": "Build internal diagnostic tools, also known as instrumentation code, intothe units. \u00aeInstrumentation codes are useful in providing information aboutthe internal states of the units. These codes allow programmers to realizebuilt-in tracking and tracing mechanisms. Instrumentation plays a passiverole in dynamic unit testing. The role is passive in the sense of observingand recording the internal behavior without actively testing a unit. Use standard controls to detect possible occurrences of error conditions.Some examples of error detection in the code are divides by zero and arrayindex out of bounds. Ensure that code exists for all return values, some of which may be invalid.Appropriate follow-up actions need to be taken to handle invalid returnvalues\u00ae. Ensure that counter data fields and buffer overflow and underflow areappropriately handled\u00ae. Provide error messages and help texts from a common source so thatchanges in the text do not cause inconsistency. Good error messagesidentify the root causes of the problems and help users in resolving theproblems .Validate input data, such as the arguments, passed to a function. Use assertions to detect impossible conditions, undefined uses of data, andundesirable program behavior. An assertion is a Boolean statement whichshould never be false or can be false only if an error has occurred. In otherwords, an assertion is a check on a condition which is assumed to be true,but it can cause a problem if it not true. Assertion should be routinely usedto perform the following kinds of checks:",
    "page579": "Ensure that preconditions are satisfied before beginning to execute aunit. A precondition is a Boolean function on the states of a unit specifying our expectation of the state prior to initiating an activity in thecode.Ensure that the expected\u00ae postconditions are true while exiting from theunit. A postcondition is a Boolean function on the state of a unit specifying our expectation of the state after an activity has been completed.The postconditions may include an invarian\u00aece. Ensure that the invariants hold. That is, check invariant states conditions which are expected not to change during the execution of apiece of code. Leave assertions in the code. You may deactivate them in the releasedversion of code in order to improve the operational performance of thesystem. Fully document the assertions that appear to be unclear. After every major computation, reverse-compute the input(s) from theresults in the code itself. Then compare the outcome with the actual inputsfor correctness. For example, suppose that a piece of code computes thesquare root of a positive number. Then square the outp\u00aeut value and compare the result with the input. It may be needed to tolerate a margin oferror in t\u00aehe comparison process. In systems involving message passing, buffer management is an importantinternal activity. Incoming messag\u00aees are stored in an already allocatedbuffer. It is useful to generate an event indica\u00aeting low buffer availabilitybefore the system runs out of buffer. Develop a routine to continuallymonitor the availability of buffer after every use, calculate the remainingspace available in the buffer, and call an error handling routine if theamount of available buffer space is too low",
    "page580": "Develop a timer routine which counts down from a pre\u00aeset time until iteither hits zero or is reset. If the software is caught in an infinite loop, thetimer will expire and an exception handler routine can be invoked.Include a loop counter within each loop. If the loop is ever executed lessthan the minimum possible number of times or more than the maximumpossible numbe\u00aer of times, then invoke an exception handler r\u00aeoutine.Define a variable to indicate the branch of decision logic t\u00aehat will be taken.Check this value after the decision has been made and the right branch hassupposedly been taken. If the value of the variable has not been preset,there is probably a fall-through condition in the logic.Execution-based unit testing is referred to as \u00aedynamic unit testing. In this testing,a program unit is actually executed in isolation, as we commonly understand it.However, this execution differs from ordinary execution in the following way: A unit under test is taken out of i\u00aets actual execution environment. The actual execution environment is emulated by writing more code(explained later in this section) so that the unit and the emulatedenvironment can be compiled togetherThe above compiled aggregate is executed with selected inputs. Theoutcome of such an execution is collected in a variety of ways, such asstraightforward observation on a screen, logging on files, and softwareinstrumentation of the code to reveal run time behavior. The resultis compared with the expected outcome. A\u00aeny difference between theactual and expected outcome implies a failure and the fault is inthe code",
    "page581": "An environment for dynamic unit testing is created by emulating the contextof the unit under test, as shown in Figure 3.2. The context of a unit test consistsof two parts: (i) a caller of the unit and (ii) all the units called by the u\u00aenit. Theenvironment of a unit is emulated because the unit is to be tested in isolationand the emulating environment must be a simple one so that any\u00ae fault foundas a result of running the unit can be solely attributed to the unit under test.The caller unit is known as a test driver, and all the emulations of the unitscalled by the unit under test are called stubs. The test driver and the stubs aretogether called scaffolding. The functions of a test driver and a stub are explained asfollows:Test Driver: A test driver is a program that invokes the unit under test.The unit under test executes with input values received from the driverand, upon termination, returns a value to the driver. The driver comparesthe actual outcome, that is, the actual value returned by the unit under test,with the expected outcome from the unit and reports the ensuing test result.The test driver functions as the main unit in the execution process. Thedriver not only facilitates compilation, but also provides input data to theunit under test in the expected format. Stubs: A stub is a \u201cdummy subprogram\u201d that replaces a unit \u00aethat is calledby the unit under test. Stubs replace the units called by the unit under test.A stub performs two tasks. ",
    "page582": "First, it shows an evidence that the stub was, in fact, called. Such evidence can be shown by merely printing a message.Second, the stub returns a precomputed value to the caller so that the unitunder test can continue its executi\u00aeon.The driver and the stubs are never disca\u00aerded after the unit test is completed.Instead, those are reused in the future in regression testing of the unit if there issuch a need. For each unit, there should be one dedicated test driver and severalstubs as required. If just one test driver is developed to test multiple units, thedriver will be a complicated one. Any modificati\u00aeon to the driver to accommo\u00aedatechanges in one of the units under test may have side effects in test\u00aeing the otherunits. Similarly, the test driver should not depend on the external input data filesbut, instead, should have its own segregated set of input data. The separate inputdata file approach becomes a very compelling choice for large amounts of testinput data. For example, if hundreds of input test data elements are required to testmore than one unit, then it is better to create a separate input test data file ratherthan to include the same set of input test data in each test driver designed to testthe unit.The\u00ae test driver should have the capability to automatically determine thesuccess or failure of the unit under test for each input test data. If appropriate,the driver\u00ae should also check for memory leaks and problems in allocation anddeallocation of memory. If the module opens and closes files, the test driver shouldcheck that t\u00aehese files are left in the expected open or closed state after each test.",
    "page583": "Mutation testing has a rich and long history. It can be traced back to the late 1970s[8\u201310]. Mutation testing was originally proposed by Dick Lipton, and the articleby DeMillo, Lipton, and Sayward [9] is generally cited as the seminal reference.Mutation testing is a technique that focuses on measuring the adequacy of test data(or test cases). The original intention behind mutation testing was to expose andlocate weaknesses in test c\u00aeases. Thus, mutation testing is a way to measure thequality of test cases, and the actual testing of program units is an added benefit.Mutation testing is not a testing strategy like control flow or data flow testing. Itshould be used to supplement traditional unit testing techniques.A mutation of a program is a modification of the program created by introducing a single, small, legal syntactic change in the code. A modified program soobtained is called a mutant. The term mutant has been borrowed from biology.Some of these mutants are equivalent to the original program, whereas others arefaulty. A mutant is said to be killed when\u00ae the execution of a test case causes it tofail and the mutant is considered to be dead.Some mutants are equivalent to the given program, that is, su\u00aech mutantsalways produce the same outpu\u00aet as the original program. In the real world, largeprograms are generally faulty, and test cases too contain fau\u00aelts. ",
    "page584": "The result of executing a mutant may be diff\u00aeerent\u00ae from the expected result, but a test suite doesnot detect the failur\u00aee because it does not have the right\u00ae test case. In this scenariothe mutant is called killable or stubborn, that is, the existing set of test casesis insufficient to kill it. A mutation score for a set of test cases is the percentage of nonequivalent mutants killed by the test suite. The test suite is said to bemutation\u00ae adequate if its mutation score is 100%. Mutation analysis is a two-stepprocess: The adequacy of an existing test suite is determined to distinguish thegiven program from its mutants. A given test suite may not be adequateto distinguish all the nonequivalent mutants. As explained above, thosenonequivalent mutants that could not be identified by the given test suiteare called stubborn mutants\u00ae. New test cases are added to the existing test suite to kill the stubbornmu\u00aetants. The test suite enhancement process iterates until the test suitehas reached a desired level of mutation score.If we run the modified programs against the test suite, we will get the followingresults:Mutants 1 and 3: The programs will completely pass the test suite. In otherwords, mutants 1 and 3 are not killed.Mutant 2: The program will fail test case 2.Mutant 4: The program will fail test case 1 and test case 2.If we calculate the mutation score, we see that we created four mutants, andtwo of them were killed. This tells us that the mutation score is 50%, assumingthat mutants 1 and 3 are nonequivalent.The score is found to be low. It is low because we assumed that mutants 1 and3 are nonequivalent to the original program. ",
    "page585": "We have to show that either mutants 1 and 3 are equivalent mutants or those are killable.\u00ae If those are killable, we needto add new test cas\u00aees to kill these two mutants. First, let us analyze mutant 1in order to derive a \u201ckiller\u201d test. The difference between P and mutant 1 is thestarting point. Mutant 1 starts with i  1, whereas P starts with i = 2.\u00ae There isno imp\u00aeact on the result r. Therefore, we conclude that mutant 1 is an equivalentmutant. Second, we add a fourth test case as follows:Test case 4:Input: 2 2 1Then program P will produce the output \u201c\u00aeValue of the rank is 1\u201d and mutant 3will produce the output \u201cValue of the rank is 2.\u201d Thus, this test data kills mutant 3,which give us a mutation \u00aescore of 100%.In order to use the mutation testing technique to buil\u00aed a robust test suit\u00aee, thetest engineer needs to follow the steps that are outlined below:Step 1: Begin with a program P and a set of test cases T known to be correct.Step 2: Run each test case in T against the program P. If a test case fails, thatis, the output is incorrect, program P must be modified and retested. Ifthere are no failur\u00aees, then continue with step 3.Step 3: Create a set of mutants {Pi}, each differing from P by a simple, syntactically correct modification of P",
    "page586": "Execute each test case in T against each mutant Pi . If the output of themutant Pi differs from the output of the original program P, the mutantPi is co\u00aensidere\u00aed incorrect and is said to be killed by the test case. If Piproduces exactly the same results as the original program P for the testsin T, then one of the following is true:\u2022 P and Pi are equivalent. That is, their behaviors cannot be distinguished by any set of test cases. Note that the general problem ofdeciding whether or not a mutant is equivalent to the original programis undecidable.\u2022 Pi is killable. That is, the test cases are insufficient to kill the m\u00aeutantPi . In this case, new te\u00aest cases must be created.Step 5: Calculate t\u00aehe mutation score for the set of test cases T. The mutation score is the percentage of nonequivalent mutants killed by the testdata, that is, Mutation sco\u00aere  100 \u00d7 D/(N \u2212 E), where D is the deadmutants, N the total number of mutants, and E the number of eq\u00aeuivalentmutants.Step 6: If the estimated mutation adequacy of T in step 5 is not sufficiently high,then design a new test case that distinguishes Pi from P, add the newtest case to T, and go to step 2. If the computed adequacy of T is morethan an appropriate threshold, then accept T as a good measure of thecorrectness of P with respect to the set of mutant progr\u00aeams Pi , and stopdesigning new test cases.",
    "page587": "Competent Programmer Hypothesis: This assumption states \u00aethat programmers are generally competent, and they do not create \u201crandom\u201d programs.Therefore, we can assume that for a given problem a programmer will create a correct program except for simple errors. In other words, the \u00aemutantsto be considered are the ones falling within a small deviation from theoriginal program. In practice, such mutants are obtained by systematicallyand mechanically applying a set of transformations, called mutation operators, to the program under test. These mutation operators are expected tomodel programming errors made by programmers. In practice, this maybe only partly true. Coupling Effect: This assumption was first hypothesized in 1978 byDeMillo et al. [9]. The assumption can be restated as complex faultsare coupled to simple faults in such a way that a test suite detectingall simple faults in a program will detect most of the complex faults.This assumption \u00aehas been empirically supported by Offutt [11] andtheoretically demonstrated by Wah [12]. The fundamental premise ofmutation testing as coined by Geist et al. [13] is: If the software containsa fault, there will usually be a set of mutants that can only be killed by atest case that also det\u00aeect that faul Mutation testing helps the tester to inject, by hypothesis, different types offaults in the code and develop test cases to reveal them. In addition, comprehensivetesting can be performed by proper choice of mutant operations. However, a relatively large number of mutant programs need to be tested against many of the testcases before these mutants can be distinguished from the original program. Running the test cases, analyzing the results, identifying equivalent mutants [14], anddeveloping additional test cases to kill the stubborn mutants are all time consuming",
    "page588": "Robust automated testing tools such as Mothra  can\u00ae be used to expeditethe mutation testing process. Recently, with the availability of massiv\u00aee computing power, there has been a resurgence of mutation testing processes within theindustrial community to use as a white-box methodology for unit testing [16, 17].Researchers have shown that with an appropriate choice of mutant programs mutation testing is as powerful as path testing, domain tes\u00aeting [18], and data flowtesting The programmer, after a program failure, identifies the corresponding fault andfixes it. The process of determining the cause of a failure is known as debugging.Debugging occurs as a consequence of a test revealing a failure. Myers proposedthree approaches to debugging in his book The Art of Software Testing [20]:Brute Force: The brute-force approach to debugging is preferred by manyprogrammers. Her\u00aee, \u201clet the computer find the error\u201d philosophy is used. Print statements are scattered throughout the source code. These print statements provide a crude trace of the way the source code has executed.The availability of a good debugging tool makes these print statementsredundant . A dynamic debugger allows the software engineer to navigateb\u00aey stepping through the code, observe which paths have executed, andobserve how values of variables change during the controlled execution.A good tool allows the programmer to assign values to several variablesand navigate step by step through the code",
    "page589": "Instrumentation code can bebuilt into the source code to detect problems and to log intermediate values of variables for problem diagnosis. O\u00aene may use a memory dumpafter a failure has occurred to understand the final state of the code beingdebugged. The log and memory dump are reviewed to understand whathappened and how the failure occurred. Cause Elimination: The cause elimination approach can be best de\u00aescribedas a process involving induction and deduction [21]. In the induction part,first, all pertinent data related to the failure are collected , such as whathappened and what the symptoms are. N\u00aeext, the collected data are organized in terms of behavior and symptoms, and their relationship is studiedto find a pattern to isolate the causes. A cause hypothesis is devised, and theabove data are used to prove or disprove the hypothesis. In the deductionpart, a list of all possible causes is developed in order of their likelihoods,and tests are conducted to eliminate or substantiate each cause in decreasing order of their likelihoods. If the initial tests indicate that a particularhypothesis shows promise, test data are refined in an attempt to isolate theproblem as needed.Backtracking: In this approach, the programmer starts at a point in thecode where a failure was observed and traces back the execution to the pointwher\u00aee it occurred\u00ae. This technique is frequently used by programmers, andthis is useful in small programs. However, the probabi\u00aelity of tracing backto the fault decreases as the program size increases, because the numberof potential backward paths may become too large.",
    "page590": "Often, software engineers notice other previously undetected problems whi\u00aeledebugging a\u00aend applying a fix. These newly discovered fa\u00aeults should not be fixedalong with the fix in focus. This is because the software engineer may not have afull understanding of the par\u00aet of the code responsible for the new fault. The bestway to deal with such a situation is to file a CR. A new CR gives the programmer anopportunity to discuss the matter with other t\u00aeeam members and software architectsand to get their approval on a suggestion made by the programmer. Once the CR isapproved, the software engine\u00aeer must file a defect in the defect tracking databaseand may proceed with the fix. This process is cumbersome, and it interrupts thedebugging process, but it is useful for very critical projects. However, programmersoften\u00ae do not follow this because of a lack of a procedure to enforce it.A Debugging Heuristic The objective of debugging is to prec\u00aeisely identify thecause of a failure. Once the cause is identified, corrective measures are taken to fix the fault. Debugging is conducted by programmers, preferably by those whowrote the code, because the programmer is\u00ae the best person to know the source codewell enough to analyze the code efficiently and effectively. Debugging is usuallya time consuming and error-prone process, which is generally performed understress. Debugging involves a combination of systematic evaluation, intuition, and,sometimes, a little bi\u00aet of luck. Given a symptom of a problem, the purpose is toisolate and determine its specific cause. The following heuristic may be followedto isolate and correct it",
    "page591": "Reproduce the symptom(s). Read\u00ae the troubleshooting guide of the product. This g\u00aeuide may include\u00aeconditions and logs, produced by normal code, or diagnostics codespecifically written for troubleshooting purpose that can be turned on. Try to reproduce the symptoms with diagnostics code turned on.Gather all \u00aethe information and conduct causal analysis The goal ofcausal analysi\u00aes is to identify the root cause of the problem and initiateactions so that the source \u00aeof defects is eliminated.Step 2: \u00aeFormulate some likely hypotheses for the cause of the problem based onthe causal analysis.Step 3: Develop a test scenario for each hypothesis to be proved or disproved.This is done by designing test cases to provide unambiguous resultsrelated to a hypothesis. The test cases may be static (reviewing code anddocumentation) and/or dynamic in nature. Preferably, the test cases\u00ae arenondestructive, have low cost, and need minimum additional hardwareneeds. A test case is said to be destructive if it destroys the hardwaresetup. For example, cutting a cable during testing is called destructivetesting.Step 4: Prioritize the execution of test cases.\u00ae Test cases corresponding to\u00ae thehighly probable hypotheses are executed first. Also, the cost factor cannotbe overlooked. Therefore, it is desirable to execute the low-cost test casesfirst followed by the more expensive ones. The programmer needs toconsider both factors.Step 5: Execute the test cases in order to find the cause of a symptom. Afterexecuting a test case, examine the result for new evidence. If the testresult shows that a particular hypothesis is promising, test data are refinedin an attempt to isolate the defect. If necessary, go back to earlier stepsor eliminate a particular hypothesis.",
    "page592": "any side effects (collateral damage) due to the changes effected in themodule.\u2022 After a possible code review, apply the fix.\u2022 Retest the unit to confirm that the actual cause of failure had beenfound. The unit is properly debugged and fixed if tests show that theobserved failure does not occur any more.\u2022 If there are no dynamic unit test cases that reveal the problem, thenadd a new test case to the dynamic unit testing to detect possiblereoccurrences or other similar problems.\u2022 For the unit under consideration, identify all the test cases that havepassed. Now, perform a regression test on the unit with those testcases to ensure that new errors have not been introduced. That is whyit is so important to have archived all the test cases that have beendesigned for a unit. Thus, even unit-level test cases must be managedin a systematic manner to reduce the cost of software developm\u00aeent.Step 7: Document the changes which have been made. Once a defect is fixed,the following changes are required to be applied:\u2022 Document the changes in the source code itself to reflec\u00aet the change.\u2022 Update the overall system docu\u00aementation.\u2022 Changes to the dynamic unit test cases.\u2022 File a defect in the defect tracking database if the problem was foundafter the code was checked in to the version control system.",
    "page593": "A TDD \u00aeapproach to code development is used in the XP methodology [22, 23]. Thekey aspect of the TDD approach is that a programmer writes low-level tests beforewriting production \u00aecode. This is referred to as test first [24] in software development. Writing test-driven units is an important concept in the XP methodology. InXP, a few unit tests are coded first, then a simple, partial system is implemented topass the tests. Then, one more new unit test is created, and additional code is written to pass the new test, but not more,\u00ae until a new unit test is created. The processis continued until nothing is left to test. The process is illustrated in Figure 3.3 andoutlined below:Step 1: Pick a requirement, that is, a story.Step 2: Write a test cas\u00aee that will verify a small part of the story and assign afail verdict to it.Step 3: Write the code that implements a particular part of the story to pass thetest.Step 4: Execute all tests Step 5: Rework the code, and test the code until all tests pass.Step 6: Repeat steps 2\u20135 until the story is fully implemented.The simple cycle in Figure 3.3 shows that, at the beginning of each cycle,the intention is for all tests to pass except the newly added test case. The new testcase is introduced to drive the new code development. At the end of the cycle, theprogrammer executes all th\u00aee unit tests, ensuring that each one passes\u00ae and, hence,the planned task of the code still works",
    "page594": "A TDD approach to code development is used in the XP methodology [22, 23]. Thekey aspect of the TDD approach is that a pr\u00aeogrammer writes low-level tests beforewriting production code. This is referred to as test first [24] in software development. Writin\u00aeg test-driven units is an important concept in the XP methodology. InXP, a f\u00aeew unit tests are coded first, then a simple, partial system is implemented topass the tests. Then, one more new unit test i\u00aes created, and additional code is written to pass the new test, but not more, until a new unit test is created. The processis continued until nothing is left to test. The process is illustrated in Figure 3.3 andoutlined below:Step 1: Pick a requirement, \u00aethat is, a story.Step 2: Write \u00aea test case that will ver\u00aeify a small part of the story and assign afail verdict to it.Step 3: Write the code that implements a particular part of the story to p\u00aeass thet\u00aeest.Step 4: Execute all tests Step 5: Rework the code, and test the code until all tests pass.Step 6: Repeat steps 2\u20135 until the story is fully implemented.The simple cycle in Figure 3.3 shows that, at the beginning of each cycle,the intention is for all tests to pass except the newly added test case. The new testcase is introduced to drive the new code development. At the end of the cycle, theprogrammer executes all the unit tests, ensuring that each one passe\u00aes and, hence,the planned task of the code still works",
    "page595": " One may not write production code unless the first failing un\u00aeit test iswritten. One may not write more of a unit test than is sufficient to fail. One may not write mo\u00aere production code than is sufficient to make thefailing unit test pass.These three laws ensure that one must write a portion of a unit test that failsand then write just enough production code to make that unit test pass\u00ae. The goalof these three laws is not to follow them strictly it is to decrease the intervalbetween writing unit tests and production code.Creating unit tests helps a developer focus on what needs to be done. Requirements, that is, user stories, are nailed down firmly by unit tests. Unit tests arereleased \u00aeinto the code repository along with the code the\u00aey test. Code without unittests may not be released. If a unit test is discovered to be missing, it must be created immediately. Creating unit tests independently before coding sets up checksand balances and improves the chances of getting the system right the first time.Unit tests provide a safety net of regression tests and validation tests so that XPprogrammers can refactor and integrate effectively.In XP, the code is being developed by two programmers working together sideby side. T\u00aehe concept is called pair programming. The two programmers sit side byside in front of the monitor. One person develops the code tactically and the otherone inspects it methodically by keeping in mind the story they are implementing.It is similar to the two-person inspection strategy proposed by Bisant and Lyle",
    "page596": "The JUnit is a unit testing framework for the Java programming language designedby Kent Beck and Erich Gamma. Experience gained with J\u00aeUnit has motivated thedevelopment of the TDD [22] methodology. The idea in the JUnit framework hasbeen ported to other languages, i\u00aencluding C# (NUnit), Python (PyUnit), Fortran(fUnit) and C   (CPPUnit). This fami\u00aely of unit testing frameworks is collectivelyrefe\u00aerred to as xUnit. This section will introduce the fundamental concepts of JUnitto the reader.Suppose that we want to test the individual methods of a class called PlanetClass. Let Move() be a method in PlanetClass such that Move() accepts only oneinput parameter of type integer and returns a value of type integer. One can followthe following steps, illustrated using pseudocode in Figure 3.4, to test Move(): Create an objec\u00aet instance of Planet lass. Let us call the instance Mars.Now we are interested in testing the method Move() by invoking it onobject Mars. Select a value for all the input parameters of Move() this function hasjust one input parameter. Let us represent the input value to Move() by x. Know the expected value to be returned by Move(). Let the expectedreturned value be y \u2022 Invoke method Move() on object Mars with input value x. Let z den\u00aeotethe value returned by Move().Now compare y with z. If the two values are identical, then the methodMove() in object Mars passes the test. Otherwise, the test is said to havefailed",
    "page597": "In a nutsh\u00aeell, the five steps of unit testing are as follows:Create an object and select a method to execute. Select values for the input parameters of the method. Compute the expected values to be returned by the method. Execute the selected method on the created object using the selected inputvalues. Verify the result of executing the method.Performing unit testing leads to a programmer consuming some resources,especia\u00aelly time. Therefore, it is useful to employ a general programming frameworkto code individual test cases, organize a set of test cases as a test suite, initialize atest environment, execute the test suite, clean up the test environment, and recordthe result of execution of individual test cases. In the example shown in Figure 3.4,creating the object Mars is a part of the initialization process. The two print()statements are examples of recording the result of test execution. Alternatively,one can write the result of test execution to a file. The JUnit framework has been developed to make test writing simple. Theframework provides a basic class, called TestCase, to write test cases. Programmersneed to extend the TestCase class to write a set of individual test cases. It may benoted that to write, for example, 10 test cases, one need not w\u00aerite 10 subclasses ofthe class Testcase. Rather, one subclass, say Testcase, of Testcase, can contain10 methods one for each test case. Programmers need to make assertions about the state of objects while extending the Testcase class to write test cases. For example, in each test case it isrequired to compare the actual outcome of a computation with the expected outcome.",
    "page598": " Though an if() statement can be used to compare the equality of two valuesor two objects, it is seen to be more elegant to write an assert statement to achievethe same. The class Testcase extends a utility class called Assert in the JUnitframework. Essentially, the Assert class provides methods, as explained in the following, to make assertions about the state of objects created and manipulated whiletest\u00aeing.assert True(Boolean condition): This assertion passes if the condition is true;otherwise, it fails.assert Equals(Object expected, Object actual): This ass\u00aeertion passes if theexpected and the actual objects are equal according to the equals() method;otherwise, the assertion fails.assert Equals(int expected, int actual): This assertion passes if expected andactual are equal according to the  = operator; otherwise, the assertion fails. For each primitive type int, float, double, char, byte, long, short, andBoolean, the assertion has an overloaded version.assert Equ\u00aeals(double expected, d\u00aeouble actual, double toler\u00aeance): This assertion passes if the absolute value of\u00ae the difference between expected andactual is less than or equal to the tolerance value; otherwise, the assertionfails. The assertion has an overloaded version for float inputs.assert Same(Object expe\u00aected, Object actual): This assertion passes if theexpected an\u00aed actual values refer to the same object in memory; otherwise,the assertion fails asse\u00aert Null(Object testobject): This assertion passes if testobject is null; otherwise the assertion fails.assert False(Boolean condition): This is the logical opposite of assert True()",
    "page599": "The reader may\u00ae note that the above list of assertions is not exhaustive. Infact, one can build other assertions while extending the TestCase class. When anassertion fails, a programmer may want to know immediately the nature of thefailure. This can be done by displaying a message when the assertion fails. Eachassertion method listed above accepts an optional first parameter of type String ifthe assertion fails, then the String value is displayed. This facilitates \u00aethe programmerto display a desired message when the assertion fails. As an aside, upon failure,the assert Equals() method displays a customized message showing the expectedvalue and the actual value At this point it is interesting to note that only failed tests are reported. Failedtests can be reported by various means, such as displaying a message, displaying anidentifier for the test case, and counting the total number of failed test cases. Essentially, an assertion me\u00aethod throws an exception, called AssertionFailedError, whenthe assertion fails, and JUnit catches the exception. The code shown in Figure 3.5illustrates how the assert True() assertion works: When the JUnit framework catchesan exception, it records the fact that the ass\u00aeertion failed and proceeds to the nexttest case. Having executed all the test cases, JUnit produces a list of all those teststhat have failed. MyTestSuite and invoke the two methods MyTest1() and MyTest2(). Whether ornot the two methods, namely Method1() and Method()2, are to be invoked on twodifferent instances of the class TestMe depends on the i\u00aendividual objectives ofthose two test cases. In other words, it is the programmer who decides whether ornot two instances of the class TestMe are to be created",
    "page600": "Programmers can b\u00aeenefit from using tools in unit testing by reducing testing timewithout \u00aesacrificing thoroughness. The well-known tools in everyday life are aneditor, a compiler, an operating system, and a debugger. \u00aeHowever, in some cases, the real execution environment of a unit may not be available to a programmerwhile the code is being developed. In such cases, an emulator of the environmentis useful in testing and debugging the code. Other kinds of tools that facilitateeffective unit testing are as follows:1. Code Auditor: This tool is used to check the quality of software to ensuretha\u00aet it meets some minimum coding standards. It detects violations of programming, naming, and style guidelines. It can identify portions of code that cannotbe ported between different operating systems and processors. Moreover, it cansuggest improvements to the structure and style of the source code. In addition, itcounts the number of LOC which can be used to measure productivity, that is, LOCproduced per unit time, and calculate defect density, that is, number of defects perKLOC.2. Bound Checker: This tool can check for accidental writes into the instruction areas of me\u00aemory or to any other memory location outside the data storage areaof the application. This fills unused memory space with a signature pattern (distinct binary pattern) as a way of determining at a later time whether any of thismemory space has been overwritten. The tool can issue diagnostic messages whenboundary violations on data items occur. It can detect violation of the boundaries of array, for example, when the array index or pointer is outside its allowedrange. ",
    "page601": "Documenters: These tools read source code and automatically generatedescriptions and caller/called tree diagram or data model from the source code. Interactive Debuggers: These tools assist software developers in implementing different debugging approaches discussed in this chapter. These\u00ae toolsshould have the trace-back and breakpoint capabilities to enable the programmers tounderstand the dynamics of program execution and to identify problem areas in thecode. Breakpoint debuggers are based on deductive logic. Breakpoints are placedaccording to a heuristic analysis of code [32]. Another popular kind of debuggeris known as omn\u00aeiscient debugger (ODB), in which there is no deduction. It simplyfollows the trail of \u201cbad\u201d values back to their source no \u201cguessing\u201d where to putthe breakpoints. An ODB is like \u201cthe snake in the grass,\u201d that is, if you see a snakein the grass and you pull its tail, sooner or later you get to its head. In contrast,breakpoint debuggers suffer from the \u201clizard in the grass\u201d problem, that is, whenyou see the lizard and grab its tail, the lizard breaks off its tail and gets away [33].n-Circuit Emulators: An in-circuit emulator, commonly known as ICE,is an invaluable software development tool in embedded system design. It providesa high-speed Ethernet connection between a host debugger and a target microprocessor, enabling developers to perform common source-level debugging activities,such as watching memory \u00aeand controlling large numbers of registers, in a matterof seconds. It is vital for board bring-up, solving complex problems, and manufacturing or testing of products. Many emulators have advanced features, such as ",
    "page602": "performance analysis, coverage analysis, buffering of traces, and advance triggerand breakpoint possibilities.6. Memory Leak Detectors: These tools test the allocation of memory to anapplication which requests for memory, but fails to deallocate. These detect thefollowing overflow problems in application programs:Illegal read, that is, accesses to memory which is not allocated to theapplication or which the application is not authorized to access. Reads memory which has not been initialized. Dynamic memory overwrites to a memory location that has not been allocated to the application. Reading from a memory location not allocated, or not initialized, prior tothe read operation.The tools watch the heap, keep track of heap allocations to applications, anddetect memory leaks. The tools also build profiles of memory use, for example,which line-of-code source instruction accesses a particular memory address.7. Static Code (Path) Anal\u00aeyzer: These tools identify paths to test, basedon the \u00aestructure of the code such as McCabe\u2019s cyclometric complexity measure(Table 3.3). Such tools are dependent on source language and require the sourcecode to be recompiled with the tool. These tools can be used to improve producti\u00aevity, resource manage\u00aement, quality, and predictability by providing complexitymeasurement metrics.8. Software Inspection Support: Tools can help schedule group inspections.These can also provide status of i\u00aetems reviewed and follow-up actions and distributethe reports of problem resolution. They can be integrated with other tools, such asstatic code analyzers Test Coverage Analyzer: These tools measure internal test coverage, oftenexpressed in terms of the control structure of the test object, and report the coverage metric. Coverage analyzers track and report what paths were exercised duringdynamic unit testing. ",
    "page603": "Test coverage analyzers are powerful tools that increase confidence in product quality by assuring that tests cover all of the structural parts ofa unit or a program. An important aspect in test coverage analysis is to identifyparts of source code that were never touched by any dynamic unit test. Feedbackfrom the coverage reports to the source c\u00aeode makes it easier to desi\u00aegn new unittest\u00ae cases to cover the specific untested paths. Test Data Generator: These tools assist programmers in selecting test datathat cause a program to behave in a desired manner. Test data generators can offerseveral capabilities beyond the basics of data ge\u00aeneration:They have generate a large number of variations of a desired data set basedon a description of the characteristics which has been fed into the tool. They can generate test input data from sour\u00aece code. They can generate equivalence classes and values close to the boundaries. They can calculate the desired extent of boundary value testing. They can estimate the likelihood of the test data being able to reveal faults. They can generate data to assist in mutation analysis.Automatic generation of test inputs is an active area of research. Several tools,such as CUTE [34], DART [35], and EGT system [36], have been developed byresearchers to improve test coverage. Test Harness: This class of tools supports the execution of dynamic unittests by making it almost painless to (i) install the unit under test in a test environment, (ii) drive the unit under test with input data in the expected input format, (iii)generate stubs to emulate the behavior of subordinate modules, and (iv) capturethe actual outcome as generated\u00ae by the unit under test and log or display it in ausable form. ",
    "page604": "Advanced tools may compare the expected outcome with the actualoutcome and log a test verdict for each input test data.Performance Monitors: The timing characteristics of software componentscan be monitored and evaluated by these tools. These tools are essential for anyreal-time system in order to evaluate the performance characteristics of the system,such as delay and throughput. Fo\u00aer example, in telecommunication systems, thesetools can be used to calculate the end-to-end delay of a telephone call. Network Analyzers: Network operating systems such as software that runon routers, switches, and client/server systems ar\u00aee tested by network analyzers.These tools have \u00aethe ability to analyze the traffic and identify problem areas.Many of these networking tools allow test engineers to monitor performance metrics and diagnose performance problems across the networks. These tools areenhanced to improve the network security monitoring (NSM) capabilities to detectintrusion Simulators a\u00aend Emulators: These tools are used to replace the real software and hardware that are currently not ava\u00aeilable. Both kinds of tools are usedfor training, safety, and economy reasons. Some examples are flight simulators,terminal emulators, and emulators for base transceiver stations in cellular mobilenetworks. These tools are bundled with traffic generators and performance analyzersin order to generate\u00ae a large volume of input data Traffic Generators: Large volu\u00aemes of data needed to stress the interfacesand the integrated system are gener\u00aeated by traffic generators. These produce stream\u00aesof transactions or data packets. For example, in testing routers, one needs a trafficthat simulates streams of varying \u00aesize Internet Protocol (IP) packets arriving fromdifferent sources. These tools can set parameters for mean packet arrival rate,duration, and packet size. Operational profiles can be used to generate traffic forload and stability testing.",
    "page605": "Version Control: A version control system provides functionalities to storea sequence of revisions of the software and associated information files underdevelopment. A system release is a collection of the associated files from a version control tool perspective. These files may contain source code, compiled code,documentation, and environment information, such as version of the tool used towrite the software. The objective of version control is to ensure a systematic andtraceable software development process in which all changes are precisely managed, so that a software system \u00aeis always in a well-defined state. With most of theversion control tools, the repository is a central place that holds the master copyof all the files.The configuration management system (CMS) extends the version control fromsoftware and documentation to control the changes made to h\u00aeardware, firmware,software, documentation, test, test fixtures, test documentation, and execution environments throughout the development and operational life of a system. Therefore,configuration management tools are larger, better variations of version control tools.The characteristics of the version control and configuration management tools areas follows: Access Control: The \u00aetools monitor and control access to components.One can specify which users can access a component or group of components. One can also restrict access to components currently undergoingmodification or testing.Cross Referencing: The tools can maintain linkages among related components, such as problem reports, components, fixes, and documentations. One can merge files and coordinate multiple updates from different versionsto produce one consolidated file.",
    "page606": "Tracking of Modifications: The tools maintain records of all modifications to components. These also allow merging of files and coordinatemultiple updates from different versions to produce on\u00aee consolidated file.These can track similarities and differences among versions of code, documentation, and test libraries. They also provide an audit trail or history ofthe changes from version to version.Release Generation: T\u00aehe tools can automatically build new systemreleases and insulate the development, test, and shipped versions of theproduct. System Version Management: The tools allow sharing of common components across system versions and controlled use of system versions. Theysupport coordination of parallel development, maintenance, and integrationof multiple components among several programmers or project teams. Theyalso c\u00aeoordinate geographically dispersed development and test teams. Archiving: The tools support automatic archiving of retired componentsand system versions. This chapter began with a description of unit-level testing, which means identifyingfaults in a program unit analyzed and executed in isolation. Two complementarytypes of unit testing were introduced: static unit testing and dynamic unit testing. Static unit testing involves visual inspection and analysis of code, whereas aprogram unit is executed in a controlled manner in dynamic unit testing.Next, we described a code review process, which comprises six steps: readiness, preparation, examination, rework, validation, and exit. The goal of codereview is to assess the quality of the software in question, not the quality of theprocess used to develop the product. We discussed a few basic metrics that canbe collected from the code review process. Those metrics facilitate estimation ofreview time and resources required for similar projects. Also, the metrics makecode review visible to the upper management and allow upper management to besatisfied with the viability of code review as a testing tool ",
    "page607": "We explained several preventive measures that can be taken during codedevelopment to reduce the number of faults in a program. The preventive measures were presented in the form of a set of guidelines that programmers canfollow to construct code. Essentially, the guidelines focus on incorporating suitablemechanisms into the code.Next, we studied dynamic unit testing in detail. In dynamic unit testing, aprogram unit is actually executed, and the outcomes of program execution areobserved. The concepts of test driver and stubs were explained in the contextof a unit under test. A test driver is a caller of the unit under test and a\u00aell the \u201cdummy modules\u201d\u00ae called by the unit are known as stubs. We described how mutation analysis can be used to locate weaknesses in test data used for unit testing.Mutation analysis should be used in conjunction with traditional unit testing techniques such as domain a\u00aenalysis or data flow analysis. That is, mutation testing is\u00aenot an alternative to domain testing or d\u00aeata f\u00aelow analysis.With the unit test model in place to reveal defects, we examined how programmers can locate faults by debug\u00aeging a unit. Debugging occurs as a consequenceof a test rev\u00aeealing a defect. We discussed three approaches to debugging: bruteforce, cause elimination, and backtracking. The objective of debugging is to precisely identify the cause \u00aeof a failure. Given the symptom of a problem, the purposeis to isolate and determine its specific cause. We explained a heuristic to performprogram debugging. Next, we explained dynamic unit testing is an integral part of the XP softwaredevelopment process. In the XP process, unit tests are created prior to coding thisis known as test first.",
    "page608": "The test-first approach sets up checks and balances to improvethe chances of getting things right the first time. We then introduced the JUnitframework, which is used to create and execute dynamic unit tests.We concluded the chapter with a description of several tools that can be useful in improving the effectiveness of unit testing. These tools are of the followingtypes: code auditor, bound checker, documenters, interactive debuggers, in-circuitemulators, memory leak detectors, static code analyzers, tools for software inspection support, test coverage analyzers, test data generators, tools for creating testharness, performance monitors, network analyzers, simulators and emulators, trafficgenerators, and tools for version control.The Institut\u00aee of Electrical and Electronics Engineers (IEEE) standard 1028-1988(IEEE Standard for Software R\u00aeeviews and Audits: IEEE/ANSI Standard) describesthe det\u00aeailed examination process for a technical review, an inspection, a softwarewalkthrough, and an audit. For e\u00aeach of the examination processes, it includes anobjective, an abstract, special responsibilities, program input, entry criteria, procedures, exit criteria, output, and auditability.Several improvements on Fagan\u2019s inspection techniques have been proposedby researchers during the past three decades. Those proposals suggest ways toenhance the effectiveness of the review process or to fit specific applicationdomains. A number of excellent articles address various issues related to softwareinspection as follows Biffl, and M. Halling, \u201cInvestigating the Defect Effectiveness and CostBenefit of Nominal Inspection Teams,\u201d IEEE Transactions on SoftwareEngineering, Vol. 29, No. 5, May 2003, pp. 385\u2013397.A. A. Porter and P. M. Johnson, \u201cAssessing Software Review Meeting:Results of a Comparative Ana\u00aelysis of Two Experimental Studies,\u201d IEEE",
    "page609": "Transactions on Software Engineering, Vol. 23, No. 3, March 1997, pp.129\u2013145.A. A. Porter, H. P. Say, C. A. Toman, and L. G. Votta, \u201cAn Experimentto Assess the Cost-Benefits of Code Inspection in Large Scale SoftwareDevelopment,\u201d IEEE Transactions on Software Engineering, Vol. 23, No.6, June 1997, pp. 329\u2013346.A. A. Porter and L. G. Votta, \u201c\u00aeWhat Makes Inspection Work,\u201d IEEE Software,Vol. 14, No. 5, May 1997, pp. 99\u2013102.C. Sauer, D. Jeffery, L. \u00aeLand, and P. Yetton, \u201cThe Effectiveness of Software Development Technical Reviews: A Behaviorally Motivated Programof Search,\u201d IEEE Transactions on Software Engineering\u00ae, Vol. 26, No. 1,January 2000, pp. 1\u201314.An alternative non-execution-based technique is formal verification of code.Formal verification consists of mathematical proofs to show that a program iscorrect. The two most prominent methods for proving progra\u00aem properties are thoseof Dijkstra and Hoare:E. W. Dijkstra, A Discipline of Programming, Prentice-Hall, EnglewoodCliffs, NJ, 1976.C. A. R. Hoare, \u201cAn Axiomatic Basis of Computer Programming,\u201d Communications of the ACM , Vol. 12, No. 10, October 1969, pp. 576\u2013580. Hoare presented \u00aean axiomatic approach in which properties of program fragmentsare described using preconditions and postconditions. An example statement witha precondition and a postcondition is {PRE} P {POST}, where PRE is the precondition, POST is the postcondition, and P is the program fragment. Both PREand POST are expressed in first-order predicate calculus, which means that theycan include the universal quantifier \u2200 (\u201cfor all\u201d) and existential quantifier \u2203 (\u201cthereexists\u201d). The interpretation of the above statement is that if the program fragmentP starts executing in a state satisfying PRE, then if P terminates, P will do so in astate satisfying POST ",
    "page610": "Hoare\u2019s logic led to Dijkstra\u2019s closely related \u201ccalculus of programs,\u201d whichis based on the idea of weakest preconditions. Th\u00aee weakest preconditions R withrespect to a program fragment P and a postcondition POST is the set of all statesthat, when subject to P, will terminate and leave the state o\u00aef computation in POST.The weakest precondition is written as WP(P, POST).While mutation testing systematically implants faults in programs b\u00aey applyingsyntactic transformations, perturbation testing is performed to test a program\u2019srobustness by changing the values of program data during run time, so that thesubsequent execution will either fail or succeed. Program perturbation is based onthree parts of software hypothesis as explained in the following: Execution: A fault must be executed. Infection: The fault must change the data state of the computation directlyafter the fault location.  Propagation: The erroneous data state must propagate to an output variable.In the perturbation technique, the pr\u00aeogrammer injects faults in the data stateof an executing program and traces the injected faults on the program\u2019s output.A fault injection is performed by applying a perturbation function that changesthe program\u2019s data state. A perturbation function is a mathematical function thattakes a data state as its input, changes the data state according to some specifiedcriteria, and produces a modified data state as output. For the interested readers,two excellent references on perturbation testing are as follow",
    "page611": "M. A. Friedman and J. M. Voas, Software Assessment Reliability, Safety,Testability, Wiley, New York, 1995.J. M. Voas and G. McGraw, Software Fault Injection Inoculating ProgramsAgainst Errors,\u00ae Wiley, New York, 1998.The paper by Steven J. Zeil (\u201cTesting for Perturbation of Program Statement,\u201d IEEE Transactions on Software Eng\u00aeineering, Vol. 9, No. 3, May 1983,pp. 335\u2013346) describes a method for deducing sufficient path coverage to ensurethe absence of prescribed errors in a program. It models the program computationand potential errors as a vector space. This enables the conditions for nondetectionof an error to be calculated. The above article is an advanced reading \u00aefor studentswho are interested in perturbation \u00aeanalysis.Those readers actively involved in so\u00aeftware configuration management(SCM) systems or interested in a more sophisticated treatment of the topic mustread the article by Jacky Estublier, David Leblang, Andre V. Hoek, Reidar \u00b4Conradi, Geoffrey Clemm, Walter Tichy, and Darcy Wib\u00aeorg-Weber (\u201cImpactof Software Engineering Research on the Practice of Software ConfigurationManagement,\u201d ACM Transactions on Software Engineering and Methodology,Vol. 14, No. 4, October 2005, pp. 383\u2013430). The authors discussed the evolutionof software configuration management technology, with a particular emphasis\u00ae onthe impact that univers\u00aeity and industrial resea\u00aerch has had along the way. Thisarticle creates a detailed record of the critical value of software configurationmanagement research and illustrates the research results that have shaped thefunctionality of SCM systems ",
    "page612": "Two kinds of basic statements in a program unit are assignment statements andconditional statements. An assignment statement is explicitly represented by usingan assignment symbol, such as x = 2*y;, where x and y are variables.Program conditions are at the core of conditional statements, such as if(), for()loop, while() loop, and goto. As an example, in if(x! = y), we are testing for theinequality of x and y. In the absence of conditional statements, program instructionsare executed in the sequence they appear. The idea of successive execution ofinstructions gives rise to the concept of control flow in a program unit. Conditionalstatements alter the default, sequential control flow in a program unit. In fact,even a small number of conditional statements can lead to a\u00ae complex control flowstructure in a program.Function calls are\u00ae a mechanism to provide abstraction in program design.A call to a program function leads to control entering the called function. Similarly,when the called function executes its return statement, we say that control exitsfrom the function. Though a function can have many return statements, for simplicity, one can restructure the function to have exactly one return. A program unit canbe viewed as having a well-defined entry point and a well-defined exit point. Theexecution of a sequence of instructions from the entry point to the exit point of aprogram unit is called a program path. There can be a large, even infinite, numberof paths in a program unit. Each program path can be characterized by an inputand an expected output. A specific input value causes a specific program path to beexecuted; it\u00ae is expected that the program path performs the desired computation,thereby producing the expected output value. ",
    "page613": "The overall idea of generating test input data for performing control f\u00aelow testinghas been depicted in Figure 4.1. The activities performed, \u00aethe intermediate resultsproduced by those activities, and programmer preferences in the test generationprocess are explained below.Inputs: The source code of a program unit and a set of path selection criteriaare the inputs to a process for generating test data. In the following, twoexamples of path selection criteria are given.Example. Select paths such that every statement is executed at least once.Example. Select paths such that every conditional statement, forexample, an if() statement, evaluates to true and false at least once ondifferent occasions. A conditional statement m\u00aeay evaluate to true in onepath and false in a second path.Generation of a Control Flow Graph: A control flow graph (CFG) is adetailed graphical representation of a program unit. The idea behind drawing a CFG is to be able to visualize all the paths in a program unit. Theprocess of drawing a CFG from a program unit will be explained in thefollow\u00aeing section. If the process of test\u00ae generation is automated, a compilercan be modified to produce a CFG. Selection of Paths: Paths are selected from the CFG to satisfy the path selection criteria, and it is done by considering the structure of the CFG.Generation of Test Input Data: A path can be executed if and only if acertain instance of the inputs to the program unit causes all the conditionalstatements along the path to evaluate to true or false as dictated by thecontrol flow. Such a path is\u00ae called a feasible path. Otherwise, the path issaid to be infeasible. ",
    "page614": "It is essential to identify certain values of the inputsfrom a given path for the path to execute.Feasibility Test of a Path: The idea behind checking the feasibility of aselected path is to meet the path selection criteria. If some chosen pathsare found t\u00aeo be infeasible, then new paths\u00ae are selected to meet the criteria A CFG is a graphical representation of a program unit. Three symbols are usedto construct a CFG, as shown in Figure 4.2. A rectangle represents a sequential computation. A maximal sequential computation can be represented either by asingle rectangle or by many rectangles, each corresponding to one statement in thesource code.We label each computation and decision box with a unique integer. The twobranches of a decision box are \u00aelabeled with T and F to represent the true and falseevaluations, respectively, of the \u00aecondition within the box. We will not label a mergenode, because one can easily identify the paths in a CFG even without explicitlyconsidering the merge nodes. Moreover, not mentioning the merge nodes in a pathwill make a path description shorter.We consider the open files() function shown in Figure 4.3 to illustrate theprocess of drawing a CFG. The function has three statements: an assignment statement int i 0;, a conditional statement if(), and a return(i) statement. The readermay note that irrespective of the evaluation of the if(), the function performs thesame action, namely, null. In Figure 4.4, we show a high-level representa\u00aetion of the control flow in openfiles() with three nodes numbered 1, 2, and 3. The flowgraph \u00aeshows just two paths in open files().",
    "page615": "A closer examination of the condition part of\u00ae the if() statement reveals thatthere are not only\u00ae Boolean and relational operators in the condition part, but alsoassignment statements. Some of \u00aetheir examples are given below:Assignment statements: fptr1  fopen(\u201cfile1\u201d, \u201cr\u201d) and i   Relational operator: fptr1! = NULLBoolean operators: Execution of the assignment statements in the condition part of the if statementdepends upon the component conditions. For example, consider the following component condition in the if part:((( fptr1  fopen(\"file1\", \"r\")) != NULL) nThe above condition is executed as follows: Execute the assignment statement fptr1 fopen(\u201cfile1\u201d, \u201cr\u201d). Execute the relational operation fptr1! = NULL. If the above relational operator evaluates to false, skip the evaluation ofthe subsequent condition components (i  ) ",
    "page616": "A CFG, such as the one shown in Figure 4.7, can have a \u00aelarge number of differentpaths. One may be tempted to test the execution of each and every path in a programunit. For a program unit with a small number of paths, executing all the paths may be desirable and achievable as well. On the other hand, for a program unit with alarge number of paths, executing every distinct path may not be practical. Thus,it is more produ\u00aective for programmers to select a small number of program pathsin an effort to reveal defects in the code. Given the set of all paths, one is facedwith a question \u201cWhat paths do I select for testing?\u201d The concept of path selectioncriteria is useful is answering the above question. In the following, we state theadvantages of selecting paths based on defined criteria All program constructs are exercised at least once. The programmer needsto observe the outcome of executing each program construct, for example,statements, Boolean conditions, and returns. We do not generate test inputs which execute the same path repeat\u00aeedly.Executing the same path several times is a waste of resources. However,if each e\u00aexecution of a program path potentially updates the state of thesystem, for example, the database state, then multiple executions of thesame path may not be identical.We know the program features that have been tested and those not tested.For example, we may execute an if statement only once so that it evaluatesto true. If we do not execute it once again for its false evaluation, we are,at least, aware that we have not observed the outcome of the program wi\u00aetha false evaluation of the \u00aeif statement.",
    "page617": "Now we explain the following well-known path sele\u00aection criteria: Select all paths. Select paths \u00aeto achieve complete statement coverage. Select paths to achieve complete branch coverage.Select paths to achieve predicate coverage If all the paths in a CFG are selected, then one can detect all faults, except thosedue to missing path errors. However, a program may contain a large number ofpaths, or even an infinite number of paths. The small, loop-free openfiles() functionshown in Figure 4.3 contains more than 25 paths. One does not know whether ornot a path \u00aeis feasible at the time of selecting paths, though only eight of all thosepaths are feasible. If one selects all possible paths in a program, then we say thatthe \u00aeall-path selection criterion has been satisfied.Le\u00aet us consider the example of the openfiles() function. This function tries toopen the three files file1, file2, and file3. The function returns an integer representingthe number of\u00ae files it has successfully opened. A file is said to be successfullyopened with \u201cread\u201d access if the \u00aef\u00aeile exists. The existence of a file is either \u201cyes\u201dor \u201cno.\u201d Thus, the input domain of the function consists of eight combinations ofthe existence of the three files, as shown in Table 4.2.We can trace a path in the CFG of Figure 4.5 for each input, that is, eachrow of Table 4.2. Ideally, we identify test inputs to execute a certain path in a program; this will be explained later in this chapter. We give three examples of thepaths executed by the test inputs (Table 4.3). In this manner, we can identify eightpossible paths .The all-paths selection criterion is desirable since itcan detect faults; however, it is difficult to achieve in practice.",
    "page618": "Statement coverage \u00aerefers to executing individual program statements and observing t\u00aehe outcome. We say that 100% statement coverage has been\u00ae achieved if allthe statements have been executed at least once. Complete statement coverage isthe weakest coverage criterion in program testing. Any test suite that achieves lessthan statement coverage for new software is considered to be unacceptable.All program statements are represented in some form in a CFG. Referringto the ReturnAverage() method in Figure 4.6 and its CFG in Figure 4.7, the fourassignment statementsi 0;ti = 0;tv = 0;sum = 0;have been represented by node 2. The while statement has been represented as aloop, where the loop control condition(ti < AS value[i] ! -999) as been represented by nodes 3 and 4. Thus, covering a statement in a programmeans visiting one or more nodes represen\u00aeting the statement, more precisely, selecting a feasible entry\u2013exit path that includes the corresponding nodes. Since a singleentry\u2013exit path includes many nodes, we need to select just a few paths to coverall the nodes of a CFG. Therefore, the basic problem is to select a few feasiblepaths to cover all the nodes of a CFG in order to achieve the complete statementcoverage criterion. We follow \u00aethese rules while selecting paths: Select short paths. Select paths of increasingly longer length. Unfold a loop several times ifthere is a need.Select arbitrarily long, \u201ccomplex\u201d paths.One can select the two paths shown in Figure 4.4 to achieve complete statementcoverage.",
    "page619": "Syntactically, a branch is an \u00aeoutgoing edge from a node. All the rectangle nodeshave at most one outgoing branch (edge). The exit node of a CFG does not have anoutgoing branch. All the diamond nodes have two outgoing branches. Covering abranch means selecting a path that includes the branch. Complete branch coveragemeans selecting a number of paths such that every branch is included in at leastone path.In a preceding discussion, we showed that one can select two paths, SCPath 1and SCPath 2 in Table 4.4, to achieve complete statement coverage. These twopaths cover all the nodes (statements) and most of the branches of the CFG shownin Figure 4.7. The branches which are not covered by these two paths have beenhighlighted by bold dashed lines in Figure 4.8. These uncovered branches correspond to the three independent conditions  evaluating to false. This means that as a programmer we have not observed theoutcome of the program execution as a result of the conditions evaluating to false.Thus, complete branch coverage means selecting enough number of paths such thatevery condition evaluates to true at least once and to false at least once.We need to selec\u00aet more paths to cover the branches highlighted by the bolddashed lines  We refer to the partial CFG of Figure 4.9a to\u00ae explain the concept of predicatecoverage. OB1, OB2, OB3, and OB are four Boolean variables. The programcomputes the values of the individual variables OB1, OB2, and OB3  details oftheir computation are irrelevant to our discussion and have been omitted. Next, OBis computed as shown in the CFG. The CFG checks the value of OB and executeseit\u00aeher OBlock1 or OBlock2 depending on whether OB evaluates to true or false,respectively.",
    "page620": "We need to design j\u00aeust two test cases to achi\u00aeeve both statement coverageand branch coverage. We select inputs such that the four Boolean conditions inFigure 4.9a evaluate to the values shown in Table 4.6. The reader may note thatwe have shown just one way of forcing OB to true.\u00ae If we select inputs so tha\u00aet thesetwo cases hold, then we do not observe the effect of the computations taking placein nodes 2 and 3. There may be faults in the  computation parts of nodes 2 a\u00aend 3such that OB2 and OB3 always evaluate to false .  Therefore, there is a need to design test cases such that a path is executedunder all possible conditi\u00aeons. The False branch of node 5 (Figure 4.9a) is executedunder exactly one condition, namely, when OB1 False, OB2 = False, and OB3 =False, whereas the true branch executes under seven conditions. If all possiblecombinations of truth values of the conditions affecting a selected path have beenexplored under some tests, then we say that predicate coverage has been achieved.Therefore, the path taking the true branch of node 5 in Figure 4.9a must be executedfor all seven possible combinations of truth values of OB1, OB2, and OB3 whi\u00aechresult in OB = True.A similar situation holds for\u00ae the partial CFG shown in Figure 4.9b, whereAB1, AB2, AB3, and AB are Boolean variables. ",
    "page621": "In Section 4.5 we explained the concept of path selection criteria to cover certainaspects of a program with a set of paths. The program aspects we consideredwere all statements, true and false evaluations of each condition, and combinationsof conditions affecting execution of a path. Now, having identified a path, thequestion is how to select input values such that when the program is executedwith the selected inputs, the chosen paths get executed. In other words, we needto identify inputs \u00aeto fo\u00aerce the executions of the paths. In the following, we definea few terms an\u00aed give an example of generating test inputs for a selected path.1. Input Vector: An input vector is a collection of all data entities read bythe routine whose values must be fixed prior to entering the routine. Members ofan input vector of a routine can take different forms as listed below:Input arguments to a routine Global variables and constants Files Contents of registers in assembly language programming\u00ae Network connections TimersA file is a complex input element. In one case, mere existence of a file can beconsidered as an input, whereas in another case, contents of the file are considered to be inputs. Thus, the idea of an input vector is more general than the concept ofinput arguments of a function.Example. An input vector for openfiles() (Figure 4.3) consists of individual presence or absence of the files file1, file2, and file3.Example. The input vector of the ReturnAverage() method shown in Figure 4.6is < value [], AS, MIN, MAX > .2. Pre\u00aedicate: A predicate is a logical function evaluated at a decision point.Example. The construct ti < AS is the predicate in decision node 3 of Figure 4.7.",
    "page622": "he construct O\u00aeB is the predicate in decision node 5 of Figure 4.9.3. Path Predicate: A path predicate is the set of predicates associated witha path.The path in Fi\u00aegure 4.10 indicates that nodes 3, 4, 6, 7, and 10 are decision nodes. The predicate associated with node 3 appears twice in the path; inthe first instance it evaluates to true and in the second instance it evaluates tofalse. The path predicate associated with th\u00aee path under consideration is shown inFigure 4.11.We also specify the intended evaluation o\u00aef the component predicates as foundin the path specification. For instance, we specify that value[i] ! 999 mustevaluate to true in the path predicate show\u00aen in Figure 4.11. We keep this additionalinf\u00aeormation for the following two reasons:In the absence of this additional information denoting the intended evaluation of a predicate, we will have no way t\u00aeo distinguish between the twoinstances of the predicate ti < AS, namely 3(T) and 3(F), associated withnode 3.We must know whether the individual component predicates of a pathpredicate evalua\u00aete to true or false in order to generate path forcing inputs.4. Predicate Interpretation: The path predicate shown in Figu\u00aere 4.11 is composed of elements of the input vector < value[], AS, MIN, MAX >, a vector oflocal variables < i, ti, tv >, and the constant \u2212999. The local variables are notvisible outside a function but are used to hold intermediate results, point to array elements, and control loop iterations. ",
    "page623": "In other words, they play no roles in selecting inputs that force the paths to execute.Therefore, we can easily substitute all the local variables in a predicate with theelements of the input vector \u00aeby using the idea of symbolic substitution. Let usconsider the method shown in Figure 4.12. The input vector for the method inFigure 4.12 is given by < x1, x2 > . The method defines a local variable y and alsouses the constants 7 and 0.The predicatex1   y > 0can be rewritten asx1   x2   7 >= 0by symbolically substituting y with x 2   7. The rewritten predicatex1   x2   7 >= 0has been expressed solely in terms of the input vector < x1,x2 > and the constantvector < 0,7 > . Thus, predicate interpretation is defined as the process of symbolically substituting operations along a path in order to express the predicates solelyin terms of the input vector and a constant vector.In a CFG, there m\u00aeay be several different paths leading up to a decision pointfrom the initial node, with each path doing different computations. Therefore, apredicate may have d\u00aeifferent interpret\u00aeations depending on how control reaches thepredicate under consideration. Path Predicate Expression: An interpreted path predicate is called a pathpredicate expression. A path predicate expression has the following properties:It is void of local variables and is solely composed of elements of the inputvector and possibl\u00aey a vector of constants.It is a set of constraints constructed from the elements of the input vectorand possibly a vector of constants.Path forcing input values can be generated by solving the set of constraintsin a path predicate expression.",
    "page624": "If the s\u00aeet of constraints cannot be solved, there exist no input which cancause the selected path to execute. In other words, the selected path is saidto be infeasible. An infeasible path does not imply that one or more components of a pathpredicate expression are unsatisfiable. It simply means that the total combination of all the components in a path predicate expression is unsatisfiable.Infeasibility of a path predicate expression suggests that one considers otherpaths i\u00aen an effort to meet a chosen path selection criterion.Example. Consider the path shown in Figure \u00ae4.10 from the CFG of Figure 4.7.Table 4.7 shows the nodes of the path in column 1, the corresponding descriptionof each node in column 2, and the interpretation of each node in column 3. The intended evaluation of each interpreted predicate can be found in column 1 of thesame row.We show the path predicate expression of the path under consideration in Figure 4.13 for the sake of clarity. The rows of Figure 4.13 have beenobtained from Table 4.11 by combining each interpreted predicate in column 3 withits intended evaluation in column 1. Now the reader may comp\u00aeare Figures 4.11and 4.13 to note that the predicates in Figure 4.13 are interpretations of the corresponding predicates in Figure 4.11 We show in Figure 4.14 \u00aean infeasible path appearing in the CFG ofFigure 4.7. The path predicate and its interpretatio\u00aen are shown in Table 4.8, and thepath predicate expression is shown in Figure 4.15. The path predicate expression isunsolvable because the constraint 0 > 0 \u2261 True is unsatisfiable. Therefore, the pathshown in Figure 4.14 is an infeasible path.",
    "page625": "Generating Input Data from Path Predicate Expression: We must solvethe corresponding path predicate expression in order to generate input data whichcan force a program to execute a selected path. Let us consider the path predicateexpression shown in Figure 4.13. We observe that constraint 1 is always satisfied.Constraints 1 and 5 must be solved together to obtain AS 1. Similarly, constraints2, 3, and 4 must be solved together. We note that MIN < = value[0] < = MAXand value[0]! = \u2212999. Therefore, we have many choices to select values of MIN,MAX, and value[0]. An instance of the solutions of the constraints of Figure 4.13is shown in Figure 4.16  We give examples of selected test data to achieve complete statement and branchcoverage. We show four sets of test data in Table 4.9. The first two data sets coverall statements of the CFG in \u00aeFigure 4.7. However, we need all four sets of testdata for complete branch coverage.If we execute the method ReturnAverage shown in Figure 4.6 with the foursets of test input data shown in Figure 4.9, then each statement of the method isexecuted at least once, and every Boolean condition evaluates once to true andonce to false. We have thoroughly tested the method in the sense of completebranch coverage. However, it is possible to introduce simple faults in the methodwhich can go undetected when the method with the above four sets of test data isexecuted. Two examples of fault insertion are given below. in the method. Here the fault is that the method computes the average of thetotal number of \u00aeinpu\u00aets, denoted by ti, rather than the total number of valid i\u00aenputs,denoted by tv.",
    "page626": "he JPA specification only allows properties/paths in the ORDER BY clause if the SELECT clause projects the same properties/paths. The following queries may be nonportable but work in Hibernate: select i.name from Item i order by i.buyNowPrice asc select i from Item i order by i.seller.username des Be careful with implicit inner joins in path expressions and ORDER BY: The last query returns \u00aeonly Item instances that have a seller. This may be unexpected, as the same query without the ORDER BY clause would retrieve all Item instances. (Ignoring for a moment that in our model the Item always has a seller, this issue is visible with \u00aeoptional references.) You\u2019ll find a more detailed discussion of inner joins and path. expressions later in this chapter.  You now know how to write the FROM, WHERE, and ORDER BY clauses. You know how. to select the entities, you want to retrieve instances of and the necessary expressions. and operations to restrict and order the result. All you need now is the ability to project the data of this result to what you need in your application. In simple terms, selection and restriction in a query is the process of declaring which. tables and rows you want to query. Projection is defining the \u201ccolumns\u201d you want returned to the application: the data you need\u00ae. The SELECT clause in JPQL performs projections. As promised earlier,\u00ae this criteria query shows how you can add \u00aeseveral Roots by calling the from() method several times. To add several elements to your projection, either call the tuple() method\u00ae of CriteriaBuilder, or the shortcut multiselect().",
    "page627": " Because this is a product, the result contains every possible combination of Item. and Bid rows found in the two underlying tables. Obviously, this query isn\u2019t usefu\u00ael, but you shouldn\u2019t be surprised to receive a \u00aecollection of Object   as a query result. Hibernate manages all Item and Bid entity instances in persistent state, in the persistence. context. Note how the HashSets filter out duplicate Item and Bid instances.  Alternatively, with the Tuple API, in criteria queries you get typed access to the result list. Start by calling createTupleQuery() to create a CriteriaQuery<Tuple>. The\u00aen, refine the query definition by adding aliases for the entity classes The Object   returned by this query contain a Long at index 0, a String at index 1, and an Address at index 2. The first two are scalar values; the third is an embedded class instance. None are managed entity instances! Therefore, these values aren\u2019t in any persistent state, like an entity instance would be. They aren\u2019t transactional and obviously aren\u2019t checked automatically for dirty state. We say that all of these values are transient. This is the kind of query you need to write for a simple reporti\u00aeng screen, showing all user names and their home addresses.  You have now seen path expressions several times: using dot-notation, you can reference properties of an entity, such as User#username with u.username. For a nes\u00aeted embedded property, for example, you can write the path u.homeAddress.city.zip\u0002code.\u00ae These are single-valued path expressions, because they don\u2019t terminate in a mapped collection property  A more convenient alternative than Object[] or Tuple, especially for report que\u0002ries, is dynamic instantiation in projections, which is next",
    "page628": "Let\u2019s say you have a reporting scree\u00aen in your application where you need to show. some data in a list. You want to show all auction items and when each auction ends. You don\u2019t want to load managed Item entity instances, because no data will be modified: you only read\u00ae data.  First, write a class called ItemSummary with a constructor that takes a Long for the item\u2019s identifier, a String for the item\u2019s name, and a Date for the item\u2019s auction end timestamp: We sometimes call these kinds of classes data transfer objects (DTOs), because their main purpo\u00aese is to shuttle data around in the application. The ItemSummary class isn\u2019t mapped to the database, and you can add arbitrary methods (getter, setter, printing of values) as needed by your reporting user interface.  Hibernate can directly return instances of Item Summary from a query with the new keyword in JPQL and the construct() met\u00aehod in criteria We sometimes call these kinds of classes data transfer objects (DTOs), becaus\u00aee their main purpose is to shuttle data around in the application. The ItemSummary class isn\u2019t mapped to the database, and you can add arbitrary methods (getter, setter, printing of values) as needed by your reporting user interface.  Hibernate can directly return instances of ItemSummary from a query with the\u00ae new keyword in JPQL and the construct() method in criteria ",
    "page629": "your DTO class doesn\u2019t have the right constructor, and you want to populate it from a query result through setter methods or fields, apply a ResultTransformer, as shown in in section 16.1.3. Later, we have more examples of aggregation and grouping.  Next, we\u2019re going to look at an issue with projection that is frequently confusing for many engineers: handling duplicates When you create a projection in a query, the elements of the result aren\u2019t guaranteed. to be unique. For example\u00ae, item names aren\u2019t unique, so the following query may return the same name more than once: It\u2019s difficult to see how it could be meaningful to have two identical rows in a que\u00aery result, so if you think duplicates are likely, you normally apply the DISTINCT keyword or distinct() method This eliminates duplicates from the returned list of Item descriptions and translates. directly into the SQL DISTINCT operator. The filtering occurs at the database level. Later in this chapter, we show you that this isn\u2019t always the case.  Earlier, you saw function calls in restrictions, in the WHERE clause. You can also call functions in projections, to modify the returned data within the query  If an Item doesn\u2019t have a buyNowPrice, a BigDecimal for the value zero is returned instead of nul\u00ael.  Similar to coalesce() but more powerful are case/when expressions. The following query returns the username of each User and an additional String with either \u201cGermany\u201d, \u201cSwitzerland For the built-in standard functions, refer to the tables in the previous sect\u00aeion. Unlike function calls in restrictions, Hibernate won\u2019t pass on an unknown function call in a projection to the database as a pla\u00aein direct SQL function call. Any function you\u2019d like to call in a projection must be known to Hibernate and/or invoked with the special function() operation of JPQL.",
    "page630": " This projection returns the name of each auction Item and the number of days between item creation and auction end, calling the SQL datediff() function of the H2 database If instead you want to call a function directly, you give Hibernate the function\u2019s return type, so it can parse the query. You add funct\u00aeions for invocation in projections by extending your configured org.hibernate.Dialect. The datediff() function is already registered for you in the H2 dialect. Then, you can either call it as shown with function(), which works in other JPA providers when accessing H2, or directly as datediff(), which most likely only works in Hibernate. Check the source code of the dialect for you\u00aer database; you\u2019ll probably find many other proprietary SQL functions. already registered there.  Furthermore, you can add SQL functions programmatically on boot to Hibernate by calling the method applySqlFunction() on a H\u00aeibernate MetadataBuilder See the Javadoc of SQLFunction and its subclasses for more information.   Next, we look at aggregation functions, which are the most useful functions in reporting queries. Reporting queries take advantage of the da\u00aetabase\u2019s ability to perform efficient group\u0002ing and aggregation of data. For example, a typical report query would retrieve the highest initial item price in a given category. This calculation can occur in the data\u0002base, and you don\u2019t have to load many Item entity instances into memory.  The aggregation functions standardized in JPA are count(), min(), max(), sum(), and avg(). This query returns a BigDecimal, because the amount property is of type BigDecimal. The sum() function also recognizes the BigInteger property type and retu\u00aerns Long for all other numeric property types",
    "page631": "When you call an aggregation function in the SELECT c\u00aelause, without specifying any grouping in a GROUP BY clause, you collapse the results down to a single row, containing. the aggregated value(s). This means (in the\u00ae absence of a GROUP BY clause) any SELECT. clause that contains an aggregation function must contain only aggregation function for more advanced statistics and for reporting, you need to be able to perform. grouping, which is up next JPA standardizes several features of SQL that are most commonly used for reporting  although they\u2019re also used for other things. In reporting queries, you write the SELECT. clause for projection and the GROUP BY and HAVING clauses for aggregation.  Just like in SQL, any property or alias that appears outside of an aggregate function. in the SELECT clause must also appear in the GROUP BY clause in this example, the u.lastname property isn\u2019t inside an aggregation function, so projected data has to be \u201cgrouped by\u201d u.lastname. You also don\u2019t need to specify the property you wan\u00aet to count; the count(u)expression is automatically translated into. count(u\u00ae.id) When grouping, you may run into a Hibernate limitation. The following query is specification compliant but not properly handled in Hiberna\u00aete The JPA specification allows grouping by an entity path expression, group b\u00aey i. But Hibernate doesn\u2019t auto\u00aematically expand the properties of Item in the generated SQL  GROUP BY clause, which then doesn\u2019t match the SELECT clause. You have to expand the The JPA specification allows grouping by an entity path expression, group by i. But Hibernate doesn\u2019t automatically expand the properties of Item in the generated SQL GROUP BY clause, which then doesn\u2019t match the SELECT clause. You have to expand the fix",
    "page632": "J\u00aeoin operations combine data in two (or more) rela\u00aetions. Joining data in a query also enable\u00aes you to fetch several associat\u00aeed instances and collections in a single query: for example, to load an Item and all its bids in one round trip to the database. We now show you how basic join operations work and how to use them to write such dynamic fetching strategies. Let\u2019s first look at how joins work in SQL querie\u00aes, without JPA  Let\u2019s start with the example we already mentioned: joining the data in the ITEM and BID tables, as shown in figure 15.1. The database contains three items: the first has three bids, the second has one bid, and the third has no bids. Note that we don\u2019t show. all columns; hence the dotted lines. What m\u00aeost people think of when they hear the word join in the context of SQL databases is an inner join. An inner join is the most important of several types of joins and the easiest to unde\u00aerstand. Consider the SQL statement and result in figure 15.2. This SQL statement contains an ANSI-style inner join in the FROM clause.  If you join the ITEM and BID tables with an inner join, with the condition that the ID of an ITEM row must match the ITEM_ID value of a BID row, you get items combined wi\u00aeth their bids in the result. Note that\u00ae the result of this operation contains only items that have bids",
    "page633": "You can think of a join as working as follows: first you take a \u00aeproduct of the two. tables, by taking all possible combinations of ITEM rows with BID rows. Second, you filter these combined rows with a join condition: the expression in the ON clause. (Any good database engine has much more sophisticated algorithms to eval\u00aeuate a join; it usually doesn\u2019t bui\u00aeld a memory-consuming product and then filter out rows.) The join condition is a Boolean expression that evaluates to true if the combined row is to be included in the result.  It\u2019s crucial to understand that the join conditio\u00aen can be any expression that evalu\u0002ates to true. You can join data in arbitrary ways; you \u00aearen\u2019t limited to comparisons of identifier values. For example, the join condition on i.ID  b.ITEM_ID and amount > 100 would only include rows from the BID table that also have an AMOUNT gr\u00aeeater than 100.  that a BID has a reference to an ITEM row. This doesn\u2019t mean you can only join by comparing primary and foreign key columns. Key columns are of cours\u00aee the most common operands in a join condition, because you often want to retrieve related information together.  If you want all items, not just the ones wh\u00aeich have related \u00aebids, \u00aeand NULL instead of bid data when there is no corresponding bid, then you write a (left) outer join",
    "page634": " In case of the left outer join, each row in\u00ae the (left) ITEM table that never satisfies the join condition is also included in the result, with NULL returned for all columns of B\u00aeID. Right outer joins are rarely used; d\u00aeevelopers always think from left to right and put the \u201cdriving\u201d table of a join operation first. In figure 15.4, you can see the same result with BID instead of ITEM as the driving table, and a right outer join. In SQL, you usually specify the join condition explicitly. Unfortunately, it isn\u2019t possible. to use the name of a foreign key constraint to specify how two tab\u00aeles are to be joined: select * from ITEM join BID on FK_BID_ITEM_ID doesn\u2019t work.  You specify the join condition in the ON clause for an ANSI-style join or in the WHERE. clause for a so\u00ae-called theta-style join: select * from ITEM, BID b where i.ID b.ITEM_ID. This is an inner join; here you see that a product is created first in the FROM clause.  We now discuss JPA join options. Remember that\u00ae Hibernate eventually t\u00aeranslates all queries into SQL, so even if the syntax is slightly different, you should always refer to the illustrations shown in this section and verify that you understand what the resulting SQL and result set looks like JPA provides four ways of expressing (inner and outer) joins in queries:  An implicit association join with path expressions.  An ordinary join in the FROM clause with the join operator A fetch join in the FROM clause with th\u00aee join operator and the fetch keyword for eager fetching A theta-style join in the WHERE clause. Let\u2019s start with implicit association joins. In JPA queries, you don\u2019t have to specify a join condition explicitly. Rather, you specify the name of a mapped Java class association. This is the same feature we\u2019d prefer to have in SQL: a join condition expressed with a foreign key constraint name. Because you\u2019ve mapped most, if not all, foreign key relationships of your database schema, you can use the names of these mapped associations in the query language. This is syntac\u0002tical sugar, but it\u2019s convenient",
    "page635": " For example, the Bid entity class has a mapped many-to-one association named item, with the Item entity class. If you refer to this association in a qu\u00aeery, Hibern\u00aeate has enough information to deduce the join expression with a key column compari\u0002son. This helps make queries less verbose and more readable.  Earlier in this chapter, we showed \u00aeyou property path expressions, using dot-notation: single-valued path expressions such as user.homeAddress.zipcode and collection\u0002valued path expressions such as item.bids. You can create a path expression in an implicit inner join query The path b.item.name creates an implicit join on the many-to-one associations from Bid to Item the name of this association is item. Hibernate knows that you mapped this association with the ITEM_ID foreign key in the BID table and generates the SQL join condition accordingly. Implicit joins are always directed along many-t\u00aeo-one or one-to-one associations, never through a collection-valued association (you can\u2019t write item.bids.amount). This query joins rows from the BID, the ITEM, and the USER tables.  We frown on the use of this syntactic sugar for more complex queries. SQL joins are important, and especially when optimizing queries, you need to be able to see at a glance exactly how many o\u00aef them there are How many joins are required to express such a query in SQL? Even if you get the answer right, it takes more than a few seconds to figure out. The answer is two. The generated SQL looks something like th\u00aeis: Alternatively, instead of joins wit\u00aeh such complex path expressions, you can write ordinary joins explicitly in the FROM clause",
    "page636": "JPA differentiates between purposes you may have for joining. Suppose you\u2019re querying items; there are two possible reasons you may be interested in joining them with bids.  You may want to limit the items returned by the query based on some criterion to apply to their bids. For example, you may want all items \u00aethat have a bid of more than 100, which requires an inner join. Here, you aren\u2019t interested in items that have no bids.  On the other hand, you may be primarily interested in the items but may want to execute an outer join just because you want to retrieve all bids for the queried items in a single SQL statement, something we called eager join fetching earlier. Remember that you prefer to ma\u00aep all associations lazily by default, so an eager fetch query will override the default fetching strategy at runtime for a particular use case.  Let\u2019s first write some queries that use joins for the purpose of restriction. If you want to retrieve Item instances and restrict the result to items that have bids with a certain amount, you have to assign an alias to a joined association. Then you\u00ae refer to the alias in a WHERE clause to restrict the data you want This query assigns the alias b to the collection bids and limits the returned Item instances to those with Bid#amount greater than 100.  So far, you\u2019ve only written inner joins. Outer joins are mostly used for dynamic fetching, which we discuss soon. Somet\u00aeimes, you want to write a simple query with an outer join without applying a dynamic fetching strategy. For example, the following query and retrieves items that have no bids, and items with bids of a minimum bid amount:",
    "page637": "This query returns ordered pairs of Item and Bid, in a List<Object[]>.  The first thing that is new in this query is the LEFT keyword and JoinType.LEFT in the criteria query. Optionally you can write LEFT OUTER JOIN and RIGHT OUTER JOIN in JPQL, but we usually prefer \u00aethe short form.  The second change is the additional join condition following the ON keyword. If instead you place\u00ae the amount > 100 expression into the WHERE clause, you restrict the result to Item instances that have bids. This isn\u2019t what you want here: you want to retrieve items and bids, and even items that don\u2019t have bids. If an item has bids, the bid amount must be greater than 100. By adding an additional join condition in the FROM clause, you can restrict the Bid instances and still ret\u00aerieve all Item instances, whether they have bids or not The SQL query will always contain the implied join condition of the mapped associa\u0002tion, i.ID  b.ITEM_ID. You can only append additional expressions to the join condition. JPA and Hibernate don\u2019t support arbitrary outer joins without a mapped entity association or collection.  Hibernate has a proprietary WITH keyword, it\u2019s the same as the ON keyword in JPQL. You may see it in older code examples, because JPA only recently standardized ON.  You can write a query returning the same data with a right outer join, switching the driving table This right outer join query is more important than you may think. Earlier in this book, we told you to avoid mapping a persistent collection whenever possible. I\u00aef you don\u2019t have a one-to-many Item#bids collect\u00aeion, you need a right outer join to ret\u00aerieve all Items and their Bid instances. You drive the query from the \u201cother\u201d side: the many-to\u0002one Bid#item. ",
    "page638": "All the queries you saw in the previous sections have one thing in common: the returned Item instances have a collection named bids. This @OneToMany collection, if mapped as FetchType.LAZY (the default for collections), isn\u2019t initialized, and an addi\u0002tional SQL statement is triggered as soon as you access it. The same is true for all sin\u0002gle-valued associations, like the @ManyToOne association seller of each Item. By default, Hibernate generates a proxy and loads the associ\u00aeated User instance lazily and only on demand.  What options do you have to change this behavior? First, you can change the fetch. plan in your mapping metadata and declare a collection or single-valued association. as FetchType.EAGER. Hibernate then executes the necessary SQL to guarantee that. the desired network of instances is loaded at all times. This also means a single JPA. query may result in several SQL operations! As an example, the simple query select\u00aes I from Item i may trigger additional SQL statements to load the bids of each Item, the seller of each Item, and so on.  In chapter 12, we made the case for a lazy global fetch plan in mapping metadata, where you shouldn\u2019t have FetchType.EAGER on association and collection mappings. Then, for a particular use case in your application, you dynamically override the lazy fetch plan an\u00aed write a query that fetches the data you need as efficiently as possible. For example, there is no reason you need several SQL statements to fetch all Item instances and to initialize their bids collections, or to retrieve the seller for each Item. You can do this at the same time, in a single SQL statement, with \u00aea join operation.  Eager fetching of associated data is possible with the FETCH keyword in JPQL and the fetch() method in the criteria query API",
    "page639": "You\u2019ve already see\u00aen the SQL query this produces and the result set in figure 15.3.  This query returns a List<Item>; each Item instance has its bids collection fully initialized. This is different than the \u00aeordered pairs returned by the queries in the previous section!  Be careful you\u00ae may not expect the duplicate results from the previous query: Make sure you understand why these duplicates appear in the result List. Verify the number of Item \u201crows\u201d in the result set, as shown in figure 15.3. Hibernate preserves the rows as list elements; you may need the correct row count to make rendering a report table in the user interface easier.  You can filter out duplicate Item instances \u00aeby passing the result List through a LinkedHashSet, which doesn\u2019t allow duplicate elements but preserves the order of elements. Alternatively, Hibernate can remove the duplicate elements with the DISTINCT operation and distinct() criteria method:\u00ae Understand that in this case the DISTINCT operation does not execute in the database. There will be no DISTINCT keyword in the SQL statement. Conceptually, you can\u2019t remove the duplicate rows at the SQL ResultSet level. Hibernate per\u00aeforms deduplication in memory, just as you would manually with a LinkedHashSet This query returns a List<Item>, and each Item has its bids collection initialized. The seller of each Item is loaded as well. \u00aeFinally, the bidder of each Bid instance is loaded. You can do this in one SQL query by joining rows of the ITEM, BID, and USERS tables.  If you write JOIN FETCH without LEFT, you get eager loading with an inner join (also if you use INNER JOIN FETCH) ",
    "page640": "An eager inner join fetch makes sense if there must be a fetched value: an Item must have a seller, and a Bid must have a\u00ae bidder.  There are limits to how many associations you should eagerly load in one query and how much dat\u00aea you should fetch in one round trip. Consider the following query, which initializes the Item bi\u00aeds and Item images collections: This is a bad \u00aequery, because it creates a Cartesian product of bids and images, with a potentially extremely large result set. We covered this issue in section 12.2.2.  To summarize, eager dynamic fetching in queries has the following caveats:  Never assign an alias to any fetch-joined association or c\u00aeollection for further restriction or projection. The query left join fetch i.bids b where b.amount ... is invalid. You can\u2019t say, \u201cLoad the Item instances and initialize their bids col\u0002lections, but only with Bid instances that have a certain amount.\u201d You can assign an alias to a fetch-joined association for further fetching: for example, retrieving the bidder of each Bid: left join fetch i.bids b join fetch b.bidder. You shouldn\u2019t fetch more than one collection; otherwise, you create a Cartesian product. You can fetch as many single-valued associations as you like wi\u00aethout creating a product Queries ignore any fetching strategy you\u2019ve defined in mapping metadata with @org.hibernate.annotations.Fetch. For example, mapping the bids collec\u0002tion with org.hibernate.annotations.FetchMode.JOIN has no effect on the queries you write. The dynamic fetching strategy of your query ignores the global fetching strategy. On the other hand, Hibernate doesn\u2019t ignore the mapped fetch plan: Hibernate always considers a FetchType.EAGER, and you may see several additional SQL statements when you execute your query.",
    "page641": "If you eager-fetch a collection, the List returned by Hibernate preserves the number \u00aeof rows in the SQL result as duplicate references. You can filter out the duplicates in-memory either manually with a LinkedHashSet or with the special DISTINCT operation in the query. There is one more issue to be aware of, and it deserves some special attention. You can\u2019t paginate a result set at the database level if you eagerly fetch a collection. For example, for the query select i from Item i fetch i.bids, how should Query#set\u0002FirstResult(21) and Query#se\u00aetMaxResults(10) be handled?  Clearly, you expect to get only 10 items, starting with item 21. But you also want to load all bids of each Item eagerly. Therefore, the database can\u2019t do the paging opera\u0002tion; you can\u2019t limit the SQL result to 10 arbitrary rows. Hibernate will execute paging in-memory if a collection is eagerly fetched in a query. This means all Item instances will be loaded into memory, each with the bids collection fully initialized. Hibernate then gives you the requested page of items: for example, only items 21 to 30.  Not all items might fit into memory, and you probably expected the paging to occur in the database before it transmitted the result to the application! Therefore, Hibernate will log a warning message if your query contains fetch [collectionPath] and you call setFirstResult() or setMaxResults().  We don\u2019t recommend the use of fetch [collectionPath] with setMaxResults() or setFirstResult() options. Usually there is an easier query you can write to get the data you want to render and we don\u2019t e\u00aexpect that you load data page by page to modify it. For example, if you want to show\u00ae several pages of items and for each item the number of bids, write\u00ae a report query ",
    "page642": "In traditional SQL, a theta-style join is a Cartesian product together with a join condition in the WHERE clause, which is applied\u00ae on the product to restrict the result. In JP queries, the theta-style syntax is useful when your join condition isn\u2019t a foreign key relationship mapped to a class association.  For example, suppose you store the User\u2019s name in log records instead of mapping an association from LogRecord to User.\u00ae The classes don\u2019t know anything about each other, because they aren\u2019t associated. You can then find all the Users and their Log Records with the following theta-style join The join condition here is a comparison of use\u00aername, present as an attribute in both. classes. If both rows have the same username, they\u2019re joined (with an inner join) in the result. The query result consists of ordered pairs You probably won\u2019t need to use the theta-style joins often. Note that it\u2019s currently not. possible in JPA to outer join two tables that don\u2019t have a mapped association theta\u0002style joins are inner joins.  Another more common case for theta-style joins is comparisons of primary key or foreign key values to either query parameters or other primary or foreign key values in the WHERE clause: This query returns pairs of Item \u00aeand Bid instances, where the bidder is also the seller. This is an important query in CaveatEmp\u00aetor because it lets you detect people who bid on their own items. You probably should translate this query into a database constraint and not allow such a Bid instance to be stored.   The previous query also has an interesting comparison expression: i.seller  b.bidder. This is an identifier comparison, our next topic",
    "page643": "In this query, i.seller refers to the SELLER_ID foreign key column of the ITEM table, referencing the USERS table. The alias u refers to the primary key of the USERS table. (on the ID column). Hence, this query has a theta-style join and is equivalent to the easier, readable alternative A path expression ending with id is special in Hibernate: the id name always refers to the identifier property of an entity. It doesn\u2019t matter what the actual name of the property annotated with @Id is you can always reach it with entityAlias.id. That\u2019s why we recommend you always name the identifier property of your\u00ae entity classes id, to a\u00aevoid confusion in queries. Note that this isn\u2019t a requirement or standardized in JPA; only Hibernate tr\u00aeeats an id path element specially.  You may also want to compare a key valu\u00aee to a query parameter, perhaps to find all Items for a given seller (a User) The first query pair uses an implicit table join; the second has no joins at all!   This completes our discussion of queries that involve joins. Our fin\u00aeal topic is nesting selects within selects: subselect Sub selects are an important and powerful featu\u00aere of SQL. A\u00ae subselect is a select query embedded in another query, usually in the SELECT, FROM, or WHERE clause. JPA supports subqueries in the WHERE clause. Subselects in the\u00ae FROM clause aren\u2019t supported because the query languages doesn\u2019t have transitive closure. The result of a query may not be usable for further selection in a FROM clause. The query languag\u00aee also doesn\u2019t support subselects in the SELECT clause, but y\u00aeou map can subselects to derived properties with @org.hibernate.annotations.Formula, as shown in section 5.1.3.  Subselects can be either correlated with the rest of the query or uncorrelated.",
    "page644": "The result of a subquery may contain either a single row or multiple rows. Typically, subqueries that return single rows perform aggregation. The following subquery returns the total number of items sold by a user; the outer query returns all users who have sold more than one item: The subquery in this example returns the maximum bid amount in the entire system; the outer query returns all bids whose amount is within one (U.S. dollar, Euro, and so on) of that amount. Note that in both cases, parentheses enclose the subquery in JPQL. This is always required Un\u00aecorrelated subqueries are harmless, and there is no reason not to use them when convenient. You can always rewrite them as two queries, because they don\u2019t. reference each other. You should think more carefully about the performance impact. of correlated subqueries. On a mature database, the performance cost of a simple correlated subquery is similar to the cost of a join. But it isn\u2019t necessarily possible to rewrite a correlated subquery using several separate queries.   If a subquery returns multiple rows, you combine it with quantification The following quantifiers are standardized: ALL The expression evaluates to true if the comparison is true for all values in the result of the subquer\u00aey. It evaluates to false if a single value of the subquery result fails the comparison test.  ANY The expression \u00aeevaluates to true if the comparison is true for some (any) value in the result of the subquery. If the subquery result is empty or no value satis\u0002fies the comparison, it evaluates to false. The keyword SOME is a synonym for ANY.  EXISTS Evaluates to true if the result of the subquery consists of one or more values",
    "page645": "This chapter explains query options that you may consider optional or advanced: transforming query results, filtering collections,\u00ae and the Hibernate criteria query API. First, we discuss Hibernate\u2019s ResultTransformer API, with which you can apply a re\u00aesult transformer to a query result to filter or marshal the result with your own code instead of Hi\u00aebernate\u2019s\u00ae default behavior.  In previous chapters, we always advised you to be careful when mapping collections, because it\u2019s rarely worth the effort. In this chapte\u00aer, we introduce collection filters, a native Hibernate feature that makes persistent collections more valuable. Finally, we look at another proprietary Hibernate feature, the org.hibernate.Cri\u0002teria query API, and some situations when you might prefer it to \u00aethe standard JPA query-by-criteria.  Let\u2019s start with th\u00aee transformation of query results.  Transforming query results You can apply a result transformer t\u00aeo a query result so that you can filter or marshal the result with your own procedure instead of the Hibernate default behavior. H\u00aeibernates default behavior provides a set of default transformers that you can replace and/or customize.  The result you\u2019re going to transform is that of a simple query, but you need to access the native Hibernate API org.hibernate.Query through the Session, as shown in the following listing Each object array is a \u201crow\u201d of the query result. Each element of that tuple can be accessed by index: here index 0 is a Long, index 1 a String, and index 2 a Date. The first result transformer we introduce instead returns a List of Lists",
    "page646": "In section 15.3.2, we showed how a query can return instances of a JavaBean dynamically by calling the ItemSummary constructor. In JPQL, you achieve this with the new operator. For criteria queries, you use the construct() method. The ItemSummary class must have a constructor that matches the projected query result.  Alternatively, if \u00aeyour JavaBean doesn\u2019t have the right constructor, you can still instantiate and populate its values through setters and/or fields with the AliasTo\u0002BeanResultTransformer.   You create the transformer with the JavaBean class you want to instantiate, here Item\u0002Summary. Hibernate requires that this class either has no constructor or a public nonargument constructor.  When transforming the query result, Hibernate looks for setter methods and fields with the same names as the aliases in the query. The ItemSummary class must either have the fields itemId, name, unauctioned, or the set\u00aeter methods setItemId(), setName(), and setAuctionEnd(). The fiel\u00aeds or setter method parameters must be of \u00aethe right typ\u00aee. If you have fields that map to some query aliases and setter methods for the rest, that\u2019s fine too.   You should also know how to write your \u00aeown ResultTransformer when none of the built-in ones suits you The built-in transformers in Hibernate aren\u2019t sophisticated; there isn\u2019t much differ\u0002ence between result tuples represented as lists, maps, or object arrays.   Next, we show you how to implement a ResultTransformer. Let\u2019s assume that you want a List<ItemSummary> returned from the query shown in listing 16.1, but you can\u2019t let Hibernate create an instance of ItemSummary through reflection on a con\u0002structor. Maybe your ItemSummary class is predefined and doesn\u2019t have the right con\u0002structor, fields, and setter methods. Instead, you have an ItemSummaryFactory to produce instances of ItemSummary",
    "page647": "For each result \u201crow,\u201d an Object[] tuple must be transformed into the desired result value for that row. Here you access each projection element by index in the tuple array and then call the ItemSummaryFactory to produce the query result value. Hibernate passes the method the aliases found in the query, for each tuple element. You don\u2019t need the aliases in this transformer, though. C You can wrap or modify the result list after transforming the tuples. Here you make. the returned List unmodifiable: ideal for a reporting screen where nothing should change the data. As you can see in the example, you transform query results in two steps: first you customize how to convert each \u201crow\u201d or tuple of the query result to whatever value you desire. Then you work on the entire List of these values, wrapping or converting again.  Next, we discuss another convenient Hibernate feature (where JPA doesn\u2019t have an equivalent): collection filters.  In chapter 7, you saw reasons you should (or rather, shouldn\u2019t) map a collection in your Java domain model. Th\u00aee main benefit of a collection mapping is easier access to data: you can call item.getI\u00aemages() or item.getBids() to access all images and bids associated with an Item. You don\u2019t have to write a JPQL or criteria query; Hibernate will execute the query for you when you start iterating through the collection elements.  The most obvious problem with this automatic data access is that Hibernate will always write the same query, retrieving all images or bids for an Item. Yo\u00aeu can customize the order of collection elements, but even that is a static mapping. What would you do to rend\u00aeer two lists of bids for an Item, in ascending and descending order by cre\u0002ation date? ",
    "page648": " Instead, you can use a Hibernate propri\u00aeetary feature, collection filters, that makes writing these queries easier, using the mapped collection. Let\u2019s say you have a persistent Item instance in memory, probably loaded with the EntityManager API. You want to list all bids made for this Item but further restrict the result to bids made by a particular User. You also want the list sorted in descending order by Bid#amount.The session.createFilter() \u00aemethod accepts a persistent collection and a JPQL query fragment. This query fragment doesn\u2019t require a select or from clause; here it only has a restriction with the where clause and an order by clause. The alias this always refers to elements of the collection, here Bid instances. The filter created is an ordinary org.hibernate.Query, prepared with a bound parameter and executed with list(), as usual.  Hibernate doesn\u2019t execute collection filters in memory. The Item bids collection may be uninitialized when you call the filter and, and if so, remains uninitialized. Furthermore, filters don\u2019t apply to transient collections or query results. You may only apply them to a mapped persistent collection currently referenced by an entity instance managed by the persistence context. The term filter is somewhat misleading, because the result of filtering is a comple\u00aetely new and different collection; the original collection isn\u2019t touched\u00ae.  To the great surprise of everyone, including th\u00aee designer of this feature, even trivial filters turn out to be useful. For example, you can use an empty query to paginate collection elements:",
    "page649": "Here, Hibernate executes the query, loading the collection elements and limiting the returned rows to two, starting with row zero of the result. Usually, you\u2019d use an order by with paginated queries.  You don\u2019t need a from clause in a collection filter, but you can have one if that\u2019s your style. A collection filter doesn\u2019t even need to return elements of the collection being filtered.  This next filter returns any Item sold by any of the bidders All this is a lot of fun, but the most important reason for the existence of collection filters is to allow your application to retrieve collection elements without initializing the entire collection. For large collections, this is important to achieve acceptable perfor\u0002mance. The following query retrieves all bids made for the Item with an amount greater or \u00aeequal to 100: Again, this doesn\u2019t initialize the Item#bids collection but returns a new collection.  Before JPA 2, query-by-criteria was only available as a proprietary Hibernate API. Today, the standardized JPA interfaces are equally as powerful as the old org.hiber\u0002nate.Criteria API, so you\u2019ll rarely need it. But several features are still only available in the Hibernate API, s\u00aeuch as query-by-example and embedding of arbitrary SQL frag\u0002ments. In the following section, you find a short overview of the org.hibernate .Criteria API and some of its unique options Using the org.hibernate.Criteria and org.hibernate.Example interfaces, you can build queries programmatically b\u00aey creating and combining org.hibernate.crite\u0002rion.* instances. You see how to use these APIs and how to express selection, restriction, joins, and projection. We assume that you\u2019ve read the previous chapter and that you know how these operations are translated into SQL.",
    "page650": ". All query examples shown here have an equivalent JPQL or JPA criteria example in the previous chapter, so you can easily flip back and\u00ae forth if you need to compare all three APIs.  Let\u2019s start with some basic selection examples. When you\u2019re ready to execute the query, \u201cattach\u201d it to a Session with getExecutable\u0002Criteria().  Note that thi\u00aes is a unique feature of the Hibernate criteria API. With JPA, you always need at least an EntityManagerFacto\u00aery to get a CriteriaBuilder.  You can declare the order of the results, equivalent to an order by clause in JPQL. The following query loads all User instances sorted in ascending order by first and last name: In this example, the code is written in the fluent style (using method chaining); method's such as add Order() return the original org.hibernate.Criteria.   Next, we look at restricting the selected records The Restrictions interface is the factory for individual Criterion you can add to the Criteria. Attributes are addressed with simple strings, here Item#name with \"name\".  You can also match substrings, similar to the like operator in \u00aeJPQL. The following query loads all User instances with username starting with \u201cj\u201d or \u201cJ\u201d A unique feature of the Hibernate Criteria API is the ability to write plain SQL frag\u0002ments in restrictions. This query loads all User instances with a username shorter than eight characters Hibernate sends the SQL fragment to th\u00aee database as is. You need the {alias} place\u0002holder to prefix any table alias in the final SQL; it always refers to the table the root entity is mapped to (USERS, in this case). You also apply a position parameter (named parameters aren\u2019t supported by this API) and specify its type as StandardBasic\u0002Types.INTEGER",
    "page651": "The result of this query is a List of Object[], one array for each tuple. Each array contains a Long (or whatever the type of the user\u2019s identifier is), a String, and an Address.  Just as with restrictions, you can add arbitrary SQL expressions and function c\u00aealls to projections This query returns a List of Strings, where strings have the form \u201c[Item name]:[Auc\u0002tion end date]\u201d. The seco\u00aend parameter for the projection is the name of the alias(es) you used in the query: Hibernate needs this to read the value of the ResultSet. The type of each projected element/alias is also needed: here, StandardBasic\u0002Types.STRING.  Hibernate supports grouping and aggregation. This query coun\u00aets users\u2019 last names This query returns all Bid instances of any Item sold by User \u201cjohndoe\u201d that doesn\u2019t have a buyNowPrice. The first inner j\u00aeoin of the Bid#item association is made with createCriteria(\"item\") on the root Criteria of the Bid. This nested Criteria now represents the association path, on which another\u00ae inner join is made with createCri\u0002teria(\"seller\"). Further restrictions are placed on each join Criteria; they will be combined with logical and in the where clause of the final SQL query. This query returns all Item instances, loads the Item#bids collection with an outer join, and loads Bid#bidder with an inner join. The Ite\u00aem#seller is also loaded: because it can\u2019t be null, it doesn\u2019t matter whether an inner or outer join is used. As always, don\u2019t fetch several collections in one query, or you\u2019ll create a Cartesian products (see section 15.4.5).   Next, you see that subqueries with criteria also work with nested Criteria instances.",
    "page652": "The DetachedCriteria is a query that returns the number of items sold restricted by a given User. The restriction relies on t\u00aehe alias u, so this is a correlated subquery. The \u201couter\u201d query then embeds the DetachedCriteria and provides the alias u. Note that the subquery is the\u00ae right operand of the lt() (less than) operation, which translates into 1 < ([Result of count query]) in SQL. Again, the position of the operands dictates that the comparison is based on geAll() (greater or equal than all) to find the bids with \u201cless or equal \u00aethan 10\u201d amount.  So far, there are a few good reasons to use the old org.hibernate.Criteria API. You really should use the standardized JPA query languages in new applications, though. The most interesting features of the old proprietary API we\u2019ve shown are embedded SQL expressions in restrictions and projections. Another Hibernate-only feature you may find interesting is query-by-example The idea behind example queries is that you provide an example entity instance, and Hibernate loads all entity instances that \u201clook like the example.\u201d This can be convenient if you have a complex search screen in your user interface, because you don\u2019t have to write extra classes to hold the e\u00aentered\u00ae search terms.  Let\u2019s say you have a search form in your application where you can search for User instances by last name. You can bind the form field for \u201clast name\u201d directly to the User#lastname property and \u00aethen tell Hibernate to load \u201csimilar\u201d User instances",
    "page653": "Create an \u201cempty\u201d instance of User as a template for your search, and set the property values you\u2019re looking for: people with the last name \u201cDoe\u201d Create an instance of \u00aeExample with the template. This API\u00ae allows you to fine-tune the search. You want the case of t\u00aehe last name to be ignored, and a substring search, so \u201cDoe\u201d, \u201cDex\u201d, or \u201cDoe Y\u201d will match.  D The User class has a Boolean property called activated. As a primitive, it can\u2019t be null, and its default value is false, so Hibernate would include it in the search and only return users that aren\u2019t activated. \u00aeYou want all users, so tell \u00aeHibernat\u00aee to ignore that property. E The Example is \u00aeadded to a Criteria as a restriction. Because you\u2019ve written the User entity class following JavaBean rules, binding it to a UI form should be trivial. It has regular getter and setter methods, and you ca\u00aen create an \u201cempty\u201d instance with the public no-argument constructor (remember our discus\u0002sion of constructor design in section 3.2.3.)  One obvious disadvantage of the Example API is that any string-matching options, such as ignoreCase() and enableLike(), apply to all string-valued properties of the template. If you searched for both last name and first name, both would be case insensitive substring matches. nsensitive substring matches.  By default, all non-null valued properties of the given entity template are added to the restriction of the example query. As shown in the last code snippet, you can manually exclude properties of the entity template by name wi\u00aeth excludePrope\u00aerty",
    "page654": "Other exclusion options are exclusion of zero-valued properties (such as int or long) with excludeZeroes() and disabling exclusion altogether with excludeNone(). If n\u00aeo properties are excluded, any null property of the template is added to the restriction in the SQL query with an is null check.  If you need more control over exclusion and inclusion of properties, you can extend Example and write your own PropertySelector:  After adding an Example restriction to a Criteria, you can add further restrictions to the query. Alternatively, you can add multiple example restrictions to a single query. The following query returns all Item instances with names starting with \u201cB\u201d or \u201cb\u201d and a seller matching a User example: You used the ResultTransformer API to write custom code to process a query result, returning a list of lists and a list of maps, and mapping aliases to bean properties.  We covered Hibernate\u2019s collection-filtering interfaces as well as making better use of mapped persistent collections. You explored the older Hi\u00aebernate Criteria query facility and when you might use it instead of the standardized criteria queries in JPA. We covered all the rela\u0002tional and Hibernate goodies using this API\u00ae: selection and ordering, restriction, projection and \u00aeaggregation, joins, subselects, and example queries.",
    "page655": "In this chapter, we cover customizing and embedding SQL in a Hibernate application. SQL was created in the 1970s, but ANSI didn\u2019t standardized it until 1986. Although each update of the SQL standard has seen new (and many controversial) features, every DBMS product that supports SQL does so in its own unique way. The burden of \u00aeportability is again on the database application developers. This is where Hibernate helps: its built-in query languages produce SQL that depends on the configured database dialect. Dialects also help produce all other automatically generated SQL (for example, when Hibernate has to retri\u00aeeve a collection on demand). With a simple dialect switch, you can run your application on a different DBMS. Hibernate generates all SQL statements for you, for all create, read, update, and delete (CRUD) operations.  Sometimes, though, you need more control than Hibernate and the\u00ae Java Persistence API provide: you need to work at a lower level of abstraction. With Hibernate, you can write your own SQL statements: Fall back to the JDBC API, and work directly with\u00ae the Connection, Prepare\u00aed\u0002Statement, and ResultSet interfaces. Hibernate provides the Connection, so you don\u2019t have to maintain a separate connection pool, and your SQL state\u0002ments execute within the same (current) transaction. \uf0a1 Write plain SQL SELECT statements, and either embed them within your Java code or externalize them (in XML files or annotations) as named queries. You execute these SQL\u00ae queries with the Java Persistence API, just like a regular JPQL query. Hibernate can then transform the query result according to your map\u0002ping. This also works with stored procedure calls.",
    "page656": "Replace SQL statements generated by Hibernate with your own hand-written SQL. For example, when Hibernate loads an entity instance with em.find() or loads a collection on-demand, your own SQL query can perform the load. You can also write your ow\u00aen Data Manipulation Language (DML) statements, such as UPDATE, INSERT, and DELETE. You might even call a stored procedure to preform a CRUD operation. You can replace all SQL statements automatically generated by Hibernate with custom statements. We start with JDBC fallback usage and then discuss Hibernate\u2019s automa\u00aetic result-map\u0002ping capabilities. Then, we show you how to override queries and DML statements in Hibernate. Last, we discuss integration with stored database procedures. Sometimes you want Hibernate to get out of \u00aethe way and directly access the database through the JDBC API. To do so, you need a java.sql.Connection interface to write and execute your own PreparedStatement and direct access to your statement ResultSet. Because Hibernate already knows how to obtain and close database con\u0002nections, it can provide your application with a Connection and release it when you\u2019re done.  This functionality is available with the org.hibernate.jdbc.Work API, a\u00ae callback\u0002style interface. You encapsulate your JDBC \u201cwork\u201d by implementing this interface; Hibernate calls your implementation providing a Connection. The following example executes an SQL SELECT and iterates through the ResultSet For this \u201cwork,\u201d an item identifier is needed, enforced with the final field and the con\u0002structor paramet",
    "page657": "The execute() method is called by Hibernate with a JDBC Connection. You don\u2019t have to close the connection when you\u2019re done. D You have to close and release other resources you\u2019ve obtained, though, such as the PreparedStatement and ResultSet In this case, Hibernate has already enlisted the JDBC Connection it provides with the current system transaction. Your statements are committed when the system transaction is committed, and all operations, whether executed with the EntityManager or Session API, are part of the same unit of work. Alternatively, if you want to return \u00aea value from your JDBC \u201cwork\u201d to the application, implement the interface org.hiber\u0002nate.jdbc.ReturningWork.  There are no limits on the JDBC operations you can perform in a Work implementation. Instead of a PreparedStatement, you may use a CallableStatement and execute a stored procedure in the database; you have full access to the JDBC API.  For simple queries and working with a ResultSet, such as the one in the previous example, a more convenient \u00aealter\u00aenative is available.  When you execute an SQL SELECT query with the JDBC API or execute a stored procedures that returns a ResultSet, you iterate through each row of this result set and retrieve the data you need. This is a labor-intensive task, and you end u\u00aep duplicating the same lines of code repeatedly. When you execute an SQL SELECT query with the JDBC API or execute a stored proce\u0002dure that returns a ResultSet, you iterate through each row of this result set and retrieve the data you need. This is a labor-intensive task, and you end up duplicating the same lines of code repeatedly.",
    "page658": "The returned Item instances are in persistent state, managed by the current persistence context. The result is therefore the same as with the JPQL query select i from Item i.  For this transformation, Hibernate reads the result set of the SQL query and tries to discover the column names and\u00ae types as defined in your entity mapping metadata. If the column AUCTIONEND is returned, and it\u2019s mapped to the Item#auctionEnd property, Hibernate knows how to populate that property and returns fully loaded entity instances.  Note that Hibernate expects the query to return all columns required to create an instance of Item, including all properties, embedded components, and foreign key\u00ae columns. If Hibernate can\u2019t find a mapped column (by name\u00ae) in the result set, an exception is thrown. You may have to use aliases in SQL to return the same column names as defined in your entity mapping metadata.  The interfaces javax.persistence.Query and org.hibernate.SQLQuery both support parameter binding. Th\u00aee following query returns only a single Item\u00ae entity instance Although available in Hibernate for both APIs, the JPA sp\u00aeecification doesn\u2019t consider named parameter binding for native queries portable. Therefore, some JPA providers may not support named parameters for native queries.   If your SQL query doesn\u2019t return the columns as mapped in your Java entity class, and you can\u2019t rewrite the query with aliases to rename columns in the result, you must create a result-set mapping",
    "page659": "The following query returns a List of managed Item entity instances. All columns of the ITEM table are included in the SQL projection, as required for the construction of an Item instance. But the query renames the NAME column to EXTENDED_NAME with an alias in the projection Hibernate can no longer automatically match the result set fields to Item properties: the NAME column is missing from the result set. You therefore specify a \u201cresult map\u0002ping\u201d with You map all fields of the result set to properties of the entity class. Even if only one field/column doesn\u2019t match the already mapped column name (here EXTENDED_NAME), all other columns and properties have to be mapped as wel\u00ael  SQL result mappings in annotations are difficult to read and as usual with JPA\u00ae annotations, they only work when declared on\u00ae a class, not in a package-info.java meta\u0002data file. We prefer externalizing such mappings into \u00aeXML files. The followin\u00aeg provides the same mapping: If both result-set mappings have the same name, the mapping declared in XML overrides the one defined with annotations.  You can also externalize the actual SQL query with @NamedNativeQuery or <named\u0002nativ\u00aee-query>, as shown in section 14.4. In all following examples, we keep the SQL statement embedded in the Java code, because this will make it easier for you to understand what the code does. But most of the time, you\u2019ll see result-set mappings in the more succinct XML syntax.",
    "page660": "With the Hibernate API, you can perform the result-set mapping directly within the query through alias placeholders. When calling addEntit\u00aey()\u00ae, you provide an alias, here i. In the SQL string, you then let Hibernate generate the actual aliases in the projection with placeholders such as {i.name} and {i.auctionEnd}, which refer to properties of the Item entity. No additional result-set mapping declaration is necessary; Hibernate generates\u00ae the aliases in the SQL string and knows how to read the property values from the query ResultSet. This is much more convenient than the JPA result-set mapping option.  Or, if you can\u2019t or don\u2019t want to \u00aemodify the SQL statement, use add Root() and add Property() on the org.hibernate.SQLQuery to per\u00aeform the mapping This is effectively an eager fetch of the association Item#seller. Hibernate knows that each row contains the fields for an Item\u00ae and a User enti\u00aety instance, linked by the SELLER_ID.  The duplicate columns in the result set would be i.ID and u.ID, which both have the same name. You\u2019ve renamed them with an a\u00aelias to ITEM_ID and USER_ID, so you have to map how the result set is to be transformed As before, you have to map all fields of each entity result to column names, even if only two have different names as the original entity mapping.  This query is much easier to map with the Hibernate API:",
    "page661": "Hibernate will add auto-generated unique aliases to the SQL statement for the {i.*} and {u.*} placeholders, so the query won\u2019t return duplicate column names.   You may have noticed the dot syntax in the previous JPA result mapping for the home Address embedded component in a User. Let\u2019s look at this special case again We\u2019ve shown this dot syntax several times before when discussing embedded components: you reference the street property of home Address with homeAddress.street. For nested embedded components, you can write homeAddress.city.name if City isn\u2019t just a string but another embed\u00aedable class.  Hibernate\u2019s SQL query A\u00aePI also supports the dot syntax in alias placeholders for component properties. Here are the same query and result-set mapping: The query (outer) joins the ITEM and BID tables. The projection returns all columns required to construct\u00ae Item and Bid instances. The query renames duplicate columns such as ID with aliases, so field names are unique in the result. C Because of the renamed fields, you have to map each column to its respective entity property. D Add a Fetch Return for the bids collection with the alias of the owning entity  and map the key a\u00aend element special properties to the foreign key column BID_ITEM_ID and the identifier of the Bid. Then the code maps each property of Bid to a field of the result set. Some fields are mapped twice, as required by Hibernate for construction of the collection The number of rows in the result set is a product: one item has three bids, one item has one bid, and the last item has no bids, for a total of five rows in the result. F The first element of the result tuple is the Item instance; Hibernate initialized the bids collection",
    "page662": "The second element of the result tuple is each Bid. Alternatively, if you don\u2019t have to manually map the result because the field names returned by your SQL query match the already-mapped columns of the entities, you can let Hibernate insert ali\u00aeases into your SQL statement with placeholder\u00aes: Eager fetching of collections with dynamic SQL result mappings is only available with the Hibernate API; it\u2019s not standardized in JPA  So far, you\u2019ve seen SQL queries returning managed entity instances. You can also return transient instances of any class with the right constructor. The returned column types have to match the constructor parameter types; Hibernate would default to BigInteger for the ID column, so you map it to a Long with the class attribute.  The Hibernate API gives you a choice. You can either use an existing result map\u0002ping for the query by name, or apply a r\u00aeesult transformer, as you saw for JPQL queries in section 16.1: You can use an existing result mapping. C Alternatively, you can map the fields returned by the query as scalar values. Without a result transformer, you\u2019d get an Object[] for each result row. D Apply a built-in result transformer to turn the Object[] into instances of ItemSummary. As explained in section 15.3.2, Hibernate can use any class constructor with such a mapping. Instead of ItemSummary, you can construct Item instances. They will be in either transient or detached state, depending on whether you return and map an identifier value in your query.   You can also mix different kinds of result mappings or return scalar values directly.",
    "page663": "Hibernate usually generates CRUD SQL statements at startup. It caches the SQL statements internally for future use, thus avoiding any runtime costs of \u00aeSQL generation forthe most c\u00aeommon operations. You\u2019ve seen how you can override the R in CRUD, sonow, let\u2019s do the same for CUD. For each entity, you can define custom CUD SQL statements with the Hibernate annotations @SQLInsert, @SQLUpdate, and @SQLDelete,respectivelyYou need to be careful with the binding of arguments for SQL statements and parameter placeholders (the ? in the statements). For CUD customization, Hibernate onlysupports positional parameters. What is the right order for the parameters? There\u2019s an internal order to howHibernate binds arguments to SQL parameters for CUD statements. The easiest way tofigure out the right SQL statement and parameter order is to let Hibernate generatean example for you. Without any custom\u00ae SQL statements, enable DEBUG logging forthe org.hibernate.persister.entity category, and search the Hibernate startupoutput for lines like the following:These automatically generated SQL statements show the right parameter order, andHibernate always binds the values in that order. Copy the SQL statements you want tocustomize into annotations, and make the necessary changes. A special case is properties of an entity class mapped to another table with@SecondaryTable. The CUD customization statements we\u2019ve shown so far only refer tothe columns of the main entity table. Hibernate still executes automatically generatedSQL statements for insertion, updates, and deletion of rows in the secondary table(s).",
    "page664": "statements in an XML file. This also simplifies ad hoc testing, because you can copyand paste SQL statements between an XML file and your SQL database console. You\u2019ve probably noticed that all the SQL examples in the previo\u00aeus sections weretrivial. In fact, none of the examples required a query written in SQL we could haveused JPQL in each case. To make the next example moreinteresting, we write a query that can\u2019t be expressed inJPQL, only in SQL. This is the mapping of the association a \u00aeregular@ManyToOne of the PARENT_I\u00aeD foreign key column:Categories form a tree. The root of the tree is a Category node without a parent. Thedatabase data for the ex\u00aeample tree is in figure 17.2. You can also represent this data as a tree diagram, as shown in figure 17.3. Alternatively, you can use a sequence of paths and the level of each node:Now, consider how your application loads Category instances. You may want\u00ae to findthe root Category of the tree. This is a trivial JPQL query:You can easily query for t\u00aehe categories in a particular level of the tree, such as all children of the root:This query\u00ae will only return direct children of the root: here, categories Two and Three. How can you load the entire tree (or a subtree) in one query? This isn\u2019t possible withJPQL, because it would require recursion: \u201c\u00aeLoad categories at this level, then all the children on the\u00ae next level, then all the children of those, and so on.\u201d In SQL, you can writesuch a query, using a common table expression (CTE), a feature also\u00ae known as subquery factoring.",
    "page665": "It\u2019s a complex query\u00ae, and we won\u2019t spend too much time on it here. To understand it,read the last SELECT, querying the CATEGORY_LINK view. Each row in that view represents a node in the tree\u00ae. The view is declared here in the WITH() AS operation. TheCATEGORY_LINK view is a combined (union) result of two other SELECTs. You add additional information to the view during recursion, such as the PATH and the LEVEL ofeach node.The XML maps the ID, CAT_NAME, and PARENT_ID fields to properties of the Categoryentity. The mapping returns the \u00aePATH and LEVEL as additional scalar values. To execute the named SQL query and access the result, write the following:Each tuple contains a managed, persistent Category instance; its path in the tree as astring (such as /One, /One/Two, and so on); and the tree level of the node. Alterna\u00aetively, you can declare and map an SQL query in a Hibernate XML metadata file:We left out the SQL query in this snippet; it\u2019s the same as the SQL statement shownearlier in the JPA example. As mentioned in section 14.4, with regard to the execution in Java code, it doesn\u2019tmatter which syntax you declare your named queries in: XML file or annotations. Eventhe language doesn\u2019t matter it can be JPQL or SQL. Both Hibernate and JPA queryinterfaces have methods to \u201cget a named query\u201d and execute it independently fromhow you defined it. This concludes our discussion of SQL result mapping for queries. The next subjectis customization of SQL statements for CRUD operations, replacing the SQL automatically generated by Hibernate for creating, reading, updating, and deleting data in thedatabase.",
    "page666": "The first custom SQL you write loads an entity instance of the User class. All the following code examples show the same SQL that Hibern\u00aeate executes au\u00aetomatically bydefault, without much customization this helps you understand the mapping \u00aetechnique more quickly. You can customize retrieval of an entity instance with a loaderHibernate has two requirements when you override an SQL query to load an entityinstance: Write a named query that retrieves the entity instance. We show an example inSQL, but as always, you can also write named queries in JPQL. For an SQL query,you may need a custom result mapping, as shown earlier in this chapter.Activate the query on \u00aean entity class with @org.hibernate.annotationsLoader. This enables the query as the replacement for\u00ae the Hibernate-generated query.Let\u2019s override how Hibernate loads an instance of the User entity, as shown in the following listing.Annotations declare the query to load an instance of User; you can also declare it inan XML file (JPA or Hibernate metadata). You can call this named query directly inyour data-access\u00ae code when needed. The query must have exactly one parameter placeholder, which Hibernate sets as theidentifier value of the instance to load. Here it\u2019s a positional parameter, but a namedparameter would also work. For this trivial query, you don\u2019t need a custom result-set mapping. The User class mapsall fields returned by the query. Hibernate can automatically transform the result.",
    "page667": "Setting the loader for an entity class to a named query enables the query for all operations that r\u00aeetrieve an instance of User from the database. There\u2019s no in\u00aedication of thequery language or where you declared it; this is independent of the loader declaration.In \u00aea named loader query for an entity, you have to SELECT (that is, perform a projection for) the following properties of the entity class: The value of the identifier property or properties, if a composite primary key isused. All scalar properties of basic type. All properties of embedded components. An entity identifier valu\u00aee \u00aefor each @JoinColumn of each mapped entity association such as @ManyToOne owned by the loaded entity class. All scalar properties, embedded component properti\u00aees, and association joinreferences that are inside a @SecondaryTable annotation. If you enable lazy loading for some properties, through interception and bytecodeinstrumentation, you don\u2019t need to load the lazy properties (see section 12.1.3).Hibernate always calls the enabled loader query wh\u00aeen a User has to be retrieved fromthe database by identifier. When you call em.find(User.class, USER_ID), your custom query will execute. When you call someItem.getSeller().getUsername(), andthe Item#seller proxy has to be initialized, your custom query will load the data. You may also want to customize how Hibernate creates, updates, and deletes aninstanc\u00aee of User in the database.",
    "page668": "Hibernate usually generates \u00aeCRUD SQL statements at startup. It caches the SQL statements internally for future use, thus avoiding any ru\u00aentime costs of SQL generation forthe most common operations\u00ae. You\u2019ve seen h\u00aeow you can override the R in CRUD, sonow, let\u2019s do the same for CUD. For each entity, you can define custom CUD SQL statements with the Hibernate annotations @SQLInsert, @SQLUpdate, and @SQLDelete,respectivelyYou need to be careful with the binding of arguments for SQL statements and paramete\u00aer placeholders (the ? in the statements). For CUD customization, Hibernate onlysupports positional parameters. What is the right order for the parameters? There\u2019s an internal order to howHibernate binds arguments to SQL parameters for CUD statements. The easiest way tofigure out the right SQL statement and parameter order is to let Hibernate generatean example for you. Without any custom SQL statements, enable DEBUG logging forthe org.hibernate.persister.entity category, and search the Hibernate startupoutput for lines like the following:These automatically generated SQL statements show the right parameter order, andHibernate always binds the values in that order. Copy the SQL statements you\u00ae want tocustomize into annotations, and make the necessary changes. A special case is properties of an entity class mapped to another table with@SecondaryTable. The CUD customization statements we\u2019ve shown so far only refer tothe columns of the main entity table. Hibernate still executes automatically generatedSQL statements for insertion, updates, and deletion of rows in the secondary table(s).",
    "page669": "You can customize th\u00aeis SQL \u00aeby adding the @org.hibernate.annotations.Table annotation to your entity class and setting its sqlInsert, sqlUpdate, and sqlDelete attributes. If you prefer to have your CUD SQL statements\u00ae in XML, your only choice is to mapthe entire entity in a Hibernate XML metadata file. The elements in this proprietarymapping format for custom CUD statements are <sql-insert>, <sql-update>, and<sql-delete>. Fortunately, CUD statements are usually much more trivial than queries, so annotations are fine in most applications. You\u2019ve now added custom SQL statements for CRUD operations of an entityinstance. Next, we show how to override SQL statements  loading and modifying acollection.Let\u2019s override the SQL stafortement Hibernate uses when loading the Item#images collection. This is a collection of embeddable components mapped with @ElementCollection; the procedure is the same for collections of basic types or many\u00ae-valuedentity associations (@OneToMany or @ManyToMany)As before, you declare that a named query will load the collection. This time, however,you must declare and map the result of the query in a Hibernate XML metadata file,which is \u00aethe only facility that supports mapping of query results to collection properties:The query has to have one (positional or named) parameter. Hibernate \u00aesets its valueto the entity identifier that owns the collection. Whenever Hibernate need to initializethe Item#images collection, Hibernate now executes your custom SQL query.",
    "page670": "QUOTING SQL IDENTIFIERS- From time to time, especially in legacy databases, you\u2019ll encounter identifiers with strange characters or whitespace, or wish to force case sensitivity. Or, as in the previous example, the automatic mapping of a class or property would require a table or col\u0002umn name that is a reserved keyword.   Hibernate 5 knows the r\u00aeeserved keywords of your DBMS through the configured\u00ae database dialect. Hibernate 5 can automatically put quotes around such strings when generating SQL. You can enable this automatic quoting with hibernate.auto_quote _keyword true in your persistence unit configuration. If you\u2019re using an older version of Hibernate, or you find that the dialect\u2019s information is incomplete, you must still apply quotes on names manually in your mappings if there is a conflict w\u00aeith a keyword.   If you quote a table or column name in your mapping \u00aewith backticks, Hibernate always quotes this identifier in the generated SQL. This still works in latest versions of Hibernate, but JPA 2.0 standardized this functionality as delimited identifiers with dou\u0002ble quotes.  If you have to quote all SQL identifiers, create an orm.xml file and add the setting <delimited-identifiers/> to its <persistence-unit-defaults> section, as shown in listing 3.8. Hibernate then enforces quoted identifiers everywhere.   You should consider renaming tables or columns with reserved keyword names whenever possible. Ad hoc SQL queries are difficult to write in an SQL console if you have to quote and escape everything properly by hand.   Next, you\u2019ll see how Hibernate can help when you encounter organizations with strict conven\u00aetions for database table and column names.To summarize, our recommendations on identifier generator strategies are as follows:  In general, we prefer pre-insert generation strategies that produce identifier values independently before INSERT. Use enhanced-sequence, which uses a native database sequence when supported and otherwise falls back to an extra database table with a single column and row, emulating a sequence. We assume from now on that you\u2019ve added identifier properties to the entity classes of your domain model and that after you complete the basic mapping of each entity and its identifier property, you continue to \u00aemap the value-typed properties of the entities. We talk about value-type mappings in the next chapter. Read on for some special options that can simplify and enhance your class mappings. Entity-mapping options- You\u2019ve now mapped a persistent class with @Entity, using defaults for all other settings, such as the mapped SQL table name. The following section explores some classlevel options an\u00aed how you control them:  Naming defaults and strategies Dynamic SQL generation Entity mutability These are options; you can skip \u00aethis section and come back later when you have to deal with a specific problem. Controlling names- Let\u2019s first talk about the naming of entity classes and tables. If you only specify @Entity on the persistence-capable class, the default mapped tabl\u00aee name is the same as the class name. Note that we write SQL artifact names in UPPERCASE to make them easier to distinguish SQL is actually case insensitive. So the Java entity class Item maps to the ITEM table. You can override the table name with the JPA @Table annotation, as shown next.",
    "page671": "Entities are the coarser-grained classes of your system. Their instances have an independent life cycle and their own identit\u00aey, and many other instances can ref\u0002erence them.  Value types, on the other hand, are dependent on a particular entity class. A value type instance is bound to its owning entity instance, and only one entity instance can reference it it has no individual identity.  We looked at Java identity, object equality, and database identity, and at what makes good primary keys. You learned which generators for primary key values Hibernate provides out of the box, and how to use and extend this identifier system.  We discussed some useful class mapping options, such as naming strategies and dynamic SQL generation. After spending the previous chapter almost exclusively on entities and the respec\u0002tive class- and identity-mapping options, we now focus on value types in their various forms. We split value types into two categories: basic value-typed classes that come with the JDK, such as String, Date, primitives, and their wrappers; and developer\u0002defined value-typed class\u00aees, such as Address and MonetaryAmount in CaveatEmptor.  In this chapter, we first map persistent properties with JDK types and learn the basic mapping annotations. You see how to work with various aspects of properties: overriding defaults, customizing access, and generated values. You also see how SQL is used with derived properties and transformed column values. We wrap up basic properties with temporal properties and mapping enumerations. We then dis\u0002cuss custom value-typed classes and map them as embeddable components. You learn how classes relate to the database schema and make your classes embeddable, while allowing for overriding embedded attributes. We complete embeddable components by mapping nested components. Finally, we discuss how to customize loading and storing of property values at a lower level with flexible JPA converters, a standardized extension point of every JPA provider. Dynamic SQL generation- By default, Hibernate creates SQL statements for each persistent class when the persis\u0002tence unit is created, on startup. These statements are simple create, read, update, and delete (CRUD) operations for reading a single row, deleting a row, and so on. It\u2019s cheaper to store these in memory up front, instead of generating SQL strings every time such a simple query has to be executed at runtime. In addition, prepared state\u0002ment caching at the JDBC level is much more efficient if there are fewer statements. How can Hibernate create an UPDATE statement on startup? After all, the columns to be updated aren\u2019t known at this time. The answer is that the generated SQL state\u0002ment updates all columns, and if the value of a particular column isn\u2019t modified, the statement sets \u00aeit to its old value.   In some situations, such as a legacy table with hundreds of columns where the SQL statements will be large for even the simplest operations (say, only one column needs updating), you should disable this startup SQL generation\u00ae and switch to dynamic state\u0002ments generated at runtime. An extremely large number of entities can also impact startup time, because Hibernate has to generate all SQL statements for CRUD up front. Memory consumption for this query statement cache will also be high if a doz\u00aeen state\u0002ments must be cached for thousands of entities. This can be an issue in virtual envi\u0002ronments with memory limitations, or on low-po\u00aewer devices.   To disable generation of INSERT and UPDATE SQL statements on startup, you need native Hibernate annotations:",
    "page672": "This configuration by exception app\u00aeroach means you don\u2019t have to annotate a property to make it persistent; y\u00aeou only have to configure the mapping in an exceptional case. Sev\u0002eral annotations are available in JPA to customize and control basic property map. Overriding basic property defaults- You might not want all properties of an entity class to be persistent. For example, although it makes sense to have a persistent Item#initialPrice property, an Item#totalPriceIncludingTax property shouldn\u2019t be persistent if you only com\u0002pute and use its value at runtime, and hence shouldn\u2019t be stored in the database. To exclude a property, mark the field or the getter method of the property with the @javax.pers\u00ae\u00aeistence.Transient annotation or use th\u00aee Java transient keyword. The transient keyword usually only excludes fields for Java serialization, but it\u2019s also rec\u0002ognized by JPA providers.   We\u2019ll come back to the placement of the annotation on fields or getter methods in a moment. Let\u2019s assume as we have before that Hibernate will access fields directly because @Id has been placed on a field. Therefore, all other JPA and Hibernate map\u0002ping an\u00aenotations are also on fields.  We have to admit that this annotation isn\u2019t very useful. It only has two parameters: the one shown here, optional, marks the property as not optional at the Java object level. By default, all persistent properties are nullable and optional; an Item may have an unknown initialPrice. Mapping the initialPrice property as non-optional makes sense if you have a NOT NULL constraint on the INITIALPRICE column in your SQL schema. If Hibernate is generating the SQL schema, it will include a NOT NULL con\u0002straint automatically for non-optional properties. Mapping basic pr\u00aeoperties- When you map a persistent class, whether it\u2019s an entity or an embeddable type (more about these later, in section 5.2), all of its properties are considered\u00ae persistent by default. The default JPA rules for properties of persistent classes are these:  \uf0a1 If the property is a primitive or a primitive wrapper, or of type String, BigInteger, BigDecimal, java.util.Date, java.util.Calendar, java.sql.Date, java.sql .Time, java.sql.Timestamp, byte[], Byte[], char[], or Character[], it\u2019s auto\u0002matically persistent. Hibernate loads and stores the value of the property in a col\u0002umn with an appropriate SQL type and the same name as the property. Otherwise, if you annotate the class of the property as @Embeddable, or you map the property itself as @Embedded, Hibernate maps the property as an embedded component of the owning class. We discuss embedding of components later in this chapter, with the Address and MonetaryAmount embeddable classes o\u00aef CaveatEmptor. Otherwise, if the type of the property is java.io.Serializable, its value is stored in its serialized form. This typically isn\u2019t what you want, and you should always map J\u00aeava classes instead of storing a heap of bytes in the database. Imag\u0002ine maintaining a database with this binary information when the application is gone in a few years. Otherwise, Hibernate will throw an exception on startup, complaining that it doesn\u2019t understand the type of the property",
    "page673": "The Column annotation has a few other parameters, most of which control SQL-level details such as ca\u00aetalog and schema names. They\u2019re rarely needed, and we only show them throughout this book when necessary.   Property annotations aren\u2019t always on fields, and you may not want Hibernate to access fields directly.  Customizing property access- The persistence engine accesses the properties of a class either directly through fields or indirectly through getter and setter methods. An annotated entity inherits the default from the position of the mandatory @Id annotation. For example, if you\u2019ve declared @Id on a field, not a getter method, all other mapping annotations for that entity are expected on fields. Annotations are never on the setter methods.   The default access strategy isn\u2019t only applicable to a single entity class. Any @Embedded class inherits the default or explicitly declared access strategy of its own\u0002ing root entity clas\u00aes. We cover embedded components later in this chapter. Further\u0002more, Hibernate accesses any @MappedSuperclass properties with the default or explicitly declared access strategy of the mapped entity class. Inheritance is the topic of chapter 6.   The JPA specification offers the @Access annotation for overriding the default behavior, with the parameters AccessType.FIELD and AccessType.PROPERTY. If you set @Access on the class/entity level, Hibernate accesses all properties of the class according to the selected strategy. You then set any other mapping annotations, including the @Id, on either fields or getter methods, respectively. Now, when you store an Item and forget to set a value on the initialPrice field, Hibernate will complain with an exception before hitting the database with an SQL statement. Hibernate knows that a value is required to perform an INSERT or UPDATE.\u00ae If you don\u2019t mark the property as optional and try to save a NULL, the database will reject the SQL statement, and Hibernate will throw a constraint-violation exception. There isn\u2019t much difference in the end result, but it\u2019s cleaner to avoid hitting the data\u0002base with a statement that fails. We\u2019ll talk about the other parameter of @Basic, the fetch option, when we explore optimization strategies later, in section  Instead of @Basic, most engineers use the more versatile @Column annotation to declare nullability: We\u2019ve now shown you three ways to declare whether a property value is required: with the @Basic annotation, the @Column annotation, and earlier with the Bean Validation @NotNull annotation in section 3.3.2. All have the same effect on the JPA provider: Hibernate does a null check when saving and generates a NOT NULL constraint in the database schema. We recommend the Bean Validation @NotNull annot\u00aeation so you can manually validate an Item instance and/or have your user interface code in the presentation layer execute validation checks automatically.   The @Column annotation can also override the mapping of the property name to the database column:",
    "page674": "Hibernate reads your Java domain model classes and mapping metadata and generates schema DDL statements. You can export them into a text file or execute them directly on your database whenever you run integration tests. Because schema languages are mostly vendor-specific, every option you put in your mapping metadata has the potential to bind the metadata to a particular database product keep this in mind when using schema features.  Hibernate creates the basic schema for your tables and constraints automatically; it even creates sequences, d\u00aeepending on the identifier generator you select. But there are some schema artifacts Hibernate can\u2019t and won\u2019t create automatically. These include all kinds of highly vendor-specific performance options and any other artifacts that are relevant only for the physical storage of data (tablespaces, for example). Besides these physical concerns, your DBA will often supply custom additional schema statements to improve the generated schema. DBAs should get involved early and verify the automatically generated schema from Hibernate. Never go into production with an unchecked automatically generated schema. If your development process allows, changes made by the DBA can flow back into your Java systems, for you to add to mapping metadata. In many projects, the mapping metadata can contain all the necessary schema changes from a DBA. Then Hibernate can generate the final production schema during the regular build by including all comments, constraints, indexes, and so on.  In the following sections, we show you how to customize the generated schema and how to add auxiliary database schema artifacts (we call them objects sometimes; we don\u2019t mean Java ob\u00aejects here). We discuss custom data types, additional integrity rules, indexes, and how you can replace some of the (sometimes ugly) auto-generated artifact names produced by Hibernate.If your application can auction an item only once in the real world, your database schema should guarantee that. If an auction always has a starting price, your\u00ae database model should include an appropriate constraint. If data satisfies all integrity rules, the data is consistent, a term you\u2019ll meet again in section 11.1.  We also assume that consistent data is correct: everything the database states, either explicitly\u00ae or implicitly, is true; everything else is false. If you want to know more about the theory behind this approach, look up the closed-world assumption (CWA) Sometimes you can start a project top-down. There is\u00ae no existing database schema and ma\u00aeybe not even any data your application is completely new. Many developers like to let Hibernate automatically generate the scripts for a database schema. You\u2019ll probably also let Hibernate deploy the schema on the test database on your development machine or your continuous build systems for integra\u00aetion testing. Later, a DBA will take the generated scripts and write an improved and final schema for production deployment. The first part of this chapter shows you how to improve the schema from within JPA and Hibernate, to make your DBA happy.  At the other end of the spectrum are systems with existing, possibly complex schema\u00aes, with years\u2019 worth of data. Your new application is just a small gear in a big m\u00aeachine, and your DBA won\u2019t allow any (sometimes even non-disruptive) changes to the database. You need a flexible object/relational mapping so you don\u2019t have to twist and bend the Java classes too much when things don\u2019t fit right away. This will be the subject of the second half of this chapter,\u00ae including a discussion of composite primary and foreign keys.  Let\u2019s start with a clean-room implementation and Hibernate-generated schemas.",
    "page675": "This property defines when the create and drop scripts should be executed. Your custom SQL scripts will contain CREATE DOMAIN statements, which must be executed before the tables using the\u00aese domains are created. With these settings, the schema generator runs the create script first before reading your ORM metadata (annotations, XML files) and creating the tables. The drop script executes after Hiberna\u00aete drops the tables, giving you a chance to clean up anything you created. Other options are metadata (ignore cu\u00aestom script sources) and script (only use a custom script source; ignore ORM metadata in annotations and XML files). D This is the location of the custom SQL script for creation of the schema. The path is (a) the location of the script resource on the classpath; (b) the location of the script as a file:// URL; or, if neither (a) nor (b) matches, (c) the absolute or relative file path on the local file system. This example uses (a). E This is the custom SQL script for dropping the schema. F This load script runs after the tables have been created. We\u2019ve mentioned that DDL is usually highly vendor-specific. If your application has to support several database dialects, you may need several sets of create/drop/load scripts to customize the schema for each database dialect. You can solve this with several persistence unit configurations in the persistence.xml file.  Alternatively, Hibernate has\u00ae its own proprietary configuration for schema customization in an hbm.xml mapping file, You can hook the following three types of custom SQL scripts\u00ae into Hibernates schema-generation process:  The create script executes when the schema is generated. A custom create script can run before, after, or instead of Hibernates automatically generated scripts. In other words, you can write an SQL script that runs before or after Hibernate generates tables, constraints, and so on from your mapping metadata.  The drop script executes when Hibernate removes schema artifacts. Just like the create script, a drop script can run before, after, or instead of Hibernates automatically generated statements. The load script always executes after Hibernate generates the schem\u00aea, as the last step after creation. Its main purpose is importing test or master data, before your application or unit test runs. It can contain any kind of SQL statement, including DDL statements such as ALTER, i\u00aef you want to further customize the schema. This customization of the schema-generation process is actually standardized; you configure it with JPA properties in persistence.xml for a persistence unit. By default, Hibernate expects one SQL statement per line in scripts. This switches to the more convenient multiline extractor. SQL statements in scripts are terminated with semicolon. You can write your own org.hibernate.tool.hbm2ddl.ImportSqlCommandExtractor implementation if you want to handle the SQL script in a different way.",
    "page676": "Table constraints An integrity rule that applies to several columns or several rows is a table co\u00aenstraint. A typical declarative table constraint is UNIQUE: all rows are checked for duplicate values (for example, each user must have a distinct email address). A rule affecting only a single row but multiple columns is \u201cthe auction end time has to be after the auction start time.\u201d Database constraints If a rule applies to more than one table, it has database scope. You should already be familiar with th\u00aee most common database constraint, the foreign key. This rule guarantees the integrity of references betw\u00aeeen rows, usually in separate tables, but not always (self-referencing foreign key constraints aren\u2019t uncommon). Other database constraints involving several tables aren\u2019t uncommon: for example, a bid can only be stored if the auction end time of the referenced item hasn\u2019t been reached. Most (if not all) SQL database management systems support these kinds of constraints and the most important options of each. In addition to simple keywords su\u00aech as NOT NULL and UNIQUE, you can usually also declare more complex rules with the CHECK constraint that applies an arbitrary SQL expression. Still, integrity constraints are one of the weak areas in the SQL standard, and solutions from vendors can differ significantly.  Furthermore, non-declarative and procedural constraints are possible with database triggers that intercept data-modification operations\u00ae. A trig\u00aeger can then implement the constraint procedure directly or call an existing stored procedure.  Integrity constraints can be checked immediately when a data-modification statement is executed, or the check can be deferred until the end of a transaction. The violation response in SQL databases is usually rejection without any possibility of customization. Foreign keys are special because you can  typically decide what should happen with ON DELETE or ON UPDATE for referenced rows. Systems that ensure data integrity only in application code are prone to data corruption and often degrade the quality of the database over time. If the data store doesn\u2019t enforce rules, a trivial undetected application bug can cause unrecoverable problems such as incorrect or lost data.  In contrast to ensuring data \u00aeconsistency in procedural (or object-oriented) application code, database management systems allow you to implement integrity rules with declar\u00aeations, as a database schema. The advantages of declarative rules are fewer possible errors in code and a chance for the DBMS to optimize data access.  In SQL databases, we identify four kinds of \u00aerules: Domain constraints A domain is (loosely speaking, and in the database world) a data type in a database. Hence, a domain constraint defines the range of possible values a particular data type can handle. For example, an INTEGER data type is usable for integer values. A CHAR data type can hold character strings: for example, all characters defined in ASCII or some other encoding. Because we mostly use data types built-in the DBMS, we rely on the domain constra\u00aeints as defined by the vendor. If supported by your SQL database, you can use the (often limited) support for custom domains to add additional constraints for particular existing data types, or create user-defined data types (UDT). Column constraints Restricting a column to hold values of a particular domain and type creates a column constraint. For example, you declare in the schema that the EMAIL column holds values of VARCHAR type. Alternatively, you could create a new domain called EMAIL_ADDRESS with further constraints and apply it to a column instead of VARCHAR. A special column constraint in an SQL database is NOT NULL.",
    "page677": "This constraint restricts valid username values to a maximum length of 15 characters, and the string can\u2019t begin with admin to avoid confusion. You can call any SQL functions supported by your DBMS; the column Definition is always passed through into the exported schema.  Note that you have a choice: creating and using a domain or adding a single column constraint has the same effect. Domains are usually easier to maintain and avoid duplicating.  At the time of writing, Hibernate doesn\u2019t support its proprietary annotation @org.hibernate.\u00aeannotations.Check on individual properties; you use it for table level constraints. Hibernate appends table constraints to the generated CREATE TABLE statement, which can contain arbitrary SQL expressions.  You can implement multirow table constraints with expressions that are more complex. You may need a sub select in the expression to do this, which may not be supported by your DBMS. But there are some common multiro\u00aew table constraints, like UNIQUE, that you can add directly in the mappings. You\u2019ve already seen the @Column(unique true | false) option in the previous section. Now all pairs of USERNAME and EMAIL must be unique, for all rows in the USERS table. If you don\u2019t provide a name for the constraint here, UNQ_USERNAME_EMAIL an automatically generated and probably ugly name is used.  The last kinds of constraints we discuss are database-wide rules that span several tables. Hibernate passes on database constraint violations in error exceptions; check whether the exception in your transaction has a cause, somewhere in the exception chain, of type org.hibernate.exception.ConstraintViolationException. This exception can provide more information about the error, such as the name of the failed database constraint. The SQL standard includes domains, which unfortunately are rather limited and often not supported by the DBMS. If your system supports SQL domains, you can use them to add constraints to data types.  In your custom SQL create script, define an EMAIL_ADDRESS domain based on the VARCHAR data type: The additional constraint is a check of the presence of an @ symbol in the string. The (relatively minor) advantage of such domains in SQL is the abstraction of common constraints into a single location. Domain constraints are always checked immediately when data is inserted and modified. Several constraints are present in this mapping. The NOT NULL constraint is common; you\u2019ve seen it many times before. The second is a UNIQUE column constraint; users can\u2019t have duplicate email addresses. At the\u00ae time of writing, there was unfortunately no way to customize the name of this single-column unique constraint in Hibernate; it will get an ugly auto-generated name in your schema. Last, the column Definiti\u00aeon refers to the domain you\u2019ve added with your custom create script. This definition is an SQL fragment, exported into your schema directly, so be careful with database-specific SQL",
    "page678": "This statement declares the foreign key constraint for the ITEM_ID column in the BID table, referencing the primary key column of the ITEM table. You can customize the name of the constraint with the foreign Key option in an @JoinColumn mapping A foreign Key attribute is also supported in @PrimaryKeyJoinColumn, @MapKeyJoinColumn, @JoinTable, @CollectionTable, and @AssociationOverride mappings.  The @ForeignKey annotation has some rarely needed options we haven\u2019t shown: You can write your own foreign Key Definition, an SQL fragment such as FOREIGN KEY ([column]) REFERENCES [table]([column]) ON UPDATE [action]. Hibernate will use this SQL fragment instead of the provider-generated fragment, it can be in the SQL dialect supported by your DBMS.  The Constraint Mode s\u00aeetting is useful if you want to disable foreign key generation completely, with the value NO_CONSTRAINT. You can then write the foreign key constraint yourself with an ALTER TABLE statement, probably in a load script as we\u2019ve shown. Naming constraints properly is not only good practice, but also helps significantly when you have to read exception messages.  This completes our discussion of database integrity rules. Next, we look at some optimization you might want to include in your schema for performance reasons.  Indexes are a key feature when optimi\u00aezing the performance of a database application. The query optimiz\u00aeer in a DBMS can use indexes to avoid excessive scans of the data tables. Because they\u2019re relevant only in the physical implementation of a database, indexes aren\u2019t part of the SQL standard, and the DDL and available indexing options are product specific. You can, however, embed the most common schema artifacts for typical indexes in mapping metadata A user can only make bids until an auction ends. Your database should guarantee that invalid bids can\u2019t be stored so that whenever a row is inserted into the BID table, the CREATEDON timestamp of the bid is checked against the auction ending time. This kind of constraint involves two tables: BID and ITEM.  You can create a rule that spans several tables with a join in a sub select in any SQL CHECK expression. Instead of referring only to the table on which the constraint is declared, you may query (usually for the existence or nonexistence of a particular piece of information) a different table. The problem is t\u00aehat you can\u2019t use the @org.hibernate.annotations.Check annotation on either the Bid or Item class. You don\u2019t know which table Hibernate will create first.  Therefore, put your CHECK constraint into an ALTER TABLE statement that executes after all the tables have been created. A good place is the load script, because it always executes at that time A row in the BID table is now valid if its CREATEDON value is less than or equal to the auction end time of the referenced ITEM row.  By far the most common rules that span several tables are referential integrity rules. They\u2019re widely known as foreign keys, which are a combination of two things: a key valu\u00aee copy from a related row and a constraint that guarantees that the referenced value exists. Hibernate creates foreign key constraints automatically for all foreign key columns in association mappings. If you check the schema produced by Hibernate, you\u2019ll notice that these constraints also have automatically generated database identifiers names that aren\u2019t easy to read and that make debugging more difficult. You see this kind of statement in the generated schema",
    "page679": "If you encounter a USERS table in a legacy schema, it\u2019s likely that USERNAME is the primary key. In this case, you have no surrogate identifier that Hibernate generates automatically. Instead, your application has to assign the identifier value when saving an instance of the User class Hibernate calls the protected no-argument constructor when it loads a User from the database and then assigns the username field value directly. When you instantiate a User, call the public constructor with a username. If you don\u2019t declare an identifier generator on the @Id property, Hibernate expects the application to take care of the primary key value assignment.  Composite (natural) primary keys require a bit more work Suppose the primary key of the USERS table is a composite of the two columns USERNAME and DEPARTMENTNR. You write a separate composite identifier class that declares just the key properties a\u00aend call this class User Id This class has to be @Embeddable and Serializable any type used as an identifier type in JPA must be Serializable. C You don\u2019t have to mark the properties of the composite key as @NotNull; their database columns are automatically NOT NULL when embedded as the primary key of an entity. D The JPA specification requires a public no-argument constructor for an embeddable identifier class. Hibernate accepts protected visibility. E The only public constructor should have\u00ae the key values as arguments. F You have to override the equals() and hashCode() methods with the same semantics the composite key has in your database. In this case, this is a straightforward comparison of the username and department values. Many queries in Caveat Emptor will probably involve the username of a User entity. You can speed up these queries by creating an index for the column of this property. Another candidate for an index is the combination of USERNAME and EMAIL columns, which you also use frequently in queries. You can declare single or multicolumn indexes on the entity class with the @Table annotation and its indexes attribute If you don\u2019t provide a name for the index, a generated name is used.\u00ae  We don\u2019t recommend adding indexes to your schema ad hoc because it feels like an index could help with a performance problem. Get the excellent book SQL Tuning by Dan Tow (Tow, 2003) if you want to learn efficient database-optimization techniques and especially how indexes can get you closer to the best-performing execution plan for your queries.  Customizing the database schema is often possible only if you\u2019re working on a new system with no existing data. If you have to deal with an existing legacy schema, one of the most common issues is working with natural and composite keys. We mentioned in section 4.2.3 that we think natural primary keys can be a bad idea. Natural keys often make it difficult to change the data model when business requirements change. They may even, in extreme cases, impact performance. Unfortunately, many legacy schemas use (natural) composite keys heavily; and for the reason we discourage the use of composite keys, it may be difficult to change the legacy schema to use non-composite natural or surrogate keys. Therefore, JPA supports natural and composite primary and foreign keys.",
    "page680": "We\u2019ve already shown the @SecondaryTable annotation in an inheritance mapping in section 6.5. It helped to break out properties of a particular subclass into a separate table. This generic functionality has more uses but be aware that a properly designed system should have, simplified, more classes than tables.  Suppose that in a legacy schema, you aren\u2019t keeping a user\u2019s billing address information with the other user details in the USERS main entity table, but in a separate table. Figure 9.5 shows this schema. The user\u2019s home address is stored in the columns STREET, ZIPCODE, and CITY of the USERS table. The user\u2019s billing address is stored in the BILLING_ADDRESS table, which has the primary key column USER_ID, which is also a foreign key constraint referencing the ID primary key of the USERS table  The User class has two properties of embedded type: home Address and billing Address. The first is a regular embedded mapping, and the Address class is @Embeddable.  Just as in section 5.2.3, you can use the @AttributeOverrides annotation to override the mapping of embedded pr\u00aeoperties. Then, @Column maps the individual properties to the BILLING_ADDRESS table, with its table option. Remember that an @AttributeOverride replaces all mapping information for a property: any annotations on the Address fields are ignored if you override. Therefore, you have to specify nullability and length again in the @Column override.  We\u2019ve shown you a secondary table mapping example with an embeddable property. Of course, you could also break out simple basic properties like the username string in a secondary table. Keep in mind that reading and maintaining these mappings can be a problem, though; you should only map legacy unchangeable schemas with secondary tables. A foreign key constraint on the SELLER column in the ITEM table ensures that the seller of the item exists by requiring the same \u00aeseller value to be present on\u00ae some column in some row on some table. There are no other rules; the target column doesn\u2019t need a primary key constraint or even a unique const\u00aeraint. The target table can be any table. The value can be a numeric identifier of the seller or a customer number string; only the type has to be the same for the foreign key reference source and target.  Of course, a foreign key constraint usually references p\u00aerimary key column(s). Nevertheless, legacy databases sometimes have foreign key constraints that don\u2019t follow this simple rule. Sometimes a foreign key constraint references a simple unique column a natural non-primary key. Let\u2019s assume that in Caveat Emptor, So far, this is nothing special; you\u2019ve seen such a simple unique property mapping before. The legacy aspect is the SELLER_CUSTOMERN\u00aeR column in the ITEM table, with a foreign key constraint referencing the user\u2019s CUSTOMERNR instead of the user\u2019s ID You specify the referencedColumnName attribute of @JoinColumn to declare this relationship. Hibernate now knows that the referenced target column is a natural key, and not the primary key, and manages the foreign key relationship accordingly.  If the target natural key is a composite key, use @JoinColumns instead as in the previous section. Fortunately, it\u2019s often straightforward to clean up such a schema by refactoring foreign keys to reference primary keys if you can make changes to the database that don\u2019t disturb other applications sharing the data.  This completes our discussion of natural, composite, and foreign key\u2013related problems you may have to deal with when you try to map a legacy schema. Let\u2019s move on to another interesting special strategy",
    "page681": "strategies fo\u00aer runtime data management. These strategies are crucial to the performance and correct behavior of your applications.  In this chapter, we discuss the life cycle of entity instances how an instance becomes persistent, and how it stops being considered persistent and the method calls and management operations that trigger these transitions. The JPA Entity Manager is your primary interface for accessing data.  Before we look at the API, let\u2019s start with entity instances, their life cycle, and the events that trigger a change of sta\u00aete. Although some of the material may be formal, a solid understanding of the persistence life cycle is essential. Because JPA is a transparent persistence mechanism classes are unaware of their own persistence capability it\u2019s possible to write application logic that\u2019s unaware whether the data it operates on represents persistent state or temporary state that exists only in memory. The application shouldn\u2019t necessarily need to care that an instance is\u00ae persistent when invoking its methods. You can, for example, invoke the Item #calculateTotalPrice() business method without having to consider persistence at all (for example, in a unit test).  Any application with persistent state must interact with the persistence service whenever it needs to propagate state held in memory to the database (or vice versa). In other words, you have to call the Java Persistence\u00ae interfaces to store and load data.  When interacting with the persistence mechanism that way, the application must concern itself with the state and life cycle of an entity instance with respect to persistence. We refer to this as the persistence life cycle: the states an entity instance goes through during its life. We also use the term unit of work: a set of (possibly) state changing operations considered one (usually atomic) group. Another piece of the puzzle is the persistence context provided by the persistence service. In part 3, you\u2019ll load and store data with Hibernate and Java Persistence. You\u2019ll introduce the progra\u00aemming interfaces, how to write transactional applications, and how Hibernate can load data from the database most efficiently.  Starting with chapter 10, you\u2019ll learn the most important strategies for interacting with entity instances in a JPA application. You\u2019ll see the life cycle of entity instances: how they become persistent, detached, and removed. This chapter is where you\u2019ll get to know the most i\u00aemportant interface in JPA: the Entity Manager. Next, chapte\u00aer 11 defines database and system transaction essentials and how to control concurrent access with Hibernate and JPA. You\u2019ll also see non transactional data access. In chapter 12, we\u2019ll go through lazy and eager loading, fetch plans, strategies, and profiles, and wrap up with optimizing SQL execution. Finally, chapter 13 covers cascading state transitions, listening to and intercepting events, auditing and versioning with Hibernate Envers, and filtering data\u00ae dynamically.  After reading this part, you\u2019ll know how to work with Hibernate and Java Persistence programming interfaces and how to load, modify, and store objects efficiently. You\u2019ll under\u00aestand how transactions work and why conversational processing can open up new approaches for application design. You\u2019ll be ready to optimize any object-modification scenario and apply the best fetching and caching strategy to increase performance and scalability You now understand how Hibernate and ORM solve the static aspects of the object/relational mismatch. With what you know so far, you can create a mapping between Java classes and an SQL schema, solving the structural mismatch problem. For a reminder of the pr\u00aeoblems you\u2019re solving, see section 1.2.  An efficient application solution requires something more: you must investigate",
    "page682": "A persistent entity instance has a representation in the database. It\u2019s stored in the database or it will be stored when the unit of work completes. It\u2019s an instance with a database identity, as defined in section 4.2; its database identifier is set to the primary key value of the database representation.  The application may have created instances and then made them persistent by calling Entity Manager #persist(). There may be instances that became persistent when  the application created a reference to the object from another persistent instance that the JPA provider already manages. A persistent entity instance may be an instance retrieved from the database by execution of a query, by an identifier lookup, or by navigating the object graph starting from another persistent instance.  Persistent instances are always associated with a persistence context. You\u00ae see more about this in a moment You can delete a persistent entity instance from the database in several ways: For example, you can remove it with Entity Manager #remove(). It may also become available for deletion if you remove a reference to it from a mapped collection with orphan removal enabled.  An entity instance is then in the removed state: the provider will delete it at the end of a unit of work. You should discard any references you may hold to it in the application \u00aeafter you finish working with it for example, after you\u2019ve rendered the removal confirmation screen your users see Think of the persistence context as a service that remembers all the modifications and state changes you made to data in a particular unit of work (this is somewhat simplified, but it\u2019s a good starting point).  We now dissect all these terms: entity states, persistence contexts, and managed scope. You\u2019re probably more accustomed to thinking about what SQL statements you ha\u00aeve to manage to get stuff in and out of the database; but one of the key factors of your success with Java Persistence is your understanding of state management, so stick with us through this section. Different ORM solutions use different terminology and define different states and state transitions for the persistence life cycle. Moreover, the states used internally may be different from those exposed to the client application. JPA defines four states, hiding the complexity of Hibernates internal implementation from the client code. Figure 10.1 shows these states and their transitions. The state chart also includes the method calls to the Entity Manager (and Query) API that trigger transitions. We discuss this chart in this chapter; refer to it whenever you need an overview. Let\u2019s explore the states and transitions in more detail. Instances created with the new Java operator are transient, which means their state is lost and garbage-coll\u00aeected as soon as they\u2019re no longer referenced. For example, new Item() creates a transient instance of the Item class, just like new Long() and new Big Decimal(). Hibernate doesn\u2019t provide any rollback functionality for transient instances; if you modify the price of a transient Item, you can\u2019t automatically undo the change.  For an entity instance to transition from transient to persistent state, to become managed, requires either a call to the Entity Manager #persist() method or the creation of a reference from an already-persistent instance and enabled cascading of ",
    "page683": "The persistence context acts as a first-level cache; it remembers all entity instances y\u00aeou\u2019ve handled in a particular unit of work. For example, if you ask Hibernate to load an entity instance using a primary key value (a lookup by identifier), Hibernate can first check the current unit of work in the persistence context. If Hibernate finds the entity instance in the persistence context, no database hit occurs this is a repeatable read for an application. Consecutive em.find(Item. Class, ITEM_ID) calls with the same persistence context will yield the same result.  This cache also affects results of arbitrary queries, executed for example with the javax.persistence.Query API. Hibernate reads the SQL result set of a query and transforms it into entity instances. This process first tries to resolve every entity instance in the persistence context by identifier lookup. Only if an instance with the same identifier value can\u2019t be found in the current persistence context does Hibernate read the rest of the data from the result-set row. Hibernate ignores any potentially newer data in the result set, due to read-committed transaction isolation at the database level, if the entity instance is already present in the persistence context.  The persistence context cache is always on it can\u2019t be turned off. It ensures the following The persistence layer isn\u2019t vulnerable to stack overflows in the case of circular references in an object graph. There can never be conflicting representations of the same database row at the end of a unit of work. The provider can safely write all changes made to an entity instance to the database.  Likewise, changes made in a particular persistence context are always immediately v\u00aeisible to all other code executed inside that unit of work and its persistence context. JPA guarantees repeatable entity-instance reads To understand detached entity instances, consider loading an instance. You call Entity Manager #find() to retrieve an entity instance by its (known) identifier. Then you end your unit of work and close the persistence context. The application still has a handle a reference to the instance you loaded. It\u2019s now in the detached state, and the data is becoming stale. You could discard the reference and let the garbage collector reclaim the memory. Or, you could continue working with the data in the detached state and later call the merge() method to save your modifications in a new unit of work. We\u2019ll discuss detachment and merging again later in this chapter, in a dedicated section.  You should now have a basic understanding of entity instance states and their transitions. Our next topic is the persistence context: an essential service of any Java Persistence provider The persistence context allows the persistence engine to perform automatic dirty checking, detecting which entity instances the application modified. The provider then synchronizes with the database the state of instances monitored by a persistence context, either automatically or on demand. Typically, when a unit of work completes, the provider propagates state held in memory to the database through the execution of SQL INSERT, UPDATE, and DELETE statements (all part of the Data Modification Language [DML]) This flushing procedure may also occur at other times. For example, Hibernate may synchronize with the database before executio\u00aen of a query. This ensures that queries are aware of changes made earlier during the unit of work.",
    "page684": "Hibernate joins the in-progress system transaction when an Entity Manager is created and waits for \u00aethe transaction to commit. When Hibernate is notified (by the JTA engine) of the commit, it performs dirty checking of the persistence context and synchronizes with the database. You can also force dirty checking synchronization manually by calling EntityManager#flush() at any time during a transaction.  You decide the scope of the persistence context by choosing when to close() the Entity Manager. You have to close the persistence context at some point, so always place the close() call in a finally block.  How long should the persistence context be open? Let\u2019s assume for the following examples that you\u2019re writing a server, and each client request will be processed with one persistence context a\u00aend system transaction in a multithreaded environment. If you\u2019re familiar with servlets, imagine the code in listing 10.1 embedded in a servlet\u2019s service() method. Within this unit of work, you access the Entity Mana\u00aeger to load and store data. A new transient Item is instantiated as usual. O\u00aef course, you may also instantiate it before creating the EntityManager. A call to persist() makes the transient instance of Item persistent. It\u2019s now managed by and associated with the current persistence context.  To store the Item instance in the database, Hibernate has to execute an SQL INSERT statement. When the transaction of this unit of work commits, Hibernate flushes the persistence context, and the INSERT occurs at that time. Hibernate may even batch the INSERT at the JDBC level with other statements. When you call persist(), only the identifier value of the Item is assigned. Alternatively, if your identifier generator isn\u2019t pre-insert, the INSERT statement will be executed immediately when persist() is called. You may want to review section In Java SE and some EE architectures (if you only have plain s\u00aeervlets, for example), you get an Entity Manager by calling Entity Manager Factory #createEntityManager(). Your application code shares the Entity Manager Factory, representing one persistence unit, or one logical database. Most applications have only one shared  (The TM class is a convenience clas\u00aes bundled with the example code of this book. Here it simplifies the lookup of the standard User Transaction API in JNDI. The JPA class provides convenient access to the shared Entity Manager Factory.)  Everything between tx. Begin() and tx.commit() occurs in one transaction. For now, keep in mind that all database operat\u00aeions in transaction scope, such as the SQL statements executed by Hibernate, completely either succeed or fail. Don\u2019t worry too much about the transaction code for now; you\u2019ll read more about concurrency control in the next chapter. We\u2019ll look at the same examp\u00aele again with a focus on the transaction and exception-handling code. Don\u2019t write empty catch clauses in your code, though you\u2019ll have to roll back the transaction and handle exceptions.  Creating an Entity Manager starts its persistence context. Hibernate won\u2019t access the database until necessary; the Entity Manager doesn\u2019t obtain a JDBC Connection from the pool until SQL statements have to be executed. You can create and close an Entity Manager without hitting the database. Hibernate executes\u00ae SQL statements when you look up or query data and when it flushes changes detected by the persistence context to the database. Hibernate joins the in-progress system transaction when an Entity Manager is created and waits for the transaction to commit. When Hibernate is notified (by the JTA engine) of the commit, it performs dirty checking of the persistence context and synchronizes with the database.",
    "page685": "You can mod\u00aeify the Item instance, and the persistence context will detect th\u00aeese changes and record them in the database automatically. When Hibernate flushes the persistence context during commit, it executes the necessary SQL DML statements to synchronize the changes with the database. Hibernate propagates state changes to the database as late as possible, toward the end of the transaction. DML statements usually create locks in the database that are held until the transaction completes, so Hibernate keeps the lock duration in the database as short as possible.  Hibernate writes the new Item#name to the database with an SQL UPDATE. By default, Hibernate includes all columns of the mapped ITEM table in the SQL UPDATE statement, up\u00aedating unchanged columns to their old values. Hence, Hibernate can generate these basic SQL statements at startup, not at runtime. If you want to include only modified (or non-nullable for INSERT) columns in SQL statements, you can enable dynamic SQL generation as discussed in section 4.3.2.  Hibernate detects the changed name by comparing the Item with a snapshot copy it took before, when the Item was loaded from the database. If your Item is different from the snapshot, an UPDATE is necessary. This snapshot in the p\u00aeersistence context consumes memory. Dirty checking with snapshots can also be time consuming, because Hibernate has to compare all instances in the persistence context with their snapshot during flushing. It\u2019s better (but not required) to fully initialize the Item instance before managing it with a persistence context. The SQL INSERT statement contains the values that were held by the instance at the point when persist() was called. If you don\u2019t set the name of the Item before making it persistent, a NOT NULL constraint may be violated. You can modify the Item after calling persist(), and your changes will be propagated to the database with an additional SQL UPDATE statement.  If one of the INSERT or UPDATE statements made when flushing fails, Hibernate causes a rollback of changes made to persistent instances in this transaction at the database level. But Hibernate doesn\u2019t roll back in-memory changes to persistent instances. If you change the Item name after persist(), a commit failure won\u2019t roll back to the old name. This is reasonable because a failure of a transaction is normally non-recoverable, and you have to discard the failed persistence context and Entity Manager immediately. We\u2019ll discuss exception handling in the next chapter.  Next, you load and modify the stored data. You can retrieve persistent instances from the database with the EntityManager. For the next example, we assume you\u2019ve kept the identifier value of the Item stored in the previous section somewhere and are now looking up the same instance in a new unit of work by identifier  You don\u2019t need to cast the returned value of the find() operation; it\u2019s a generic method, and its return type i\u00ae\u00aes set as a side effect of the first parameter. The retrieved entity instance is in persistent state, and you can now modify it inside the unit of work.  If no persistent instance with the given identifier value can be found, fi\u00aend() returns null. The find() operation always hits the database if there was no hit for the",
    "page686": "As soon as you call any method such as Item#getName() on the proxy, a SELECT is executed to fully initialize the placeholder. The exception to this rule is a mapped database identifier getter method, such as getId(). A proxy may look like the real thing, but it\u2019s only a placeholder carrying the identifier value of the entity instance it represents. If the database record no longer exists when the proxy is initialized, an EntityNotFoundException is thrown. Note that the exception can be thrown when Item\u00ae#getName() is called. E Hibernate has a convenient static initialize() method that loads the proxy\u2019s data. F After the persistence context is closed, item is in detached state. If you don\u2019t initialize the proxy while the persistence context is still open, you get a LazyInitializationException if you access the proxy. You can\u2019t load data on demand once the persistence context is closed. The solution is simple: load the data before you close the persistence context.  We\u2019ll have much more to say about proxies, lazy loading, and on-demand fetching in ch\u00aeapter 12.  If you want to remove the state of an entity instance from the database, you have to make it transient. To make an entity instance transient and delete its database representation, call the remove() method on the EntityManager If you call find(), Hibernate executes a SELECT to load the Item. If you call getReference(), Hibernate attempts to avoid the SELECT and returns a proxy. C Calling remove() queues the entity instance for deletion when the unit of work completes; it\u2019s now in removed state. If remove() is called on a proxy, Hibernate executes a SELECT to load the data. You may want to customize how Hibernate detects dirty state, using an extension point. Set the property hibernate.entity_dirtiness_strategy in your persistence.xml configuration file to a class name that implements org.hibernate.CustomEntityDirtinessStrategy. See the Javadoc of this interface for more information. org.hibernate.Interceptor is another extension point used to customize dirty checking, by implementing its findDirty() method. You can find an example interceptor in section 13.2.2.  We mentioned earlier that the persistence context enables repeatable reads of entity instances and provides an object-identity guarantee: The first find() operation hits the database and retrieves the Item instance with a SELECT statement. The second find() is resolved in the persistence context, and the same cached Item instance is returned.  Sometimes you need an entity instance but you don\u2019t want to hit the database.  If you don\u2019t want to hit the database when loading an entity instance, because you aren\u2019t sure you need a fully initialized instance, you can tell the EntityManager to attempt the retrieval of a hollow placeholder a proxy: If the persistence context already contains an Item with the given identifier, that Item instance is returned by getReference() without hitting the databa\u00aese. Furthermore, if no persistent instance with that identifier is currently managed, H\u00aeibernate produces a hollow placeholder: a proxy. This means getReference() won\u2019t access the database, and it doesn\u2019t return null, unlike find(). C JPA offers PersistenceUnitUtil helper methods such as isLoaded() to detect whether you\u2019re working with an uninitialized proxy.",
    "page687": "Java Persistence also offers bulk operations that translate into direct SQL \u00aeDELETE statements without life cycle interceptors in the application. We\u2019ll discuss these operations in section 20.1.  Let\u2019s say you load an entity instance from the database and work with the data. For some reason, you know t\u00aehat another application or maybe another thread of your application has updated the underlying row in the database. Next, we\u2019ll see how to refresh the data held in memory. After you load the entity instance, you realize (how isn\u2019t important) that someone else changed the data in the database. Calling refresh() causes Hibernate to execute a SELECT to read and marshal a whole result set, overwriting changes you already made to the persistent instance in application memory. If the database row no longer exists (someone deleted it), Hibernate throws an EntityNotFoundException on refresh().  Most applications don\u2019t have to manually refresh in-memory state; concurrent modificati\u00aeons are typically resolved at transaction commit time.  The best use case for refreshing is with an extended persistence context, which might span sever\u00aeal request/response cycles and/or system transactions. While you wait for user input with an open persistence context, data gets stale, and selective refreshing may be required depending on the duration of the co\u00aenversation and the dialogue between the user and the system. Refreshing can be useful to undo changes made in memory during a conversation, if the user cancels the dialogue. We\u2019ll have more to say about refreshing in a conversation in section. An entity instance must be fully initialized during life cycle transitions. You may have life cycle callback methods or an entity listener enabled (see section 13.2), and the instance must pass through these interceptors to complete its full life cycle. D An entity in removed state is no longer in persistent state. You can check this with the contains() operation. E You can make the removed instance persistent again, cancelling the deletion. F When the transaction commits, Hibernate synchronizes the state transitions with the database and executes the SQL DELETE. The JVM garbage collector detects that the item is no longer referenced by anyone and finally deletes the last trace of the dat\u00aea. By default, Hibernate won\u2019t alter the identifier value of a removed entity instance. This means the item.getId() method still returns the now outdated identifier value. Sometimes it\u2019s useful to work with the \u201cdeleted\u201d data further: for example, you might want to save the removed Item again if your user decides to undo. As shown in the example, you can call persist() on a removed instance to cancel the deletion before the persistence context is flushed. Alternatively, if you set the property hibernate.use_ identifier_ rollback to true in persistence.xml, Hibernate will reset the identifier value after removal of an entity instance. In the previous code example, the identifier value is reset to the default value of null (it\u2019s a Long). The Item is now the same as in\u00ae transient state, and you can save it again in a new persistence context.",
    "page688": "The persistence context is a cache of persistent instances. Every entity instance in persistent state is associated with the persistence context.  Many Hibernate users who ignore this simple fact run into an OutOfMemoryException. This is typically the case when you load thousands of entity instances in a unit of work but never intend to modify them. Hibernate still has to create a snapshot of each instance in the persistence context cache, which can lead to memory exhaustion. (Obviously, you should execute a bulk data operation if you modify thousands of rows we\u2019ll get back to this kind of unit of work in section 20.1.)  The persistence context cache never shrinks automatically. Keep the size of your persistence context to the necessary minimum. Often, many persistent instances in your context are there by accident for example, because you needed only a few items but queried for many. Extremely large graphs can have a serious performance impact and require significant memory for state snapshots. Check that your queries return only data you need, and consider the following ways to control Hibernate\u2019s caching behavior.  You can call EntityManager#detach(i) to evict a persistent instance manually from the persistence context. You can call EntityManager#clear() to detach all persistent entity instances, leaving you with an empty persistence context.  The native Session API has some extra operations you might find useful. You can set the entire persistence context to read-only mode. This disables state snapshots and dirty checking, and Hibernate won\u2019t write modifications to the database. Replication is useful, for example, when you ne\u00aeed to retrieve data from one database and store it in another. Replication takes detached instances loaded in one persistence context and makes them persistent in another persistence context. You usually open these contexts from two different EntityManagerFactory configurations, enabling two logical databases. You have to map the entity in both configurations.  The replicate() operation is only available on the Hibern\u00aeate Session API. Here is an example that loads an Item instance from one database and copies it into another Connections to both databases can participate in the same system transaction. ReplicationMode controls the details of the replication procedure: IGNORE Ignores the instance when there is an existing database row with the same identifier in the database. OVERWRITE Overwrites any existing database row with the same identifier in the database. EXCEPTION Throws an exception if there is an existing database row with the same identifier in the target database. LATEST_VERSION Overwrites the row in the database if its version is older than the version of the given entity instance, or ignores the instance otherwise. Requires enabled optimistic concurrency contro\u00ael with entity versioning You may need replication when you reconcile data entered into different databases. An example case is a product upgrade: if the new version of your application requires a new database (schema), you may want to mig\u00aerate and replicate the existing data once.  The persistence context does many things for you: automatic dirty checking, guaranteed scope of object identity, and so on. It\u2019s equally impo\u00aertant that you know some of the details of its management, and that you sometimes \u00aeinfluence what goes on behind the scenes. ",
    "page689": "Next, we look at the detached entity state. We already mentioned some issues you\u2019ll see when entity instances aren\u2019t associated with a persistence context anymore, such as disabled lazy initialization. Let\u2019s explore the detached state with some examples, so you know what to expect when you work with data outside of a persistence context.  If a reference leaves the scope of guaranteed identity, we call it a reference to a detached entity instance. When the persistence context is closed, it no longer provides an iden\u00aetity-mapping service. You\u2019ll run into aliasing problems when you work with detached entity instances, so make sure you understand how to handle the identity of detached instances. If you look up data using the same database identifier value in the same persistence context, the result is two references to the same in-memory instance on the JVM heap. Consider the two units of work shown next. In the first unit of work at begin() B, you start by creating a persistence context C and loading some entity instances. Because references a and b are obtained f\u00aerom the same persistence context, they have the same Java identity D. They're equal from the same persistence context, they have the same Java identity D. They're equa\u00ael E because by default equals() relies \u00aeon Java identity comparison. They obviously have the same database identity F. They reference the same Item instance, in persistent state, managed by the persistence context fo\u00aer that unit of work. The first part of this example finishes by committing the transaction. By default, Hibernate flushes the persistence context of an EntityManager and synchronizes changes with the database whenever the joined transaction is committed. All the previous code examples, except some in the \u00aelast section, have used that strategy. JPA allows implementations to synchronize the persistence context at other times, if the\u00aey wish.  Hibernate, as a JPA implementation, synchronizes at the following times:  When a joined JTA system transaction is committed  Before a query is executed we don\u2019t mean lookup with find() but a query with javax.persistence.Query or the similar Hibernate API  When the application calls flush() explicitly Here, you load an Item instance B and change its name C. Then you query the database, retrieving the item\u2019s name D. Usually Hibernate recognizes that data has changed in \u00aememory and synchronizes these modificatio\u00aens with the database before the query. This is the behavior of FlushModeType.AUTO, the default if you join the EntityManager with a transaction. With FlushModeType.COMMIT, you\u2019re disabling flushing before queries, so you may see different data returned by the query than what you have in memory. The synchronization then occurs only when the transaction commits.  You can at any time, while a transaction is in progress, force dirty checking and synchronization with the database by calling EntityManager#flush(). This concludes our discussion of the transient, persistent, and removed entity states, and the ba\u00aesic usage of the EntityManager API. Mastering these state transitions and API methods is essential; every JPA application is built with these operations.",
    "page690": "When you begin a journey, it\u2019s a good idea to have a mental map of\u00ae the terrain you\u2019ll be passing through. The same is true for an intellectual journey, such as learning to write computer programs. In this case, you\u2019ll need to know the basics of what computers are and how they work. You\u2019ll want to have some idea of what a computer program is and how one is created. Since you will be writing programs in the Java programming language, you\u2019ll want to know something about that language in particular and about the modern, networked computing environment for which Java is designed. As you read this chapter, don\u2019t worry if you can\u2019t understand everything in detail. (In fact, it would be impossible for you to learn all the details from the brief expositions in this chapter.) Concentrate on learning enough about the big ideas to orient yourself, in preparation for the rest of the book. Most of what is covered in this chapter will be covered in much greater detail later in the book. A computer is a complex system consisting of many different components. But at the heart or the brain, if you want of the computer is a single component that does the actual computing. This is the Central Processing Unit, or CPU. In a modern desktop computer, the CPU is a single \u201cchip\u201d on the order of one square inch in size. The job of the CPU is to execute programs. A program is simply a list of unambiguous in\u00aestructions meant to be followed mechanically by a computer. A computer is built to carry out instructions that are written in a very simple type of language called machine language. Each type of computer has its own machine language, and the computer can directly execute a program only if the program is expressed in that l\u00aeanguage. (It can execute programs written in other languages if they are first translated into machine language.) When the CPU executes a program, that program is stored in the computer\u2019s main memory (also called the RAM or random access memory). In addition to the program, memory can also hold data that is being used or processed by the program. Main memory consists of a sequence of locations. These locations are numbered, and the sequence number of a location is called its address. An address provides a way of picking out one particular piece of information from among the millions stored in memory. When the CPU needs to access the program instruction or data in a particular location, it sends the address of that information as a signal to the memory; the memory responds by sending back the data contained in the specifie location. The CPU can also store information in memory by specifying the information to be stored and the address of the location where it is to be stored. On the level of machine language, the operation of the CPU is fairly straightforward (although it is very complicated in detail). The CPU executes a program that is stored as a sequence of machine lang\u00aeuage instructions in main memory. It does this by repeatedly reading, or fetching, an instruction from memory and then carrying out, or executing, that instruction. This process fetch an instruction, execute it, fetch another instruction, execute it, and so on forever is called th\u00aee fetch-and-execute cycle. With one exception, which will be covered in the next section, this is all that the CPU ever does. The details of the fetch-and-exec\u00aeute cycle are not terribly important, but there are a few basic things you should know. The CPU contains a few internal registers, which are small memory units capable of holding a single number or machine language instruction. The CPU uses one of these registers the program counter, or PC to keep track of\u00ae where it is in the program it is executing. The PC stores the address of the next instruction that the CPU should execute. At the beginning of each fetch-and-execute cycle, the CPU checks the PC to see which instruction it should fetch. During the course of the fetch-and-execute cycle, the number in the PC is updated to indicate the instruction \u00aethat is to be executed in the next cycle. A computer executes machine language programs mechanically that is without understanding them or thinking about them simply because of the way it is physically put together. This is not an easy concept. A computer is a machine built of millions of tiny \u00aeswitches called transistors, which have the property that they can be wired together in such a way that an output from one switch can turn another switch on or off. As a computer computes, these switches turn each other on or off in a pattern determined both by the way they are wired together and by the program that the computer is executing.",
    "page691": "Machine language instructions are expressed as binary numbers. A binary number is made up of just two possible digits, zero and one. So, a machine language instruction is just a sequence of zeros and ones. Each particular sequence encodes some particular instruction. The data that the computer manipulates is also encoded as binary numbers. A computer can work directly with binary numbers because switches can readily represent such numbers: Turn the switch on to represent a one; turn it off to represent a zero. Machine language instructions are stored in memory as patterns of switches turned on or off, When a machine language instruction is loaded into CPU, all that happens is that certain switches are turned on or off in the pattern that encodes that particular instruction. The CPU is built to respond to this pattern by executing the instruction it encodes; it does this simply because of the way all the other switches in the CPU are wired together. So, you should understand this much about how computers work: Main memory holds machine language programs and data. These are encoded as binary numbers. The CPU fetches machine language instructions from memory \u00aeone after another and executes them. It does this mechanically, without thinking about or understanding what it does-and therefore the program it executes must be perfect, complete in all details, and unambiguous because the CPU can do nothing but execute it exactly as written. Here is a schematic view of this first-stage understanding of the computer Asynchronous Events: Polling Loops and Interrupts The CPU spends almost all of its time fetching instructions from memory and executing them. However, the CPU and main memory are only two out of many components in areal computer system, A complete system contains other devices such as: A hard disk for storing programs and data files.(Note that main memory holds only a comparatively small amount of information, and holds it only as long as the power is turned on. A hard disk is necessary for permanent storage of larger amounts of information, but programs have to be loaded from disk into main memory before they can actually be executed.) A keyboard and mouse for user input. A monitor and printer which can be used to display the computer's output. A modem that allows the computer to communicate with other computers over telephone lines. A network interface that allows the computer to communicate with other computers that are connected to it on a network. A scanner that converts images into coded binary numbers that can be stored and manipulated on the computer. The list of devices is entirely open ended, and computer systems are built so that they can easily be expanded by adding new devices. Somehow the CPU has to communicate with and control all these devices. The CPU can only do this by executing machine language instructions (which is all it can do, period). The way this works is that for each device in a system, there is a device driver, which consists of software that the CPU executes when it has to deal with the device. Installing a new device on a system generally has two steps: plugging the device physically into the computer, and installing the device driver software. Without the device driver, the actual physical device would be useless, since the CPU would not be able to communicate with it. A computer system consisting of many devices is typically organized by connecting those devices to one or more busses. A bus is a set of wires that carry various sorts of information between the devices connected to \u00aethose wires. The wires carry data, addresses, and controls signals. An address directs the data to a particular device and perhaps to a particular register or location within that device. Control signals can be used, for example, by one device to alert another that data is available for it on the data bus. A fairly simple computer system might be organized like this Now, devices such as keyboard, mouse, and network interface can produce input that ne\u00aeeds to be processed by the CPU. How does the CPU know that the data is there? One simple idea, which turns out to be nit very satisfactory, is for the CPU to keep checking for incoming data over and over. Whenever it finds data, it processes it. This method is called polling, since the CPU polls the input devices continually to see whether they have any input data to report. Un\u00aefortunately, although polling is very simple, it is also very inefficient. The CPU can waste an awful lot of time just waiting for input. To avoid this inefficiency, interrupts are often us\u00aeed instead of polling. An interrupt is a signal sent by another device to the CPU. The CPU responds to an interrupt signal by putting aside whatever it is doing in order to respond to the interrupt. Once it has handled the interrupt, it returns to what it was doing before th\u00aee interrupts occurred. For example, when you press a key on your computer keyboard, a keyboard interrupt is sent to the CPU. The CPU responds to this signal by inter\u00aerupting what it is doing, reading the key \u00aethat you pressed, processing it, and returning to the task it was performing before you pressed the key.",
    "page692": "Again, you should understand that this is a purely mechanical process: A device signals an interrupt simply by turning on a wire. The CPU is built so that when that wire is turned on, the CPU saves enough information about what it is currently doing so that it can return to the same state later. This information consists of the contents of important internal registers such as the program counter. Then the CPU jumps to some predetermined memory location and begins executing the instructions stored there. Those instructions make up an interrupt handler that does the processing necessary to respond to the interrupt. (This interrupt handler is part of the device driver software for the device that signalled the interrupt.) At the end of the interrupt handler is an instruction that tells the CPU to jump back to what\u00ae it was doing; it does that by restoring its previously saved state. Interrupts allow the CPU to deal with asynchronous events. In the regular fetch-and execute cycle, things happen in a predetermined order; everything that happens is \u201csynchronized\u201d with everything else. Interrupts make it possible for the CPU to deal efficiently with events that happen \u201casynchronously,\u201d that is, at unpredictable times. As another example of how interrupts are used, consider what happens when the CPU need to access data that is stored on the hard disk. The CPU can access data directly only if it is in main memory. Data on the disk has to be copied into memory before it can be accessed. Unfortunately, on the scale of speed at which the CPU operates, the disk drive is extremely slow. W\u00aehen the CPU needs data from the disk, it sends a signal to the disk drive telling it to locate the data and get it ready. (This signal is sent synchronously, under the control of a regular program.) Then, instead of just waiting the long and unpredictalble amount of time that the disk drive will take to do this, the CPU goes on with some other task. When the disk drive has the data ready, it sends an interrupt signal to the CPU. The interrupt handler can then read the requested data. Now, you might have noticed that all this only makes sense if the CPU actually has several tasks to perform. If it has nothing better to do, it might as well spend its time polling for input or waiting for disk drive operations to complete. All modern computers use multitasking to perform several tasks at once. Some computers can be used by several people at once. Since the CPU is so fast, it can quickly switch its attention from one us\u00aeer to another, devoting a fraction of a second to each user in turn. This application of multitasking is called timesharing. But a modern personal computer with just a single user also uses multitasking. For example, the user might be typing a paper while a clock is continuously displaying the time and a file is being downloaded over the network. Each of the individual tasks that the CPU is working on is called a thread. (Or a process; there are technical differences between threads and processes, but they are not important here.) At any given time, only one thread can actually be executed by a CPU. The CPU will continue running the same thread until one of several things happens The thread might voluntarily yield control, to give other threads a chance to run. The thread might have to wait for some asynchronous event to occur. For example, the thread might request some data from the disk drive, or it might wait for the user to press a key. While it is waiting, the thread is said to be blocked, and other threads have a chance to run. When the event occurs, an interrupt will \u201cwake up\u201d the thread so that it can continue running. The thread mi\u00aeght use up its allotted slice of time and be suspended to allow other threads to run. Not all computers can \u201cforcibly\u201d suspend a thread in this way; those that can are said to use preemptive multitasking. To do preemptive multitasking, a computer needs a special timer device that generates an interrupt at regular intervals, such as 100 times per second. When a timer interrupt occurs, the CPU has a chance to switch from one thread to another, whether the thread that is currently running likes it or not. Ordinary users, and indeed ordinary programmers, have no need to deal with interrupts and interrupt handlers. They can concentrate on the different tasks or threads that they want the computer to perform; the details of how the computer manages to get all those tasks done are not important to them. In fact, most users, and many programmers, can ignore threads and multitasking altogether. However, threads have become increasingly important as computers have become more powerful and as they have begun to make more use of multitasking. Indeed, threads are built into the Java programming language as a fundamental programming concept. Just as important in Java and in modern programming in general is the basic concept of asynchronous events. While programmers don\u2019t actually deal with interrupts directly, they do often find themselves writing event handlers, which, like interrupt handlers, are called asynchronously when specified events occur. Such \u201cevent-driven programming\u201d has a very different feel from the more traditional straight-through, synchronous programming. We will begin with the more traditional type of programming, which is still used for programming individual tasks, but we will return to threads and events later in the text. By the way, the software that does all the interrupt handling and the communication with the user and with hardware devices is called the operating system. The operating system is the basic, essential software without which a computer would not be able to function. Other programs, such as word processors and World Wide Web browsers, are dependent upon the operating system. Common operating systems include Linux, DOS, Windows 2000, Windows XP, and the Macintosh OS",
    "page693": "The Java Virtual Machine Machine language consists of very simple instructions that can be executed directly by the CPU of a computer. Almost all programs, though, are written in high-level programming languages such as Java, Pascal, or C++. A program written in a high-level language cannot be run directly on any computer. First, it has to be translated into machine language. This translation can be done by a program called a compiler. A compiler takes a high-level-language program and translates it into an executable machi\u00aene-language program. Once the translation is done, the machine-language program can be run any number of times, but of course it can only be run on one type of computer (since each type of computer has its own individual machine language). If the program is to run on another type of computer it has to be re-translated, using a different compiler, into the appropriate machine language. There is an alternative to compiling a high-level language program. Instead of using a compiler, which translates the program all at once, you can use an interpreter, which translates it instruction-by-instruction, as necessary. An interpreter is a program that acts much like a CPU, with a kind of fetch-and-execute cycle. In order to execute a program, the interpreter runs in a loop in which it repeatedly reads one instruction from the program, decides what is necessary to carry out that instruction, and then performs the appropriate machine-language commands to do so. one use of interpreters is to execute high-level language programs. For example, the programming language Lisp is usually executed by an interpreter rather than a compiler. However, interpreters have another purpose: they can let you use a machine-language program meant for one type of computer on a completely different type of computer. For example, there is a program called \u201cVirtual PC\u201d that runs on Macintosh computers. Virtual PC is an interpreter that executes machine-language programs written for IBM-PC-clone computers. If you run Virtual PC on your Macintosh, you can run any PC program, including programs written for Windows. (Unfortunately, a PC program will run much more slowly than it would on an actual IBM clone. The problem is that Virtual PC executes several Macintosh machine-language instructions for each PC machine-language instruction in the program it is interpreting. Compiled programs are inherently faster than interpreted programs. The designers of Java chose to use a combination of compilation and interpretation. Programs written in Java are compiled into machine language, but it is a machine language for a computer that doesn\u2019t really exist. This so-called \u201cvirtual\u201d computer is known as the Java virtual machine. The machine language for the Java virtual machine is called Java bytecode. There is no reason why Java bytecode could not be used as the machine language of a real computer, rather than a virtual computer. However, one of the main selling points of Java is that it can actually be used on any computer. All that the computer needs is an interpreter for Java bytecode. Such an interpreter simulates the Java virtual machine in the same way that Virtual PC simulates a PC computer. Of course, a different Java bytecode interpreter is needed for each type of computer, but once a computer has a Java bytecode interpreter, it can run any Java bytecode program. And the same Java bytecode program can be run on any computer that has such an interpreter. This is one of the essential features of Java: the same compiled program can be run on many different types of computers. Why, you might wonder, use the intermediate Java bytecode at all? Why not just distribute the original Java program and let each person compile it into the machine language of whatever computer they want to run it on? There are many reasons. First of all, a compiler has to understand Java, a complex high-level language. The compiler is itself\u00ae a complex program. A Java bytecode interpreter, on the other hand, is a fairly small, simple program. This makes it easy to write a bytecode interpreter for a new type of computer; once that is done, that computer can run any compiled Java program. It would be much harder to write a Java compiler for the same computer. Furthermore, many Java programs are meant to be downloaded over a network. This leads to obvious security concerns: you don\u2019t want to download and run a program that will damage your computer or your files. The bytecode interpreter acts as a buffer between you and the program you download. You are really running the interpreter, which runs the downloaded program indirectly. The interpreter can protect you from potentially dangerous actions on the part of that program. i should note that there is no necessary connection between Java and Java bytecode. A program written in Java could certainly be compiled into the machine language of a real computer. And programs written in other languages could be compiled into Java bytecode. However, it is the combination of Java and Java bytecode that is platform-independent, secure, and network compatible while allowing you to program in a modern high-level object-oriented language. I should also note that the really hard part of platform-independence is providing a \u201cGraphical User Interface\u201d with windows, butt\u00aeons, etc. that will work on all the platforms that support Java.",
    "page694": "Programs must be designed. No one can just sit down at the computer and compose a program of any complexity. The discipline called software engineering is concerned with the construction of correct, working, well-written programs. The software engineer tends to use accepted and proven methods for analyzing the problem to be solved and for designing a program to solve that problem. During the 1970s and into the 80s, the primary software engineering methodology was structured programming. The structured programming approach to program design was based on the following advice: To solve a large problem, break the problem into several pieces and work on each piece separately; to solve each piece, treat it as a new problem which can itself be broken down into smaller problems; eventually, you will work your way down to problems that can be solved directly, without further decomposition. This approach is called top-down programming. There is nothing wrong with top-down programming. It is a valuable and often-used approach to problem-solving. However, it is incomplete. For one thing, it deals almost entirely with producing the instructions necessary to solve a problem. But as time went on, people realized that the design of the data structures for a program was as least as important as the design of subroutines and control structures. Top-down programming doesn\u2019t give adequate consideration to the data that the program manipulates. Another problem with strict top-down programming is that it makes it difficult to reuse work done for other projects. By starting with a particular problem and subdividing it into convenient pieces, top-down programming tends to produce a design that is unique to that problem. It is unlikely that you will be able to take a large chunk of programming from another program and fit it into your project, at least not without extensive modification. Producing hig\u00aeh-quality programs is difficult and expensive, so programmers and the people who employ them are always eager to reuse past work. So, in practice, top-down design is often combined with bottom-up design. In bottom-up design, the approach is to start \u201cat the bottom,\u201d with problems that you already know how to solve (and for which you might already have a reusable software component at hand). From there, you can work upwards towards a solution to the overall probl\u00aeem. The reusable components should be as \u201cmodular\u201d as possible. A module is a component of a larger system that interacts with the rest of the system in a simple, well-defined, straightforward manner. The idea is that a module can be \u201cplugged into\u201d a system. The details of what goes on inside the module are not important to the system as a whole, as long as the module fulfills its assigned role corre\u00aectly. This is called information hiding, and it is one of \u00aethe most important principles of software engineering. One common format for software modules is to contain some data, along with some subroutines for manipulating that data. For example, a mailing-list module might contain a list of names and addresses along with a subroutine for adding a new name, a subroutine for printing mailing labels, and so forth. In such modules, the data itself is often hidden inside the module; a program that uses the module can then manipulate the data only indirectly, by calling the subroutines provided by the module. This protects the data, since it can only be manipulated in known, well-defined ways. And it makes it easier for programs to use the module, since they don\u2019t have to worry about the details of how the data is represented. Information about the representation of the data is hidden. Modules that could support this kin\u00aed of information-hiding became common in programmin\u00aeg languages in the early 1980s. Since then, a more advanced form of the same idea has more or less taken over software engineering. This latest approach is called object-oriented programming, often abbreviated as OOP. The central concept of object-oriented programming is the object, which is a kind of module containing data and subroutines. The point-of-view in OOP is that an object is a kind of self sufficient entity that has an internal state (the data it contains) and that can respond to messages (calls to its subroutines). A mailing list object, for example, has a state consisting of a list of names and addresses. If you send it a message telling it to add a name, it will respond by modifying its state to reflect the change. If you send it a message telling it to print itself, it will respond by printing out its list of names and addresses. The OOP approach to software engineering is to start by identifying the objects involved in a problem and the messages that those objects should respond to. The program that results is a collection of objects, each with its own data and its own set of responsibilities. The objects interact by sending messages to each other. There is not much \u201ctop-down\u201d in such a program, and people used to more traditional programs can have a hard time getting used to OOP. However, people who use OOP would claim that object-oriented programs tend to be better models of the way the world itself works, and that they are therefore easier to write, easier to understand, and more likely to be correct. You should think of objects as \u201cknowing\u201d how to respond to certain messages. Different objects might respond to the same message in different ways. For example, a \u201cprint\u201d message would produce very different results, depending on the object it is sent to. This property of objects that different objects can respond to the same message in different ways is called polymorphism. It is common for objects to bear a kind of \u201cfamily resemblance\u201d to one another. Objects that contain the same type of data and that respond to the same messages in the same way belong to the same class. (In actual programming, the class is primary; that is, a class is created and then one or more objects are created using \u00aethat class as a template.) But objects can be similar without being in exactly the same class.",
    "page695": "For example, consider a drawing program that lets the user draw lines, rectangles, ovals, polygons, and curves on the screen. In the program, each visible object on the screen could be represented by a software object in the program. There would be five classes of objects in the program, one for each type of visible object that can be drawn. All the lines would belong to one class, all the rectangles to another class, and so on. These classes are obviously related; all of them represent \u201cdrawable objects.\u201d They would, for example, all presumably be able to respond to a \u201cdraw yourself\u201d message. Another level of grouping, based on the data  to represent each type of object, is less obvious, but would be very useful in a program: We can group polygons and curves together as \u201cmultipoint objects,\u201d while lines, rectangles, and ovals are \u201ctwo-point objects.\u201d (A line is determined by its endpoints, a rectangle by two of its corners, and an oval by two corners of the rectangle that contains it.) We could diagram these relationships as follows DrawableObject, MultipointObject, and TwoPointObject would be classes in the program. MultipointObject and TwoPointObject would be subclasses of DrawableObject. The class Line would be a subclass of TwoPointObject and (indirectly) of DrawableObject. A subclass of a class is said to inherit the properties of that class. The subclass can add to its inheritance and it can even \u201coverride\u201d part of that inheritance (by defining a different response to some method). Nevertheless, lines, rectangles, and so on are drawable objects, and the class DrawableObject expresses this relationship. Inheritance is a powerful means for organizing a program. It is also related to the problem o\u00aef reusing software components. A class is the ultimate reusable component. Not only can it be reused directly if it fits exactly into a program you are trying to write, but if it just almost fits, you can still reuse it by defining a subclass and making only the small changes necessary to adapt it exactly to your needs. So, OOP is meant to be both a superior program-development tool and a partial solution to the software reuse problem. Objects, classes, and object-oriented programming will be important themes throughout the rest of this text. The Modern User Interface - When computers were first introduced, ordinary people including most programmers couldn\u2019t get near them. They were locked up in rooms with white-coated attendants who would take your programs and data, feed them to the computer, and return the computer\u2019s response some time later. When timesharing where the computer switches its attention rapidly from one person to another was invented in the 1960s, it became possible for several people to interact directly with the computer at the same time. On a timesharing system, users sit at \u201cterminals\u201d where they type commands to the computer, and the computer types back its\u00ae response. Early personal computers also used typed commands and responses, except that there was only one person involved at a time. This type of interaction between a user and a computer is called a command-line interface. Today, of course, most people interact with computers in a completely different way. They use a Graphical User Interface, or GUI. The computer draws interface components on the screen. The components include things like windows, scroll bars, menus, buttons, and icons. Usually, a mouse is used to manipulate such components. Assuming that you have not just been teleported in from the 1970s, you are no doubt already familiar with the basics of graphical user interfaces! A lot of GUI interface components have become fairly standard. That is, they have similar appearance and behavior on many different computer platforms including Macintosh, Windows, and Linux. Java programs, which are supposed to run on many different platforms without modification to the program, can use all the standard GUI components. They might vary a little in appearance from platform to platform, but their functionality should be identical on any computer on which the program runs. Shown below is an image of a very simple Java program actually an \u201capplet\u201d, since it is meant to appear on a Web page that shows a few standard GUI interface components. There are four components that the user can interact with: a button, a checkbox, a text field, and a pop-up menu. These components are labeled. There are a few other components in the applet. The labels themselves are components (even though you can\u2019t interact with them). The right half of the applet is a text area component, which can display multiple lines of text, and a scrollbar component appears along\u00aeside the text area when the number of lines of text becomes larger than will fit in the text area. And in fact, in Java terminology, the whole applet is itself considered to be a \u201ccomponent. \u201dNow, Java actually has two complete sets of GUI components. One of these, the AWT or Abstract Windowing Toolkit, was available in the original version of Java. The other, which is known as Swing, is included in Java version 1.2 or later, and is used in preference to the AWT in most modern Java programs. The apple\u00aet that is shown above uses components that are part of Swing. If your Web browser uses an old version of Java, you might get an error when the browser tries to load the applet. Remember that most of the applets in this textbook require Java 5.0 (or higher). When a user interacts with the GUI components in this applet, an \u201cevent\u201d is generated. For example, clicking a push button generates an event, and pressing return while typing in a text field generates an event. Each time an event is generated, a message is sent to the applet telling it that the event has occurred, and the applet responds according to its program. In fact, the program consists mainly of \u201cevent handlers\u201d that tell the applet how to respond to various types of events. In this example, the applet has been programmed to respond to each event by displaying a message in the text area.",
    "page696": "The use of the term \u201cmessag\u00aee\u201d here is deliberate. Messages, as you saw in the previous section, are sent to objects. In fact, Java GUI components are implemented as objects. Java includes many predefined classes that represent various types of GUI components. Some of these classes are subclasses of others. Here is a diagram showing some of Swing\u2019s GUI classes and their relationships. Don\u2019t worry about the details for now, but try to get some feel about how object-oriented programming and inheritance are used here. Note that all the GUI classes are subclasses, directly or indirectly, of a class called JComponent, which represents general properties that are shared by all Swing components. Two of the direct subclasses of JComponent themselves have subclasses. The classes JTextArea and JTextField, which have certain behaviors in common, are grouped together as subclasses of JTextComponent. Similarly JButton and JToggleButton are subclasses of JAbstractButton, which represents properties common to both buttons and checkboxes. (JComboBox, by the way, is the Swing class that represents pop-up menus.) Just from this brief discussion, perhaps you can see how GUI programming can make effective use of object-oriented design. In fact, GUI\u2019s, with their \u201cvisible objects,\u201d are probably a major factor contributing to the popularity of OOP. The Internet and the World-Wide Web - Computers can be connected together on networks. A computer on a network can communicate with other computers on the same network by exchanging data and files or by sending and receiving messages.\u00ae Computers on a network can even work together on a large computation. Today, millions of computers throughout the world are connected to a single huge network called the Internet. New computers are being connected to the Internet every day. Computers can join the Internet by using a modem to establish a connection through telephone lines. Broadband connections to the Internet, such as DSL and cable modems, are increasingly common. They allow faster data transmission than is possible through telephone modems. There are elaborate protocols for communication over the Internet. A protocol is simply a detailed specification of how communication is to proceed. For two computers to communicate at all, they must both be using the same protocols. The most basic protocols on the Internet are the Internet Protocol (IP), which specifies how data is to be physically transmitted from one computer to another, and the Transmission Control Protocol (TCP), which ensures that data sent using IP is received in its entirety and without error. These two protocols, which are referred to collectively as TCP/IP, provide a foundation for communication. Other protocols use TCP/IP to send specific types of information such as web pages, electronic mail, and data files. All communication over the Internet is in the form of packets. A packet consists of some data being sent from one computer to another, along with addressing information that indicates where on the Internet that data is supposed to go. Think of a packet as an envelope with an address on the outside and a message on the inside. (The message is the data.) The\u00ae packet also includes a \u201creturn address,\u201d that is, the address of the sender. A packet can hold only a limited amount of data; longer messages must be divided among several packets, which are then sent individually over the net and reassembled at their destination. Every computer on the Internet has an IP address, a number that identifies it uniquely among all the computers on the net. The IP address is used for addressing packets. A computer can only send data to another computer on the Internet if it knows that computer\u2019s IP address. Since people prefer to use names rather than numbers, most computers are also identified by names, called domain names. For example, the main computer of the Mathematics Department at Hobart and William Smith Colleges has the domain name math.hws.edu. (Domain names are just for convenience; your computer still needs to know IP addresses before it can communicate. There are computers on the Internet whose job it is to translate domain names to IP addresses. When you use a domain name, your computer sends a message to a domain name server to find out the corresponding IP address. Then, your computer uses the IP address, rather than the domain name, to communicate with the other computer.) The Internet provides a number of services to the computers connected to it (and, of course, to the users of those computers). These services use TCP/IP to send various types of \u00aedata over the net. Among the most popular services are instant messaging, file sharing, electronic mail, and the World-Wide Web. Each service has its own protocols, which are used to control transmission of data over the network. Each service also has some sort of user interface, which allows the user to view, send, and receive data through the service. For example, the email service uses a protocol known as SMTP (Simple Mail Transfer Protocol) to transfer email messages from one computer to another. Other protocols, such as POP and IMAP, are used to fetch messages from an email account so that the recipient can read them. A person who uses email, however, doesn\u2019t need to understand or even know about these protocols. Instead, they are used behind the scenes by the programs that the person uses to send and receive email messages. These programs provide an easy-to-use user interface to the underlying network protocols. The World-Wide Web is perhap\u00aes the most exciting of network services. The World-Wide Web allows you to request pages of information that are stored on computers all over the Internet. A Web page can contain links to other pages on the same computer from which it was obtained or to other computers anywhere in the world. A computer that stores such pages of information is called a web server. The user interface to the Web is the type of program known as a web browser. Common web browsers include Internet Explorer and Firefox. You use a Web browser to request a page of information. The browser will send a request for that page to the computer on which the page is stored, and when a response is received from that computer, the web browser displays it to you in a neatly formatted form. A web br\u00aeowser is just a user interface to the Web. Behind the scenes, the web browser uses a protocol called HTTP (HyperText Transfer Protocol) to send each page request and to receive the response from the web server.",
    "page697": "On a basic level (the level of machine language), a computer can perform only very simple operations. A computer performs complex tasks by stringing together large numbers of such operations. Such tasks must be \u201cscripted\u201d in complete and perfect detail by programs. Creating complex programs will never be really easy, but the difficulty can be handled to some extent by giving the program a clear overall structure. The design of the overall structure of a program is what I call \u201cprogramming in the large.\u201d Programming in the small, which is sometimes called coding, would then refer to filling in the details of that design. The details are the explicit, step-by-step instructions for \u00aeperforming fair\u00aely small-scale tasks. When you do coding, you are working fairly \u201cclose to the machine,\u201d with some of the same concepts that you might use in machine language: memory locations, arithmetic operations, loops and branches. In a high-level language such as Java, you get to work with these concepts on a level several steps above machine language. However, you still have to worry about getting all the details exactly right. This chapter and the next examine the facilities for programming in the small in the Java programming language. Don\u2019t be misled by the term \u201cprogramming in the small\u201d into thinking that this material is easy or unimportant. This material is an essential foundation for all types of programming. If you don\u2019t understand it, you can\u2019t write programs, no matter how good you get at designing their large-scale structure. The Basic Java Application - A program is a sequence of instructions that a computer can execute to perform some task. A simple enough idea, but for the computer to make any use of the instructions, they must be written in a form that the computer can use. This means that programs have to be written in programming languages. Programming languages differ from ordinary human languages in being completely unambiguous and very strict about what is and is not allowed in a program. The rules that determine what is allowed are called the syntax of the language. Syntax rules specify the basic vocabulary of the language and how programs can be constructed using things like loops, branches, and subroutines. A syntactically correct program \u00aeis one that can be successfully compiled or interpreted; programs that have syntax errors will be rejected (hopefully with a useful \u00aeerror message that will help you fix the problem). So, to be a successful programmer, you have to develop a detailed knowledge of the syntax of the programming language that you are using. However, syntax is only part of the story. It\u2019s not enough to write a program that will run you want a program that will run and produce the correct result! That is, the meaning of the program has to be right. The meaning of a program is referred to as its semantics. A semantically correct program is one that does what you want it to. Furthermore, a program can be syntactically and semantically correct but still be a pretty bad program. Using the language correctly is not the same as using it well. For example, a good program has \u201cstyle.\u201d It is written in a\u00ae way that will make it easy for people to read and to understand. It follows conventions that will be familiar to other programmers. And it has an overall design that will make sense to human readers. The computer is completely oblivious to such things, but to a human reader, they are paramount. These aspects of programming are sometimes referred to as pragmatics. When I \u00aeintroduce a new language feature, I will explain the syntax, the semantics, and some of the pragmatics of that feature. You should memorize the syntax; that\u2019s the easy part. Then you should get a feeling f\u00aeor the semantics by following the examples given, making sure that you understand how they work, and maybe writing short programs of your own to test your understanding. And you should try to appreciate and absorb the pragmatics this means learning how to use the language feature well, with style that will earn you the admiration of other programmers. Of course, even when you\u2019ve become familiar with all the individual features of the language, that doesn\u2019t make you a programmer. You still have to learn how to construct complex programs to solve particular problems. For that, you\u2019ll need both experience and taste. You\u2019ll find hints about software development throughout this textbook. We begin our exploration of Java with the problem that has become traditional for such beginnings: to write a program that displays the me\u00aessage \u201cHello World!\u201d. This might seem like a trivial problem, but getting a computer to do this is really a big first step in learning a new programming language (especially if it\u2019s your first programming language). It means that you understand the basic process of: 1. getting the program text into the computer, 2. compiling the program, and 3. running the compiled program. The first time through, each of these steps will probably take you a few tries to get right. I won\u00ae\u2019t go into the details here of how you do each of these steps; it depends on the particular computer and Java programming environment that you are using. See Section 2.6 for information about creating and running Java programs in specific programming environments. But in general, you will type the program using some sort of text editor and save the program in a file. Then, you will use some command to try to compile the file. You\u2019ll either get a message that the program contains syntax errors, or you\u2019ll get a compiled version of the program. In the case of Java, the program is compiled into Java bytecode, not into machine language. Finally, you can run the compiled program by giving some appropriate command. For Java, you will actually use an interpreter to execute the Java bytecode. Your pr\u00aeogramming environment might automate some of the steps for you, but you can be sure that the same three steps are being done in the background.",
    "page698": "Here is a Java program to display th\u00aee message \u201cHello World!\u201d. Don\u2019t expect to understand what\u2019s going on here just yet some of it you won\u2019t really understand until a few chapters from now: // A program to display the message // \\\"Hello World!\\\" on standard output public class HelloWorld { public static void main(String[] args) { System.out.println(\\\"Hello World!\\\"); } } // end of class HelloWorld The command that actually displays the message is: System.out.println(\\\"Hello World!\\\"); This command is an example of a subroutine call st\u00aeatement. It uses a \u201cbuilt-in subroutine\u201d named System.out.println to do the actual work. Recall that a subroutine consists of the instructions for performing some task, chunked together and given a name. That name can be used to \u201ccall\u201d the subroutine whenever that task needs to be performed. A built-in subroutine is one that is already defined as part of the language and therefore automatically available for use in any program. When you run this program, the message \u201cHello World!\u201d (without the quotes) will be displayed on standard output. Unfortunately, I can\u2019t say exactly what that means! Java is meant to run on many different platforms, and standard output will mean different things on different platforms. However, you can expect the message to show up in some convenient place. (If you use a command-line interface, lik\u00aee that in Sun Microsystem\u2019s Java Development Kit, you type in a command to tell the computer to run the program. The computer will type the output from the program, Hello World!, on the next line.) You must be curious about all the other stuff in the above program. Part of it consists of comments. Comments in a program are entirely ignored by the computer; they are there for human readers only. This doesn\u2019t mean that they are unimportant. Programs are meant to be read by people as well as by computers, and without comments, a program can be very difficult to understand. Java has two types of comments. The first type, used in the above program, begins with // and extends to the end of a line. The computer ignores the // and everything that follows it on the same line. Java has another style of comment that can extend over many lines. That type of comment begins with /* and ends with */. Everything else in the program is required by the rules of Java syntax. All programming in Java is done inside \u201cclasses.\u201d The first line in the above program (not counting the comments) says that this is a class named HelloWorld. \u201cHelloWorld,\u201d the name of the class, also serves as the name of the program. Not every class is a program. In order to define a program, a class must include a subroutine named main, with a definition that takes the form public static void main(String[] args) { (statements) } When you tell the Java interpreter to run the program, the interpreter calls the main() subroutine, and the statements that it contains are executed. These statements make up the script that tells the computer exactly what to do when the program is executed. The main() routine can call subroutines that are defined in the same class or even in other classes, but it i\u00aes the main() routine that determines how and in what order the other subroutines are used. The word \u201cpublic\u201d in the first line of main() means that this routine can be called from outside the program. This is essential because the main() routine is called by the Java interpreter, which is something external to the program itself. The remainder of the first line of the routine is harder to explain at the moment; for now, just think of it as part of the required syntax. The definition of the subroutine that is, the instructions that say what it does consists of the sequence of \u201cstatements\u201d enclosed between braces, { and }. Here, I\u2019ve used statements as a placeholder for the actual statements that make up the program. Throughout this textbook, I will always use a similar format: anything that you see in this style of text (italic in angle brackets) is a placeholder that describes something you need to type when you write an actual program. As noted above, a subroutine can\u2019t exist by itself. It has to be part of a \u201cclass\u201d. A program is defined by a public class that takes the form. public class hprogram-namei { hoptional-variable-declarations-and-subroutinesi public static void main(String[] args) { statements } optional-variable-declarations-and-subroutines } The name on the first line is the name of the program, as well as the name of the class. If the name of the class is HelloWorld, then the class must be saved in a file called HelloWorld.java. When this file is compiled, another file named HelloWorld.class will be produced. This class file, HelloWorld.class, contains the Java bytecode that is e\u00aexecuted by a Java interpreter. HelloWorld.java is called the source code for the program. To execute the program, you only need the compiled class file, not the source code. The layout of the program on the page, such as the use of blank lines and indentation, is not part of the syntax or semantics of the language. The computer doesn\u2019t care about layout you could run the entire program together on one line as far as it is concerned. However, layout is important to human readers, and there are certain style guidelines for layout that are followed by most programmers. These style guidelines are part of the pragmatics of the Java programming language.",
    "page699": "Variables and the Primitive Types - Names are fundamental to programming. In programs, names are used to refer to many different sorts of things. In order to use those things, a programmer must understand the rules for giving names to things and the rules for using the names to work with those things. That is, the programmer must understand the syntax and the semantics of names. According to the syntax rules of Java, a name is a sequence of one or more characters. It must begin with a letter or underscore and must consist entirely of letters, digits, and underscores. (\u201cUnderscore\u201d refers to the character \u2019 \u2019.) For example, here are some legal names: No spaces are allowed in identifiers; HelloWorld is a legal identifier, but \u201cHello World\u201d is not. Upper case and lower case letters are considered to be different, so that HelloWorld, helloworld, HELLOWORLD, and hElloWorLD are all distinct names. Certain names are reserved for special uses in Java, and cannot be used by the programmer for other purposes. These reserved words include: class, public, static, if, else, while, and several dozen other words. Java is actually pretty liberal about what counts as a letter or a digit. Java uses the Unicode character set, which includes thousands of characters from many different languages and different alphabets, and many of these characters count as letters or digits. However, I will be sticking to what can be typed on a regular English keyboard. The pragmatics of naming includes style guidelines about how to choose names for things. For example, it is customary for names of classes to begin with upper case letters, while names o\u00aef variables and of subroutines begin with lower case letters; you can avoid a lot of confusion by following the same convention in your own programs. Most Java programmers do not use underscores in names, although some do use them at the beginning of the names of certain kinds of variables. When a name is made up of several words, such as HelloWorld or interestRate, it is customary to capitalize each word, except possibly the first; this is sometimes referred to as camel case, since the upper case letters in the middle of a name are supposed to look something like the humps on a camel\u2019s back. Finally, I\u2019ll no\u00aete that things are often referred to by compound names which consist of several ordinary names separated by periods. (Compound names are also called qualified names.) You\u2019ve already seen an example: System.out.println. The idea here is that things in Java can contain other things. A compound name is a kind of path to an item through one or more levels of containment. The name System.out.println indicates that something called \u201cSystem\u201d contains something called \u201cout\u201d which in turn contains something called \u201cprintln\u201d. Non-compound names are called simple identifiers. I\u2019ll use the term identifier to refer to any name simple or compound that can be used to refer to something in Java. (Note that the reserved words are not identifiers, since they can\u2019t be used as names for things.) Variables - Programs manipulate data that are stored in memory. In machine language, data can only be referred to by giving the numerical address of the location in memory where it is stored. In a high-level language such as Java, names are used instead of numbers to refer to data. It is the job of the computer to keep track of where in memory the data is actually stored; the programmer only has to remember the name. A name used in this\u00ae way to refer to data stored in memory is called a variable. Variables are actually rather subtle. Properly speaking, a variable is not a name for the data itself but for a location in memory that can hold data. You should think of a variable as a container or box\u00ae where you can store data that you will need to use later. The variable refers directly to the box and only indirectly to the data in the box. Since the data in the box can change, a variable can refer to different data values at different times during the execution of the program, but it always refers to the same box. Confusion can arise, especially for beginning programmers, because when a variable is used in a program in certain ways, it refers to the container, but when it is used in other ways, it refers to the data in the container. You\u2019ll see examples of both cases below. (In this way, a variable is something like the title, \u201cThe President of the United States.\u201d This title can refer to different people at different times, but it always refers to the same office. If I say \u201cthe President went fishing,\u201d I mean that George W. Bush went fishing. But if I say \u201cHillary Clinton wants to be President\u201d I mean that she wants to fill the office, not that she wants to be George Bush.) In Java, the only way to get data into a variable that is, into the box that the variable names is with an assignment statement. An assignment statement takes the form: (variable) = (expression); where expression represents anything that refers to or computes a data value. When the computer comes to an assignment statement in the course of executing a program, it evaluates the expression and puts the resulting data value into the variable. For example, consider the simple assignment statement rate = 0.07; The variable in this assignment statement is rate, and the expression is the number 0.07. The computer executes this assignment statement by putting the number 0.07 in the variable rate, replacing whatever was there before. Now, consider the following more complicated assignment statement, which might come later in the same program.",
    "page700": "Types and Literals- A variable in Java is designed to hold only one particular type of data; it can legally hold that type of data and no other. The compiler will consider it to be a syntax error if you try to violate this rule. We say that Java is a strongly typed language because it enforces this rule. There are eight so-called primitive types built into Java. The primitive types are named byte, short, int, long, float, double, char, and boolean. The first four types hold integers (whole numbers such as 17, -38477, and 0). The four integer types are distinguished by the ranges of integers they can hold. The float and double types hold real numbers (such as 3.6 and -145.99). Again, the two real types are distinguished by their range and accuracy. A variable of type char holds a single character from the Unicode character set. And a variable of type boolean holds one of the two logical values true or false. Any data value stored in the computer\u2019s memory must be represented as a binary number, that is as a string of zeros and ones. A single zero or one is called a bit. A string of eight bits is called a byte. Memory is usually measured in terms of bytes. Not surprisingly, the byte data type refers to a single byte of memory. A variable of type byte holds a string of eight bits, which can represent any of the integers between -128 and 127, inclusive. (There are 256 integers in that range; eight bits can represent 256 two raised to the power eight different values.) As for the other integer types, short corresponds to two bytes (16 bits). Variables of type short have values in the range -32768 to 32767. int corresponds to four bytes (32 bits). Variables of type int have values in the range -2147483648 to 2147483647. long corresponds to eight bytes (64 bits). Variables of type long have values in the range -9223372036854775808 to 9223372036854775807. You don\u2019t have to remember these numbers, but they do give you some idea of the size of integers that you can work with. Usually, you should just stick to the int data type, which is good enough for most purposes. The float data type is represented in four bytes of memory, using a standard method for encoding real numbers. The maximum value for a float is about 10 raised to the power 38. A float can have about 7 significant digits. (So that 32.3989231134 and 32.3989234399 would both have to be rounded off to about 32.398923 in order to be stored in a variable of type float.) A double takes up 8 bytes, can range up to about 10 to the power 308, and has about 15 significant digits. Ordinarily, you should stick to the double type for real values. A variable of type char occupies two bytes in memory. The value of a char variable is a single character such as A, *, x, or a space character. The value can also be a special character such a tab or a carriage return or one of the many Unicode characters that come from differen\u00aet languages. When a character is typed into a program, it must be surrounded by single quotes; for example: \u2019A\u2019, \u2019*\u2019, or \u2019x\u2019. Without the quotes, A would be an identifier and * would be a multiplication operator. The quotes are not part of the value and are not stored in the vari\u00aeable; they are just a convention for naming a particular character constant in a program. A name for a constant value is called a literal. A literal is what you have to type in a program to represent a value. \u2019A\u2019 and \u2019*\u2019 are literals of type char, representing the character values A and *. Certain special characters have special literals that use a backslash, \\, as an \u201cescape character\u201d. In particular, a tab is represented as \u2019t\u2019, a carriage return as \u2019r\u2019, a linefeed as \u2019 \u2019, the single quote character as , and the backslash itself as \u2019\\\u2019. Note that even though you type two characters between the quotes in \u2019t\u2019, the value represented by this literal is a single tab character. Numeric literals are a little more complicated than you might expect. Of course, there are the obvious literals such as 317 and 17.42. But there are other possibilities for expressing numbers in a Java program. First of all, real numbers can be represented in an exponential form such as 1.3e12 or 12.3737e-108. The \u201ce12\u201d and \u201ce-108\u201d represent powers of 10, so that 1.3e12 means 1.3 t\u00aeimes 1012 and 12.3737e-108 means 12.3737 times 10\u2212108. This format can be used to express very large and very small numbers. Any numerical literal that contains a decimal point or exponential is a literal of type double. To make a literal of type float, you have to append an \u201cF\u201d or \u201cf\u201d to the end of the number. For example, \u201c1.2F\u201d stands for 1.2 considered as a value of type float. (Occas\u00aeionally, you need to know this because the rules of Java say that you can\u2019t assign a value of type double to a variable of type float, so you might be confronted with a ridiculous-seeming error message if you try to do something like \u201cx = 1.2;\u201d when x is a variable of type float. You have to say \u201cx = 1.2F;. This is one reason why I advise sticking to type double for real numbers.) Even for integer literals, there are some complications. Ordinary integers such as 177777 and -32 are literals of type byte, short, or int, depending on their size. You can make a literal of type long by adding \u201cL\u201d as a suffix. For example: 17L or 728476874368L. As another complication, Java allows octal (base-8) and hexadecimal (base-16) literals. I don\u2019t want to cover base-8 and base-16 in detail, but in case you run into them in other people\u2019s programs, it\u2019s worth knowing a few things: Octal numbers use only the digits 0 through 7. In Java, a numeric literal that begins with a 0 is interpreted as an octal number; for example, the literal 045 represents the number 37, not the number 45. Hexadecimal numbers use 16 digits, the usual digits 0 through 9 and the letters A, B, C, D, E, and F. Upper case and lower case letters can be used interchangeably in this context. The letters represent the numbers 10 through 15. In Java, a hexadecimal literal begins\u00ae with 0x or 0X, as in 0x45 or 0xFF7A. Hexadecimal numbers are also used in character literals to represent arbitrary Unicode characters. A Unicode literal consists of u followed by four hexadecimal digits. For example, the character literal \u2019u00E9\u2019 represents the Unicode character that is an \u201ce\u201d with an acute accent.",
}

const userData = JSON.parse(atob(sessionStorage.getItem("userData")));